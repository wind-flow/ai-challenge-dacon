#!/usr/bin/env python3
"""
ì¶”ë¡ ìš© ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸

4ê°€ì§€ ì¡°í•©ì„ ìœ„í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìµœì í™”
"""

import os
import sys
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì¶”ë¡  í™˜ê²½ ìŠ¤í™
RUNPOD_SPEC = {
    'gpu': 'RTX 4090',
    'vram': 24,  # GB
    'ram': 41,   # GB
    'time_limit': 270  # minutes
}

# í…ŒìŠ¤íŠ¸í•  ì¡°í•©
MODEL_COMBINATIONS = [
    {
        'id': 'combo1',
        'generation': 'Qwen/Qwen2.5-32B-Instruct',
        'inference': 'upstage/SOLAR-10.7B-v1.0',
        'description': 'Qwen-32B ìƒì„± + SOLAR ì¶”ë¡ '
    },
    {
        'id': 'combo2',
        'generation': 'Qwen/Qwen2.5-14B-Instruct',
        'inference': 'upstage/SOLAR-10.7B-v1.0',
        'description': 'Qwen-14B ìƒì„± + SOLAR ì¶”ë¡ '
    },
    {
        'id': 'combo3',
        'generation': 'Qwen/Qwen2.5-32B-Instruct',
        'inference': 'Qwen/Qwen2.5-7B-Instruct',
        'description': 'Qwen-32B ìƒì„± + Qwen-7B ì¶”ë¡ '
    },
    {
        'id': 'combo4',
        'generation': 'upstage/SOLAR-10.7B-v1.0',
        'inference': 'upstage/SOLAR-10.7B-v1.0',
        'description': 'SOLAR ìƒì„± + SOLAR ì¶”ë¡  (ë² ì´ìŠ¤ë¼ì¸)'
    }
]

def check_model_size(model_name):
    """ëª¨ë¸ í¬ê¸° ì¶”ì •"""
    try:
        config = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True
        ).config
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        params = config.hidden_size * config.num_hidden_layers * 12  # ëŒ€ëµì  ì¶”ì •
        size_gb = params * 2 / 1024**3  # FP16 ê¸°ì¤€
        
        return size_gb
    except:
        # ëª¨ë¸ ì´ë¦„ì—ì„œ ì¶”ì •
        if "32B" in model_name:
            return 64
        elif "14B" in model_name:
            return 28
        elif "10.7B" in model_name or "10B" in model_name:
            return 21
        elif "7B" in model_name:
            return 14
        else:
            return 14  # ê¸°ë³¸ê°’

def prepare_inference_model(model_name, output_dir):
    """ì¶”ë¡ ìš© ëª¨ë¸ ì¤€ë¹„"""
    logger.info(f"\nğŸ”„ {model_name} ì¤€ë¹„ ì¤‘...")
    
    # ëª¨ë¸ í¬ê¸° í™•ì¸
    size_gb = check_model_size(model_name)
    logger.info(f"ğŸ“Š ì˜ˆìƒ í¬ê¸°: {size_gb:.1f}GB")
    
    # ë©”ëª¨ë¦¬ ì²´í¬
    if size_gb > RUNPOD_SPEC['vram']:
        logger.warning(f"âš ï¸ ëª¨ë¸ì´ VRAMë³´ë‹¤ í½ë‹ˆë‹¤! ì–‘ìí™” í•„ìš”")
        quantization = "4bit"
    elif size_gb > RUNPOD_SPEC['vram'] * 0.8:
        quantization = "8bit"
    else:
        quantization = None
    
    # ì¶œë ¥ ê²½ë¡œ
    model_dir = Path(output_dir) / model_name.replace("/", "_")
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # ë‹¤ìš´ë¡œë“œ ì •ë³´ ì €ì¥
    info = {
        'model_name': model_name,
        'size_gb': size_gb,
        'quantization': quantization,
        'runpod_compatible': size_gb <= RUNPOD_SPEC['vram'],
        'recommended_batch_size': 1 if size_gb > 20 else 2 if size_gb > 14 else 4
    }
    
    import json
    with open(model_dir / "model_info.json", 'w') as f:
        json.dump(info, f, indent=2)
    
    logger.info(f"âœ… ëª¨ë¸ ì •ë³´ ì €ì¥: {model_dir}")
    
    return info

def generate_inference_script(combo, model_info):
    """ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    script = f'''#!/usr/bin/env python3
"""
ìë™ ìƒì„±ëœ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
ì¡°í•©: {combo['description']}
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import pandas as pd
import json
from tqdm import tqdm

# ì„¤ì •
MODEL_NAME = "{combo['inference']}"
QUANTIZATION = {model_info.get('quantization', None)}
BATCH_SIZE = {model_info['recommended_batch_size']}

def load_model():
    """ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”©: {{MODEL_NAME}}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model_kwargs = {{
        'trust_remote_code': True,
        'device_map': 'auto'
    }}
    
    if QUANTIZATION == "4bit":
        from transformers import BitsAndBytesConfig
        model_kwargs['quantization_config'] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
    elif QUANTIZATION == "8bit":
        from transformers import BitsAndBytesConfig
        model_kwargs['quantization_config'] = BitsAndBytesConfig(
            load_in_8bit=True
        )
    else:
        model_kwargs['torch_dtype'] = torch.float16
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        **model_kwargs
    )
    
    return model, tokenizer

def run_inference(test_file="data/test.csv", output_file="submission.csv"):
    """ì¶”ë¡  ì‹¤í–‰"""
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(test_file)
    results = []
    
    # ë°°ì¹˜ ì²˜ë¦¬
    for i in tqdm(range(0, len(df), BATCH_SIZE)):
        batch = df.iloc[i:i+BATCH_SIZE]
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompts = []
        for _, row in batch.iterrows():
            prompt = create_prompt(row)
            prompts.append(prompt)
        
        # ì¶”ë¡ 
        inputs = tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
        
        # ë””ì½”ë”©
        for j, output in enumerate(outputs):
            text = tokenizer.decode(output, skip_special_tokens=True)
            answer = extract_answer(text, prompts[j])
            results.append({{
                'id': batch.iloc[j]['id'],
                'answer': answer
            }})
    
    # ì €ì¥
    result_df = pd.DataFrame(results)
    result_df.to_csv(output_file, index=False)
    print(f"âœ… ê²°ê³¼ ì €ì¥: {{output_file}}")

def create_prompt(row):
    """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    if row['type'] == 'multiple_choice':
        return f"""ë‹¤ìŒ ê¸ˆìœµ ë¬¸ì œì— ëŒ€í•œ ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”.

ë¬¸ì œ: {{row['question']}}

ì„ íƒì§€:
{{row['choices']}}

ì •ë‹µ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”: """
    else:
        return f"""ë‹¤ìŒ ê¸ˆìœµ ë¬¸ì œì— ëŒ€í•´ ì „ë¬¸ê°€ì˜ ê´€ì ì—ì„œ ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ì œ: {{row['question']}}

ë‹µë³€: """

def extract_answer(text, prompt):
    """ë‹µë³€ ì¶”ì¶œ"""
    answer = text.replace(prompt, "").strip()
    
    # ê°ê´€ì‹ì¸ ê²½ìš° ë²ˆí˜¸ë§Œ ì¶”ì¶œ
    if "ì •ë‹µ ë²ˆí˜¸ë§Œ" in prompt:
        import re
        match = re.search(r'[1-4]', answer)
        if match:
            return match.group()
    
    return answer

if __name__ == "__main__":
    import time
    start = time.time()
    
    run_inference()
    
    elapsed = time.time() - start
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {{elapsed/60:.1f}}ë¶„")
    
    if elapsed > {RUNPOD_SPEC['time_limit']} * 60:
        print("âŒ ì‹œê°„ ì´ˆê³¼!")
    else:
        print("âœ… ì‹œê°„ ë‚´ ì™„ë£Œ!")
'''
    
    return script

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸš€ ì¶”ë¡  ëª¨ë¸ ì¤€ë¹„")
    print("="*60)
    
    output_dir = Path("models/inference_ready")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ê° ì¡°í•© ì¤€ë¹„
    for combo in MODEL_COMBINATIONS:
        print(f"\nğŸ“‹ {combo['description']}")
        print("-"*60)
        
        # ì¶”ë¡  ëª¨ë¸ ì •ë³´
        model_info = prepare_inference_model(
            combo['inference'],
            output_dir
        )
        
        # ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        script_path = output_dir / f"{combo['id']}_inference.py"
        script = generate_inference_script(combo, model_info)
        
        with open(script_path, 'w') as f:
            f.write(script)
        
        print(f"âœ… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {script_path}")
        
        # ìš”ì•½
        print(f"\nğŸ“Š ì¡°í•© ìš”ì•½:")
        print(f"- ìƒì„± ëª¨ë¸: {combo['generation']}")
        print(f"- ì¶”ë¡  ëª¨ë¸: {combo['inference']}")
        print(f"- ì–‘ìí™”: {model_info.get('quantization', 'None')}")
        print(f"- ê¶Œì¥ ë°°ì¹˜: {model_info['recommended_batch_size']}")
        print(f"- RunPod í˜¸í™˜: {'âœ…' if model_info['runpod_compatible'] else 'âŒ'}")
    
    # ìµœì¢… ê¶Œì¥ì‚¬í•­
    print("\n" + "="*60)
    print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    print("1. combo1 (Qwen-32B + SOLAR): ìµœê³  í’ˆì§ˆ")
    print("2. combo2 (Qwen-14B + SOLAR): ê· í˜•")
    print("3. combo4 (SOLAR + SOLAR): ë² ì´ìŠ¤ë¼ì¸")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. Colabì—ì„œ ê° ì¡°í•©ë³„ ë°ì´í„° ìƒì„±")
    print("2. ìƒì„±ëœ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹")
    print("3. RunPodì—ì„œ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("4. ìµœê³  ì„±ëŠ¥ ì¡°í•© ì„ íƒ")

if __name__ == "__main__":
    main()