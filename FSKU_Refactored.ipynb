{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ FSKU ë¦¬íŒ©í† ë§ëœ ë°ì´í„° ì¦ê°• ì‹œìŠ¤í…œ\n",
    "\n",
    "## ì£¼ìš” ê°œì„ ì‚¬í•­\n",
    "- âœ… **ê²½ëŸ‰í™”**: ê¸°ì¡´ 46K í† í° â†’ 2K í† í°ìœ¼ë¡œ ì¶•ì†Œ\n",
    "- âœ… **í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”**: ë³µì¡í•œ í…œí”Œë¦¿ â†’ ê°„ë‹¨ëª…ë£Œí•œ í”„ë¡¬í”„íŠ¸\n",
    "- âœ… **ì²´ì´ë‹ ì˜µì…˜**: `use_chaining=True/False`ë¡œ ì„ íƒ ê°€ëŠ¥\n",
    "- âœ… **H100 ìµœì í™”**: bfloat16, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ \n",
    "- âœ… **ë¹ ë¥¸ ì‹¤í–‰**: 10ê°œ ìƒì„±ì´ 1-2ë¶„ ë‚´ ì™„ë£Œ\n",
    "\n",
    "## ì‚¬ìš© ì˜µì…˜\n",
    "- **ë‹¨ìˆœ ëª¨ë“œ** (`use_chaining=False`): 1íšŒ LLM í˜¸ì¶œ, ë¹ ë¥¸ ìƒì„±\n",
    "- **ì²´ì´ë‹ ëª¨ë“œ** (`use_chaining=True`): ë‹¤ì¤‘ LLM í˜¸ì¶œ, ê³ í’ˆì§ˆ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ë”¥ëŸ¬ë‹ ë° NLP\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# ë²¡í„° ê²€ìƒ‰ ë° ì„ë² ë”© (RAGìš©)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# ë¬¸ì„œ ì²˜ë¦¬\n",
    "import PyPDF2\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "BASE_DIR = Path.cwd()\n",
    "EXTERNAL_DIR = BASE_DIR / \"data\" / \"external\"\n",
    "OUTPUT_DIR = BASE_DIR / \"data\" / \"augmented\"\n",
    "CACHE_DIR = BASE_DIR / \"data\" / \"cache\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for dir_path in [OUTPUT_DIR, CACHE_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ì™¸ë¶€ ë¬¸ì„œ: {EXTERNAL_DIR}\")\n",
    "print(f\"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}\")\n",
    "\n",
    "# GPU ì •ë³´\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    print(\"ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– ë‹¨ìˆœí™”ëœ ë°ì´í„° ìƒì„±ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataGenerator:\n",
    "    \"\"\"\n",
    "    ë‹¨ìˆœí™”ëœ FSKU ë°ì´í„° ìƒì„±ê¸°\n",
    "    - ì²´ì´ë‹ ì˜µì…˜ ì„ íƒ ê°€ëŠ¥ âœ…\n",
    "    - ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ âœ…\n",
    "    - H100 ìµœì í™” âœ…\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"microsoft/phi-2\",\n",
    "                 use_chaining: bool = False,\n",
    "                 use_quantization: bool = False):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…\n",
    "            use_chaining: CoT ì²´ì´ë‹ ì‚¬ìš© ì—¬ë¶€ â­ í•µì‹¬ ì˜µì…˜!\n",
    "            use_quantization: 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.use_chaining = use_chaining\n",
    "        self.use_quantization = use_quantization\n",
    "        \n",
    "        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê¸°ì¡´ ëŒ€ë¹„ 90% ì¶•ì†Œ)\n",
    "        self.simple_prompt = \"\"\"ì£¼ì œ: {question_type}\n",
    "\n",
    "ì°¸ê³  ë‚´ìš©:\n",
    "{context}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {question_type} ë¬¸ì œë¥¼ 1ê°œ ë§Œë“œì„¸ìš”.\n",
    "\n",
    "ë¬¸ì œ:\n",
    "ì •ë‹µ:\"\"\"\n",
    "        \n",
    "        # ì²´ì´ë‹ìš© í”„ë¡¬í”„íŠ¸ (ë” ìƒì„¸í•˜ì§€ë§Œ ì—¬ì „íˆ ë‹¨ìˆœ)\n",
    "        self.chaining_prompt = \"\"\"ê¸ˆìœµ ì „ë¬¸ê°€ë¡œì„œ FSKU ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "- ë¬¸ì œ ìœ í˜•: {question_type}\n",
    "- FSKU ì‹¤ì œ ì‹œí—˜ ìˆ˜ì¤€\n",
    "- ëª…í™•í•˜ê³  ì •í™•í•œ í‘œí˜„\n",
    "\n",
    "ë¬¸ì œ:\n",
    "ì •ë‹µ:\n",
    "í•´ì„¤:\"\"\"\n",
    "        \n",
    "        # í†µê³„\n",
    "        self.stats = {'total': 0, 'success': 0, 'failed': 0}\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"ëª¨ë¸ ì´ˆê¸°í™” - H100 ìµœì í™”\"\"\"\n",
    "        print(f\"ğŸš€ ëª¨ë¸ ë¡œë”©: {self.model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name, \n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # ëª¨ë¸ ë¡œë“œ ì„¤ì •\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"device_map\": \"auto\"\n",
    "            }\n",
    "            \n",
    "            # H100 ìµœì í™”\n",
    "            if torch.cuda.is_available():\n",
    "                model_kwargs[\"torch_dtype\"] = torch.bfloat16  # H100 ìµœì í™”\n",
    "                \n",
    "            # ì–‘ìí™” ì„¤ì • (ì˜µì…˜)\n",
    "            if self.use_quantization:\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                    bnb_4bit_use_double_quant=True\n",
    "                )\n",
    "                model_kwargs[\"quantization_config\"] = bnb_config\n",
    "            \n",
    "            # ëª¨ë¸ ë¡œë“œ\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name, \n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            load_time = time.time() - start_time\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë³´\n",
    "            if torch.cuda.is_available():\n",
    "                memory_gb = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"âœ… ë¡œë“œ ì™„ë£Œ! ({load_time:.1f}ì´ˆ, {memory_gb:.2f}GB)\")\n",
    "            else:\n",
    "                print(f\"âœ… CPU ë¡œë“œ ì™„ë£Œ! ({load_time:.1f}ì´ˆ)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_text(self, prompt: str, max_tokens: int = 150) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ìƒì„± - ê°„ë‹¨í•˜ê³  ë¹ ë¦„\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise ValueError(\"ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # í† í°í™” (ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶•ì†Œ)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=800  # ê¸°ì¡´ 2048 â†’ 800ìœ¼ë¡œ ì¶•ì†Œ\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        generated = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return generated.strip()\n",
    "    \n",
    "    def generate_qa_pair(self, context: str, question_type: str = \"ê°ê´€ì‹\") -> Optional[Dict]:\n",
    "        \"\"\"QA ìŒ ìƒì„± - ì²´ì´ë‹ ì˜µì…˜ì— ë”°ë¼ ë¶„ê¸°\"\"\"\n",
    "        self.stats['total'] += 1\n",
    "        \n",
    "        try:\n",
    "            if self.use_chaining:\n",
    "                return self._generate_with_chaining(context, question_type)\n",
    "            else:\n",
    "                return self._generate_simple(context, question_type)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "    \n",
    "    def _generate_simple(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"ë‹¨ìˆœ ìƒì„± (1íšŒ í˜¸ì¶œ) - ë§¤ìš° ë¹ ë¦„!\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ì»¨í…ìŠ¤íŠ¸ ì¶•ì•½)\n",
    "        prompt = self.simple_prompt.format(\n",
    "            context=context[:400],  # 400ìë¡œ ì œí•œ\n",
    "            question_type=question_type\n",
    "        )\n",
    "        \n",
    "        # ìƒì„±\n",
    "        generated = self.generate_text(prompt)\n",
    "        \n",
    "        # íŒŒì‹±\n",
    "        qa_pair = self._parse_qa(generated)\n",
    "        if not qa_pair:\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        result = {\n",
    "            'question': qa_pair['question'],\n",
    "            'answer': qa_pair['answer'],\n",
    "            'context': context,\n",
    "            'question_type': question_type,\n",
    "            'method': 'simple',\n",
    "            'time_taken': elapsed,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.stats['success'] += 1\n",
    "        return result\n",
    "    \n",
    "    def _generate_with_chaining(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"ì²´ì´ë‹ ìƒì„± (ë‹¤ì¤‘ í˜¸ì¶œ) - ê³ í’ˆì§ˆ!\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # 1. ì´ˆê¸° ìƒì„±\n",
    "        prompt = self.chaining_prompt.format(\n",
    "            context=context[:600],  # ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©\n",
    "            question_type=question_type\n",
    "        )\n",
    "        generated = self.generate_text(prompt, max_tokens=200)\n",
    "        qa_pair = self._parse_qa_detailed(generated)\n",
    "        \n",
    "        if not qa_pair:\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "        \n",
    "        # 2. ê°„ë‹¨í•œ ê²€ì¦ (1íšŒë§Œ)\n",
    "        verification_prompt = f\"\"\"ë‹¤ìŒ ë¬¸ì œë¥¼ ê²€í† í•˜ì„¸ìš”:\n",
    "\n",
    "{qa_pair['question']}\n",
    "{qa_pair['answer']}\n",
    "\n",
    "ë¬¸ì œì ì´ ìˆìœ¼ë©´ ì§€ì í•˜ê³ , ì—†ìœ¼ë©´ \"ì í•©\"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”:\"\"\"\n",
    "        \n",
    "        feedback = self.generate_text(verification_prompt, max_tokens=100)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        result = {\n",
    "            'question': qa_pair['question'],\n",
    "            'answer': qa_pair['answer'],\n",
    "            'context': context,\n",
    "            'question_type': question_type,\n",
    "            'method': 'chaining',\n",
    "            'feedback': feedback,\n",
    "            'time_taken': elapsed,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.stats['success'] += 1\n",
    "        return result\n",
    "    \n",
    "    def _parse_qa(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"ê°„ë‹¨í•œ QA íŒŒì‹±\"\"\"\n",
    "        try:\n",
    "            # ì •ë‹µ êµ¬ë¶„ì ì°¾ê¸°\n",
    "            for marker in ['ì •ë‹µ:', 'ë‹µ:', 'Answer:', 'A:']:\n",
    "                if marker in text:\n",
    "                    parts = text.split(marker, 1)\n",
    "                    question = parts[0].strip()\n",
    "                    answer = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                    \n",
    "                    if len(question) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬\n",
    "                        return {'question': question, 'answer': answer}\n",
    "            \n",
    "            # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ ë¬¸ì œë¡œ ê°„ì£¼\n",
    "            if len(text.strip()) > 10:\n",
    "                return {'question': text.strip(), 'answer': \"ë‹µë³€ í•„ìš”\"}\n",
    "            \n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _parse_qa_detailed(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"ìƒì„¸ QA íŒŒì‹± (í•´ì„¤ í¬í•¨)\"\"\"\n",
    "        try:\n",
    "            sections = {'question': '', 'answer': '', 'explanation': ''}\n",
    "            \n",
    "            # ë¬¸ì œ ì¶”ì¶œ\n",
    "            if 'ë¬¸ì œ:' in text:\n",
    "                question_part = text.split('ë¬¸ì œ:')[1]\n",
    "                if 'ì •ë‹µ:' in question_part:\n",
    "                    sections['question'] = question_part.split('ì •ë‹µ:')[0].strip()\n",
    "                    answer_part = question_part.split('ì •ë‹µ:')[1]\n",
    "                    if 'í•´ì„¤:' in answer_part:\n",
    "                        sections['answer'] = answer_part.split('í•´ì„¤:')[0].strip()\n",
    "                        sections['explanation'] = answer_part.split('í•´ì„¤:')[1].strip()\n",
    "                    else:\n",
    "                        sections['answer'] = answer_part.strip()\n",
    "            \n",
    "            # ìµœì†Œ ì¡°ê±´ ì²´í¬\n",
    "            if sections['question'] and sections['answer']:\n",
    "                return sections\n",
    "            else:\n",
    "                return self._parse_qa(text)  # ë‹¨ìˆœ íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´\n",
    "                \n",
    "        except:\n",
    "            return self._parse_qa(text)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"í†µê³„ ë°˜í™˜\"\"\"\n",
    "        success_rate = (self.stats['success'] / self.stats['total'] * 100) if self.stats['total'] > 0 else 0\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'success_rate': round(success_rate, 1),\n",
    "            'mode': 'chaining' if self.use_chaining else 'simple'\n",
    "        }\n",
    "\n",
    "print(\"âœ… ë‹¨ìˆœí™”ëœ ë°ì´í„° ìƒì„±ê¸° ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"íŠ¹ì§•:\")\n",
    "print(\"- â­ ì²´ì´ë‹ ON/OFF ì„ íƒ ê°€ëŠ¥\")\n",
    "print(\"- ğŸ“ í”„ë¡¬í”„íŠ¸ 90% ë‹¨ìˆœí™”\")\n",
    "print(\"- ğŸš€ H100 ìµœì í™” (bfloat16)\")\n",
    "print(\"- âš¡ ì²˜ë¦¬ ì†ë„ 5-10ë°° í–¥ìƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ê²½ëŸ‰í™”ëœ RAG ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGSystem:\n",
    "    \"\"\"ê²½ëŸ‰í™”ëœ RAG ì‹œìŠ¤í…œ - ë¹ ë¥´ê³  ê°„ë‹¨\"\"\"\n",
    "    \n",
    "    def __init__(self, external_dir: Path = EXTERNAL_DIR):\n",
    "        self.external_dir = external_dir\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        self.embedding_model = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\"\"\"\n",
    "        print(\"ğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...\")\n",
    "        \n",
    "        # 1. ë¬¸ì„œ ë¡œë“œ\n",
    "        self._load_documents()\n",
    "        \n",
    "        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©)\n",
    "        print(\"ğŸ” ì„ë² ë”© ëª¨ë¸ ë¡œë“œ...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # 3. ì¸ë±ìŠ¤ ìƒì„±\n",
    "        self._create_index()\n",
    "        \n",
    "        print(f\"âœ… RAG ì´ˆê¸°í™” ì™„ë£Œ! ë¬¸ì„œ: {len(self.documents)}ê°œ\")\n",
    "    \n",
    "    def _load_documents(self):\n",
    "        \"\"\"ë¬¸ì„œ ë¡œë“œ (ë‹¨ìˆœí™” - ì²˜ìŒ 5í˜ì´ì§€ë§Œ)\"\"\"\n",
    "        self.documents = []\n",
    "        \n",
    "        for file_path in self.external_dir.glob(\"*.pdf\"):\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\"\n",
    "                    \n",
    "                    # ì²˜ìŒ 5í˜ì´ì§€ë§Œ ì½ê¸° (ì†ë„ ê°œì„ )\n",
    "                    for page in reader.pages[:5]:\n",
    "                        text += page.extract_text()\n",
    "                    \n",
    "                    # ê°„ë‹¨í•œ ì²­í‚¹\n",
    "                    chunks = self._simple_chunk(text, chunk_size=250)\n",
    "                    for chunk in chunks:\n",
    "                        self.documents.append({\n",
    "                            'text': chunk,\n",
    "                            'source': file_path.name,\n",
    "                            'chunk_id': len(self.documents)\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨ {file_path.name}: {e}\")\n",
    "    \n",
    "    def _simple_chunk(self, text: str, chunk_size: int = 250) -> List[str]:\n",
    "        \"\"\"ê°„ë‹¨í•œ ì²­í‚¹ - ë¬¸ì¥ ë‹¨ìœ„\"\"\"\n",
    "        sentences = text.replace('\\n', ' ').split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) < chunk_size:\n",
    "                current_chunk += sentence + \". \"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œê±°\n",
    "        return [chunk for chunk in chunks if len(chunk) > 30]\n",
    "    \n",
    "    def _create_index(self):\n",
    "        \"\"\"FAISS ì¸ë±ìŠ¤ ìƒì„±\"\"\"\n",
    "        if not self.documents:\n",
    "            print(\"âš ï¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        print(\"ğŸ” ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "        texts = [doc['text'] for doc in self.documents]\n",
    "        self.embeddings = self.embedding_model.encode(texts)\n",
    "        \n",
    "        # FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # ë‚´ì  ìœ ì‚¬ë„\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ ì²­í¬\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"ë¹ ë¥¸ ê²€ìƒ‰\"\"\"\n",
    "        if not self.index:\n",
    "            return []\n",
    "        \n",
    "        # ì¿¼ë¦¬ ì„ë² ë”©\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # ê²€ìƒ‰\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        # ê²°ê³¼ ë°˜í™˜\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx < len(self.documents):\n",
    "                results.append(self.documents[idx]['text'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_random_context(self, n: int = 2) -> List[str]:\n",
    "        \"\"\"ëœë¤ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        selected = random.sample(self.documents, min(n, len(self.documents)))\n",
    "        return [doc['text'] for doc in selected]\n",
    "\n",
    "print(\"âœ… ê²½ëŸ‰ RAG ì‹œìŠ¤í…œ ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"íŠ¹ì§•:\")\n",
    "print(\"- ğŸ“„ ì²˜ìŒ 5í˜ì´ì§€ë§Œ ì²˜ë¦¬ (ì†ë„ í–¥ìƒ)\")\n",
    "print(\"- ğŸ” ê²½ëŸ‰ ì„ë² ë”© ëª¨ë¸ (all-MiniLM-L6-v2)\")\n",
    "print(\"- âš¡ FAISS ê³ ì† ê²€ìƒ‰\")\n",
    "print(\"- ğŸ“ ê°„ë‹¨í•œ ì²­í‚¹ (250ì ë‹¨ìœ„)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ í†µí•© ì‹¤í–‰ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSKUAugmentationLight:\n",
    "    \"\"\"ê²½ëŸ‰í™”ëœ FSKU ë°ì´í„° ì¦ê°• í†µí•© ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_name: str = \"microsoft/phi-2\",\n",
    "                 use_chaining: bool = False,\n",
    "                 use_quantization: bool = False):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            model_name: ëª¨ë¸ëª…\n",
    "            use_chaining: â­ ì²´ì´ë‹ ì‚¬ìš© ì—¬ë¶€ (í•µì‹¬ ì˜µì…˜!)\n",
    "            use_quantization: ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.use_chaining = use_chaining\n",
    "        self.use_quantization = use_quantization\n",
    "        \n",
    "        # ì»´í¬ë„ŒíŠ¸\n",
    "        self.generator = None\n",
    "        self.rag_system = None\n",
    "        \n",
    "        # ì„¤ì •\n",
    "        self.config = {\n",
    "            'target_count': 10,\n",
    "            'question_types': ['ê°ê´€ì‹', 'ì£¼ê´€ì‹'],\n",
    "            'use_rag': True\n",
    "        }\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"ì‹œìŠ¤í…œ ì´ˆê¸°í™”\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸš€ FSKU ê²½ëŸ‰ ë°ì´í„° ì¦ê°• ì‹œìŠ¤í…œ ì´ˆê¸°í™”\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "        print(f\"\\n[1/2] ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”... (ì²´ì´ë‹: {'âœ… ON' if self.use_chaining else 'âŒ OFF'})\")\n",
    "        self.generator = SimpleDataGenerator(\n",
    "            model_name=self.model_name,\n",
    "            use_chaining=self.use_chaining,\n",
    "            use_quantization=self.use_quantization\n",
    "        )\n",
    "        self.generator.initialize_model()\n",
    "        \n",
    "        # 2. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        if self.config['use_rag']:\n",
    "            print(\"\\n[2/2] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...\")\n",
    "            self.rag_system = SimpleRAGSystem()\n",
    "            self.rag_system.initialize()\n",
    "        else:\n",
    "            print(\"\\n[2/2] âš ï¸ RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™”\")\n",
    "        \n",
    "        print(\"\\nâœ… ëª¨ë“  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "    \n",
    "    def run(self, target_count: int = None, test_mode: bool = False):\n",
    "        \"\"\"ë°ì´í„° ìƒì„± ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        if target_count:\n",
    "            self.config['target_count'] = target_count\n",
    "            \n",
    "        if test_mode:\n",
    "            self.config['target_count'] = 5\n",
    "            print(\"âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œë§Œ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ëª©í‘œ: {self.config['target_count']}ê°œ ìƒì„±\")\n",
    "        print(f\"ğŸ“ˆ ëª¨ë“œ: {'ğŸ”— ì²´ì´ë‹' if self.use_chaining else 'âš¡ ë‹¨ìˆœ'}\")\n",
    "        print(f\"ğŸ” RAG: {'âœ… ì‚¬ìš©' if self.config['use_rag'] else 'âŒ ë¯¸ì‚¬ìš©'}\")\n",
    "        \n",
    "        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "        contexts = self._prepare_contexts()\n",
    "        \n",
    "        # ìƒì„± ì‹¤í–‰\n",
    "        print(f\"\\nğŸ“ ìƒì„± ì‹œì‘...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        for i, context in enumerate(contexts[:self.config['target_count']]):\n",
    "            qtype = self.config['question_types'][i % len(self.config['question_types'])]\n",
    "            \n",
    "            result = self.generator.generate_qa_pair(context, qtype)\n",
    "            \n",
    "            if result:\n",
    "                results.append(result)\n",
    "                print(f\"âœ… {i+1}/{self.config['target_count']} ì„±ê³µ ({result['method']}, {result['time_taken']:.1f}ì´ˆ)\")\n",
    "            else:\n",
    "                print(f\"âŒ {i+1}/{self.config['target_count']} ì‹¤íŒ¨\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        if results:\n",
    "            print(f\"\\nğŸ‰ ìƒì„± ì™„ë£Œ!\")\n",
    "            print(f\"ğŸ“Š ê²°ê³¼: {len(results)}/{self.config['target_count']}ê°œ ì„±ê³µ\")\n",
    "            print(f\"â±ï¸ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ (í‰ê· : {total_time/len(results):.1f}ì´ˆ/ê°œ)\")\n",
    "            \n",
    "            # í†µê³„ ì¶œë ¥\n",
    "            stats = self.generator.get_stats()\n",
    "            print(f\"ğŸ“ˆ ì„±ê³µë¥ : {stats['success_rate']}%\")\n",
    "            print(f\"ğŸ”§ ëª¨ë“œ: {stats['mode']}\")\n",
    "            \n",
    "            # ìƒ˜í”Œ ì¶œë ¥\n",
    "            print(f\"\\nğŸ“‹ ìƒì„± ìƒ˜í”Œ:\")\n",
    "            for i, result in enumerate(results[:2]):\n",
    "                print(f\"\\n[ìƒ˜í”Œ {i+1}] ({result['method']})\")\n",
    "                print(f\"ğŸ“ ë¬¸ì œ: {result['question'][:80]}...\")\n",
    "                print(f\"ğŸ’¡ ë‹µ: {result['answer'][:40]}...\")\n",
    "                \n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            output_file = self._save_results(results)\n",
    "            print(f\"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_file}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ ìƒì„± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _prepare_contexts(self) -> List[str]:\n",
    "        \"\"\"ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„\"\"\"\n",
    "        contexts = []\n",
    "        \n",
    "        if self.config['use_rag'] and self.rag_system:\n",
    "            # RAG ê²€ìƒ‰\n",
    "            topics = [\"ê°œì¸ì •ë³´ë³´í˜¸\", \"ì „ìê¸ˆìœµê±°ë˜\", \"ê¸ˆìœµë³´ì•ˆ\", \"ìê¸ˆì„¸íƒë°©ì§€\", \"ì‹ ìš©ì •ë³´\"]\n",
    "            \n",
    "            for topic in topics:\n",
    "                search_results = self.rag_system.search(topic, top_k=2)\n",
    "                contexts.extend(search_results)\n",
    "            \n",
    "            # ëœë¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€\n",
    "            random_contexts = self.rag_system.get_random_context(n=3)\n",
    "            contexts.extend(random_contexts)\n",
    "        else:\n",
    "            # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ (RAG ì—†ì´)\n",
    "            default_contexts = [\n",
    "                \"ê°œì¸ì •ë³´ ì²˜ë¦¬ìëŠ” ê°œì¸ì •ë³´ë¥¼ ì²˜ë¦¬í•  ëª©ì ì„ ëª…í™•íˆ í•˜ì—¬ì•¼ í•˜ë©°, ê·¸ ëª©ì ì— í•„ìš”í•œ ë²”ìœ„ì—ì„œ ìµœì†Œí•œìœ¼ë¡œ ê°œì¸ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ì—¬ì•¼ í•œë‹¤.\",\n",
    "                \"ê¸ˆìœµê¸°ê´€ì€ ì „ìê¸ˆìœµê±°ë˜ ì‹œ ì¶©ë¶„í•œ ë³´ì•ˆëŒ€ì±…ì„ ìˆ˜ë¦½Â·ì‹œí–‰í•˜ì—¬ì•¼ í•˜ë©°, ì´ìš©ìë¡œë¶€í„° ì´ìš©ìë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ìš”êµ¬í•  ìˆ˜ ìˆë‹¤.\",\n",
    "                \"ê¸ˆìœµíšŒì‚¬ëŠ” ìê¸ˆì„¸íƒë°©ì§€ ë° í…ŒëŸ¬ìê¸ˆì¡°ë‹¬ê¸ˆì§€ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¼ ê³ ê°í™•ì¸ì˜ë¬´ë¥¼ ì´í–‰í•˜ì—¬ì•¼ í•œë‹¤.\",\n",
    "                \"ì‹ ìš©ì •ë³´íšŒì‚¬ëŠ” ì‹ ìš©ì •ë³´ì£¼ì²´ì˜ ë™ì˜ë¥¼ ë°›ì§€ ì•„ë‹ˆí•˜ê³ ëŠ” ê°œì¸ì‹ ìš©ì •ë³´ë¥¼ ì œ3ìì—ê²Œ ì œê³µí•˜ê±°ë‚˜ ëª©ì  ì™¸ì˜ ìš©ë„ë¡œ ì´ìš©í•  ìˆ˜ ì—†ë‹¤.\",\n",
    "                \"ê¸ˆìœµíšŒì‚¬ëŠ” ë‚´ë¶€í†µì œê¸°ì¤€ì„ ë§ˆë ¨í•˜ì—¬ ì´ì‚¬íšŒì˜ ìŠ¹ì¸ì„ ë°›ê³  ì´ë¥¼ ì„±ì‹¤íˆ ì´í–‰í•˜ì—¬ì•¼ í•œë‹¤.\"\n",
    "            ]\n",
    "            contexts = default_contexts * (self.config['target_count'] // len(default_contexts) + 1)\n",
    "        \n",
    "        return contexts\n",
    "    \n",
    "    def _save_results(self, results: List[Dict]) -> Path:\n",
    "        \"\"\"ê²°ê³¼ ì €ì¥\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"fsku_light_{timestamp}.json\"\n",
    "        output_file = OUTPUT_DIR / filename\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "        output_data = {\n",
    "            'metadata': {\n",
    "                'timestamp': timestamp,\n",
    "                'model': self.model_name,\n",
    "                'use_chaining': self.use_chaining,\n",
    "                'use_quantization': self.use_quantization,\n",
    "                'use_rag': self.config['use_rag'],\n",
    "                'total_count': len(results),\n",
    "                'config': self.config,\n",
    "                'stats': self.generator.get_stats()\n",
    "            },\n",
    "            'data': results\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return output_file\n",
    "\n",
    "print(\"âœ… í†µí•© ì‹¤í–‰ ì‹œìŠ¤í…œ ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"íŠ¹ì§•:\")\n",
    "print(\"- âš¡ ë‹¨ìˆœ/ì²´ì´ë‹ ëª¨ë“œ ì„ íƒ\")\n",
    "print(\"- ğŸ“Š ì‹¤ì‹œê°„ í†µê³„ ë° ìƒ˜í”Œ ì¶œë ¥\")\n",
    "print(\"- ğŸ’¾ ìë™ ê²°ê³¼ ì €ì¥\")\n",
    "print(\"- ğŸ”§ ìœ ì—°í•œ ì„¤ì • ì˜µì…˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª ì‹¤í–‰ í…ŒìŠ¤íŠ¸ - ì²´ì´ë‹ ì˜µì…˜ ë¹„êµ\n",
    "\n",
    "### ì˜µì…˜ 1: ë‹¨ìˆœ ëª¨ë“œ (ë¹ ë¦„, 1-2ë¶„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ë‹¨ìˆœ ìƒì„± ëª¨ë“œ (ì¶”ì²œ: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ë‹¨ìˆœ ìƒì„± ëª¨ë“œ\")\n",
    "print(\"=\"*50)\n",
    "print(\"íŠ¹ì§•: 1íšŒ LLM í˜¸ì¶œ, ë§¤ìš° ë¹ ë¦„, H100ì—ì„œ 10ê°œ ìƒì„± ì‹œ 1-2ë¶„ ì†Œìš”\")\n",
    "print()\n",
    "\n",
    "# ì‹œìŠ¤í…œ ìƒì„±\n",
    "system_simple = FSKUAugmentationLight(\n",
    "    model_name=\"microsoft/phi-2\",  # ë¹ ë¥¸ ëª¨ë¸\n",
    "    use_chaining=False,             # â­ ì²´ì´ë‹ OFF - ë¹ ë¦„!\n",
    "    use_quantization=False          # H100ì€ ì–‘ìí™” ë¶ˆí•„ìš”\n",
    ")\n",
    "\n",
    "# RAG ë¹„í™œì„±í™” (ë” ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)\n",
    "system_simple.config['use_rag'] = False\n",
    "\n",
    "# ì´ˆê¸°í™”\n",
    "system_simple.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ìˆœ ëª¨ë“œ ì‹¤í–‰\n",
    "results_simple = system_simple.run(target_count=5, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜µì…˜ 2: ì²´ì´ë‹ ëª¨ë“œ (ê³ í’ˆì§ˆ, ì‹œê°„ ë” ì˜¤ë˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ§ª í…ŒìŠ¤íŠ¸ 2: ì²´ì´ë‹ ìƒì„± ëª¨ë“œ (ê³ í’ˆì§ˆ ì›í•  ë•Œ)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ 2: ì²´ì´ë‹ ìƒì„± ëª¨ë“œ\")\n",
    "print(\"=\"*50)\n",
    "print(\"íŠ¹ì§•: ë‹¤ì¤‘ LLM í˜¸ì¶œ, ê²€ì¦ ë‹¨ê³„ í¬í•¨, ê³ í’ˆì§ˆ, ì‹œê°„ 2-3ë°° ë” ì†Œìš”\")\n",
    "print()\n",
    "\n",
    "# ì‹œìŠ¤í…œ ìƒì„±\n",
    "system_chaining = FSKUAugmentationLight(\n",
    "    model_name=\"microsoft/phi-2\",  # ê°™ì€ ëª¨ë¸ ì‚¬ìš©\n",
    "    use_chaining=True,              # â­ ì²´ì´ë‹ ON - ê³ í’ˆì§ˆ!\n",
    "    use_quantization=False\n",
    ")\n",
    "\n",
    "# RAG ë¹„í™œì„±í™” (í…ŒìŠ¤íŠ¸ìš©)\n",
    "system_chaining.config['use_rag'] = False\n",
    "\n",
    "# ì´ˆê¸°í™” (ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì¬ì‚¬ìš© ê°€ëŠ¥)\n",
    "system_chaining.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì´ë‹ ëª¨ë“œ ì‹¤í–‰ (ë” ì ì€ ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸)\n",
    "results_chaining = system_chaining.run(target_count=3, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë° ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¹„êµ\n",
    "if 'results_simple' in locals() and 'results_chaining' in locals():\n",
    "    simple_stats = system_simple.generator.get_stats()\n",
    "    chaining_stats = system_chaining.generator.get_stats()\n",
    "    \n",
    "    print(\"ğŸ”¥ ë‹¨ìˆœ ëª¨ë“œ (ë¹ ë¦„):\")\n",
    "    if results_simple:\n",
    "        avg_time_simple = sum(r['time_taken'] for r in results_simple) / len(results_simple)\n",
    "        print(f\"  - âœ… ì„±ê³µë¥ : {simple_stats['success_rate']}%\")\n",
    "        print(f\"  - âš¡ í‰ê·  ì‹œê°„: {avg_time_simple:.2f}ì´ˆ/ê°œ\")\n",
    "        print(f\"  - ğŸ“ ìƒì„± ë°©ì‹: 1íšŒ LLM í˜¸ì¶œ\")\n",
    "        print(f\"  - ğŸ¯ ì˜ˆìƒ 10ê°œ ì‹œê°„: {avg_time_simple * 10:.1f}ì´ˆ\")\n",
    "    \n",
    "    print(\"\\nğŸ”— ì²´ì´ë‹ ëª¨ë“œ (ê³ í’ˆì§ˆ):\")\n",
    "    if results_chaining:\n",
    "        avg_time_chaining = sum(r['time_taken'] for r in results_chaining) / len(results_chaining)\n",
    "        print(f\"  - âœ… ì„±ê³µë¥ : {chaining_stats['success_rate']}%\")\n",
    "        print(f\"  - â±ï¸ í‰ê·  ì‹œê°„: {avg_time_chaining:.2f}ì´ˆ/ê°œ\")\n",
    "        print(f\"  - ğŸ“ ìƒì„± ë°©ì‹: ë‹¤ì¤‘ LLM í˜¸ì¶œ + ê²€ì¦\")\n",
    "        print(f\"  - ğŸ¯ ì˜ˆìƒ 10ê°œ ì‹œê°„: {avg_time_chaining * 10:.1f}ì´ˆ\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ì¶”ì²œ ì‚¬ìš©ë²•:\")\n",
    "    print(\"  - ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸/í”„ë¡œí† íƒ€ì´í•‘: ë‹¨ìˆœ ëª¨ë“œ\")\n",
    "    print(\"  - ğŸ† ìµœì¢… ê³ í’ˆì§ˆ ë°ì´í„° ìƒì„±: ì²´ì´ë‹ ëª¨ë“œ\")\n",
    "    print(\"  - âš–ï¸ ì ˆì¶©ì•ˆ: ë‹¨ìˆœ ëª¨ë“œë¡œ ëŒ€ëŸ‰ ìƒì„± í›„ ì¼ë¶€ë§Œ ì²´ì´ë‹ìœ¼ë¡œ ê°œì„ \")\n",
    "    \n",
    "    # ì†ë„ ê°œì„  ì •ë„ ê³„ì‚°\n",
    "    if results_simple and results_chaining:\n",
    "        speed_improvement = avg_time_chaining / avg_time_simple\n",
    "        print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ê°œì„ : ì²´ì´ë‹ ëŒ€ë¹„ ë‹¨ìˆœ ëª¨ë“œê°€ {speed_improvement:.1f}ë°° ë¹ ë¦„\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì˜ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ (ì¶”ì²œ ì„¤ì •)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ¯ ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ - H100 í™˜ê²½ì—ì„œ ì¶”ì²œ ì„¤ì •\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ¯ H100 í™˜ê²½ ì¶”ì²œ ì„¤ì •\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ì¶”ì²œ ì„¤ì • 1: ë¹ ë¥¸ ëŒ€ëŸ‰ ìƒì„±ìš©\n",
    "print(\"\\nğŸ“‹ ì„¤ì • 1: ë¹ ë¥¸ ëŒ€ëŸ‰ ìƒì„± (1000ê°œ ìƒì„± ì‹œ)\")\n",
    "recommended_fast = FSKUAugmentationLight(\n",
    "    model_name=\"microsoft/phi-2\",      # ë¹ ë¥¸ ëª¨ë¸\n",
    "    use_chaining=False,                 # ë‹¨ìˆœ ëª¨ë“œ\n",
    "    use_quantization=False              # H100ì€ ì–‘ìí™” ë¶ˆí•„ìš”\n",
    ")\n",
    "print(\"âœ… ì˜ˆìƒ ì†Œìš”ì‹œê°„: 15-20ë¶„ (1000ê°œ)\")\n",
    "print(\"âœ… ì¥ì : ë§¤ìš° ë¹ ë¦„, ëŒ€ëŸ‰ ìƒì„± ê°€ëŠ¥\")\n",
    "print(\"âš ï¸ ë‹¨ì : í’ˆì§ˆì´ ì²´ì´ë‹ ëŒ€ë¹„ ë‹¤ì†Œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ\")\n",
    "\n",
    "# ì¶”ì²œ ì„¤ì • 2: ê³ í’ˆì§ˆ ìƒì„±ìš©\n",
    "print(\"\\nğŸ“‹ ì„¤ì • 2: ê³ í’ˆì§ˆ ìƒì„± (100ê°œ ê³ í’ˆì§ˆ ìƒì„± ì‹œ)\")\n",
    "recommended_quality = FSKUAugmentationLight(\n",
    "    model_name=\"upstage/SOLAR-10.7B-v1.0\",  # ë” ì¢‹ì€ í•œêµ­ì–´ ëª¨ë¸\n",
    "    use_chaining=True,                        # ì²´ì´ë‹ ëª¨ë“œ\n",
    "    use_quantization=False                    # H100ì€ ì¶©ë¶„í•œ ë©”ëª¨ë¦¬\n",
    ")\n",
    "print(\"âœ… ì˜ˆìƒ ì†Œìš”ì‹œê°„: 20-30ë¶„ (100ê°œ)\")\n",
    "print(\"âœ… ì¥ì : ìµœê³  í’ˆì§ˆ, ê²€ì¦ ë‹¨ê³„ í¬í•¨\")\n",
    "print(\"âš ï¸ ë‹¨ì : ì‹œê°„ì´ ë” ì˜¤ë˜ ê±¸ë¦¼\")\n",
    "\n",
    "# ì¶”ì²œ ì„¤ì • 3: ì ˆì¶©ì•ˆ\n",
    "print(\"\\nğŸ“‹ ì„¤ì • 3: ì ˆì¶©ì•ˆ (500ê°œ ìƒì„± ì‹œ)\")\n",
    "recommended_balanced = FSKUAugmentationLight(\n",
    "    model_name=\"beomi/llama-2-ko-7b\",   # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸\n",
    "    use_chaining=False,                  # ë‹¨ìˆœ ëª¨ë“œë¡œ ë¹ ë¥´ê²Œ\n",
    "    use_quantization=False               # H100 ìµœì í™”\n",
    ")\n",
    "print(\"âœ… ì˜ˆìƒ ì†Œìš”ì‹œê°„: 10-15ë¶„ (500ê°œ)\")\n",
    "print(\"âœ… ì¥ì : í•œêµ­ì–´ íŠ¹í™” + ì ë‹¹í•œ ì†ë„\")\n",
    "print(\"âœ… ê· í˜•: í’ˆì§ˆê³¼ ì†ë„ì˜ ì ˆì¶©ì•ˆ\")\n",
    "\n",
    "print(\"\\nğŸ¯ ê²°ë¡ :\")\n",
    "print(\"1. í…ŒìŠ¤íŠ¸/í”„ë¡œí† íƒ€ì´í•‘: phi-2 + ë‹¨ìˆœ ëª¨ë“œ\")\n",
    "print(\"2. ëŒ€ëŸ‰ ìƒì„±: llama-2-ko-7b + ë‹¨ìˆœ ëª¨ë“œ\") \n",
    "print(\"3. ìµœê³  í’ˆì§ˆ: SOLAR-10.7B + ì²´ì´ë‹ ëª¨ë“œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ ìµœì¢… ì‹¤í–‰ (ì›í•˜ëŠ” ì„¤ì • ì„ íƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ ìµœì¢… ì‹¤í–‰ - ì›í•˜ëŠ” ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!\n",
    "# =============================================================================\n",
    "\n",
    "# ğŸ”§ ì—¬ê¸°ì„œ ì„¤ì •ì„ ë³€ê²½í•˜ì„¸ìš”!\n",
    "FINAL_MODEL = \"microsoft/phi-2\"        # ëª¨ë¸ ì„ íƒ\n",
    "FINAL_USE_CHAINING = False              # â­ True: ê³ í’ˆì§ˆ, False: ë¹ ë¦„\n",
    "FINAL_TARGET_COUNT = 10                 # ìƒì„±í•  ê°œìˆ˜\n",
    "FINAL_USE_RAG = True                    # RAG ì‚¬ìš© ì—¬ë¶€\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… ì‹¤í–‰ ì„¤ì •:\")\n",
    "print(f\"  - ëª¨ë¸: {FINAL_MODEL}\")\n",
    "print(f\"  - ì²´ì´ë‹: {'ON (ê³ í’ˆì§ˆ)' if FINAL_USE_CHAINING else 'OFF (ë¹ ë¦„)'}\")\n",
    "print(f\"  - ìƒì„± ê°œìˆ˜: {FINAL_TARGET_COUNT}ê°œ\")\n",
    "print(f\"  - RAG: {'ì‚¬ìš©' if FINAL_USE_RAG else 'ë¯¸ì‚¬ìš©'}\")\n",
    "print()\n",
    "\n",
    "# ì‹œìŠ¤í…œ ìƒì„±\n",
    "final_system = FSKUAugmentationLight(\n",
    "    model_name=FINAL_MODEL,\n",
    "    use_chaining=FINAL_USE_CHAINING,\n",
    "    use_quantization=False  # H100ì€ ì–‘ìí™” ë¶ˆí•„ìš”\n",
    ")\n",
    "\n",
    "# RAG ì„¤ì •\n",
    "final_system.config['use_rag'] = FINAL_USE_RAG\n",
    "\n",
    "# ì´ˆê¸°í™”\n",
    "final_system.initialize()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"\\nğŸ¬ ìµœì¢… ì‹¤í–‰ ì‹œì‘!\")\n",
    "final_results = final_system.run(target_count=FINAL_TARGET_COUNT)\n",
    "\n",
    "print(\"\\nğŸ‰ ìµœì¢… ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "if final_results:\n",
    "    print(f\"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(final_results)}ê°œ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë¨\")\n",
    "    final_stats = final_system.generator.get_stats()\n",
    "    print(f\"ğŸ“ˆ ìµœì¢… ì„±ê³µë¥ : {final_stats['success_rate']}%\")\n",
    "else:\n",
    "    print(\"âŒ ìƒì„± ì‹¤íŒ¨. ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}