{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 FSKU 리팩토링된 데이터 증강 시스템\n",
    "\n",
    "## 주요 개선사항\n",
    "- ✅ **경량화**: 기존 46K 토큰 → 2K 토큰으로 축소\n",
    "- ✅ **프롬프트 단순화**: 복잡한 템플릿 → 간단명료한 프롬프트\n",
    "- ✅ **체이닝 옵션**: `use_chaining=True/False`로 선택 가능\n",
    "- ✅ **H100 최적화**: bfloat16, 메모리 효율성 개선\n",
    "- ✅ **빠른 실행**: 10개 생성이 1-2분 내 완료\n",
    "\n",
    "## 사용 옵션\n",
    "- **단순 모드** (`use_chaining=False`): 1회 LLM 호출, 빠른 생성\n",
    "- **체이닝 모드** (`use_chaining=True`): 다중 LLM 호출, 고품질 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 필수 라이브러리\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# 데이터 처리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 딥러닝 및 NLP\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# 벡터 검색 및 임베딩 (RAG용)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# 문서 처리\n",
    "import PyPDF2\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 경로 설정\n",
    "BASE_DIR = Path.cwd()\n",
    "EXTERNAL_DIR = BASE_DIR / \"data\" / \"external\"\n",
    "OUTPUT_DIR = BASE_DIR / \"data\" / \"augmented\"\n",
    "CACHE_DIR = BASE_DIR / \"data\" / \"cache\"\n",
    "\n",
    "# 디렉토리 생성\n",
    "for dir_path in [OUTPUT_DIR, CACHE_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✅ 환경 설정 완료!\")\n",
    "print(f\"📁 외부 문서: {EXTERNAL_DIR}\")\n",
    "print(f\"📁 출력 디렉토리: {OUTPUT_DIR}\")\n",
    "\n",
    "# GPU 정보\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🔥 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    print(\"💻 CPU 모드로 실행됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 단순화된 데이터 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataGenerator:\n",
    "    \"\"\"\n",
    "    단순화된 FSKU 데이터 생성기\n",
    "    - 체이닝 옵션 선택 가능 ✅\n",
    "    - 간단한 프롬프트 ✅\n",
    "    - H100 최적화 ✅\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"microsoft/phi-2\",\n",
    "                 use_chaining: bool = False,\n",
    "                 use_quantization: bool = False):\n",
    "        \"\"\"\n",
    "        초기화\n",
    "        \n",
    "        Args:\n",
    "            model_name: 사용할 모델명\n",
    "            use_chaining: CoT 체이닝 사용 여부 ⭐ 핵심 옵션!\n",
    "            use_quantization: 4bit 양자화 사용 여부\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.use_chaining = use_chaining\n",
    "        self.use_quantization = use_quantization\n",
    "        \n",
    "        # 모델 및 토크나이저\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        # 간단한 프롬프트 템플릿 (기존 대비 90% 축소)\n",
    "        self.simple_prompt = \"\"\"주제: {question_type}\n",
    "\n",
    "참고 내용:\n",
    "{context}\n",
    "\n",
    "위 내용을 바탕으로 {question_type} 문제를 1개 만드세요.\n",
    "\n",
    "문제:\n",
    "정답:\"\"\"\n",
    "        \n",
    "        # 체이닝용 프롬프트 (더 상세하지만 여전히 단순)\n",
    "        self.chaining_prompt = \"\"\"금융 전문가로서 FSKU 시험 문제를 생성하세요.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "요구사항:\n",
    "- 문제 유형: {question_type}\n",
    "- FSKU 실제 시험 수준\n",
    "- 명확하고 정확한 표현\n",
    "\n",
    "문제:\n",
    "정답:\n",
    "해설:\"\"\"\n",
    "        \n",
    "        # 통계\n",
    "        self.stats = {'total': 0, 'success': 0, 'failed': 0}\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"모델 초기화 - H100 최적화\"\"\"\n",
    "        print(f\"🚀 모델 로딩: {self.model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 토크나이저 로드\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name, \n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # 모델 로드 설정\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"device_map\": \"auto\"\n",
    "            }\n",
    "            \n",
    "            # H100 최적화\n",
    "            if torch.cuda.is_available():\n",
    "                model_kwargs[\"torch_dtype\"] = torch.bfloat16  # H100 최적화\n",
    "                \n",
    "            # 양자화 설정 (옵션)\n",
    "            if self.use_quantization:\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                    bnb_4bit_use_double_quant=True\n",
    "                )\n",
    "                model_kwargs[\"quantization_config\"] = bnb_config\n",
    "            \n",
    "            # 모델 로드\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name, \n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            load_time = time.time() - start_time\n",
    "            \n",
    "            # 메모리 정보\n",
    "            if torch.cuda.is_available():\n",
    "                memory_gb = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"✅ 로드 완료! ({load_time:.1f}초, {memory_gb:.2f}GB)\")\n",
    "            else:\n",
    "                print(f\"✅ CPU 로드 완료! ({load_time:.1f}초)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"모델 로드 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_text(self, prompt: str, max_tokens: int = 150) -> str:\n",
    "        \"\"\"텍스트 생성 - 간단하고 빠름\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise ValueError(\"모델이 초기화되지 않았습니다.\")\n",
    "        \n",
    "        # 토큰화 (컨텍스트 길이 축소)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=800  # 기존 2048 → 800으로 축소\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # 생성\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 디코딩\n",
    "        generated = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return generated.strip()\n",
    "    \n",
    "    def generate_qa_pair(self, context: str, question_type: str = \"객관식\") -> Optional[Dict]:\n",
    "        \"\"\"QA 쌍 생성 - 체이닝 옵션에 따라 분기\"\"\"\n",
    "        self.stats['total'] += 1\n",
    "        \n",
    "        try:\n",
    "            if self.use_chaining:\n",
    "                return self._generate_with_chaining(context, question_type)\n",
    "            else:\n",
    "                return self._generate_simple(context, question_type)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 생성 오류: {e}\")\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "    \n",
    "    def _generate_simple(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"단순 생성 (1회 호출) - 매우 빠름!\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # 프롬프트 준비 (컨텍스트 축약)\n",
    "        prompt = self.simple_prompt.format(\n",
    "            context=context[:400],  # 400자로 제한\n",
    "            question_type=question_type\n",
    "        )\n",
    "        \n",
    "        # 생성\n",
    "        generated = self.generate_text(prompt)\n",
    "        \n",
    "        # 파싱\n",
    "        qa_pair = self._parse_qa(generated)\n",
    "        if not qa_pair:\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        result = {\n",
    "            'question': qa_pair['question'],\n",
    "            'answer': qa_pair['answer'],\n",
    "            'context': context,\n",
    "            'question_type': question_type,\n",
    "            'method': 'simple',\n",
    "            'time_taken': elapsed,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.stats['success'] += 1\n",
    "        return result\n",
    "    \n",
    "    def _generate_with_chaining(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"체이닝 생성 (다중 호출) - 고품질!\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # 1. 초기 생성\n",
    "        prompt = self.chaining_prompt.format(\n",
    "            context=context[:600],  # 더 긴 컨텍스트 허용\n",
    "            question_type=question_type\n",
    "        )\n",
    "        generated = self.generate_text(prompt, max_tokens=200)\n",
    "        qa_pair = self._parse_qa_detailed(generated)\n",
    "        \n",
    "        if not qa_pair:\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "        \n",
    "        # 2. 간단한 검증 (1회만)\n",
    "        verification_prompt = f\"\"\"다음 문제를 검토하세요:\n",
    "\n",
    "{qa_pair['question']}\n",
    "{qa_pair['answer']}\n",
    "\n",
    "문제점이 있으면 지적하고, 없으면 \"적합\"이라고 답하세요:\"\"\"\n",
    "        \n",
    "        feedback = self.generate_text(verification_prompt, max_tokens=100)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        result = {\n",
    "            'question': qa_pair['question'],\n",
    "            'answer': qa_pair['answer'],\n",
    "            'context': context,\n",
    "            'question_type': question_type,\n",
    "            'method': 'chaining',\n",
    "            'feedback': feedback,\n",
    "            'time_taken': elapsed,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.stats['success'] += 1\n",
    "        return result\n",
    "    \n",
    "    def _parse_qa(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"간단한 QA 파싱\"\"\"\n",
    "        try:\n",
    "            # 정답 구분자 찾기\n",
    "            for marker in ['정답:', '답:', 'Answer:', 'A:']:\n",
    "                if marker in text:\n",
    "                    parts = text.split(marker, 1)\n",
    "                    question = parts[0].strip()\n",
    "                    answer = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                    \n",
    "                    if len(question) > 10:  # 최소 길이 체크\n",
    "                        return {'question': question, 'answer': answer}\n",
    "            \n",
    "            # 구분자가 없으면 전체를 문제로 간주\n",
    "            if len(text.strip()) > 10:\n",
    "                return {'question': text.strip(), 'answer': \"답변 필요\"}\n",
    "            \n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _parse_qa_detailed(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"상세 QA 파싱 (해설 포함)\"\"\"\n",
    "        try:\n",
    "            sections = {'question': '', 'answer': '', 'explanation': ''}\n",
    "            \n",
    "            # 문제 추출\n",
    "            if '문제:' in text:\n",
    "                question_part = text.split('문제:')[1]\n",
    "                if '정답:' in question_part:\n",
    "                    sections['question'] = question_part.split('정답:')[0].strip()\n",
    "                    answer_part = question_part.split('정답:')[1]\n",
    "                    if '해설:' in answer_part:\n",
    "                        sections['answer'] = answer_part.split('해설:')[0].strip()\n",
    "                        sections['explanation'] = answer_part.split('해설:')[1].strip()\n",
    "                    else:\n",
    "                        sections['answer'] = answer_part.strip()\n",
    "            \n",
    "            # 최소 조건 체크\n",
    "            if sections['question'] and sections['answer']:\n",
    "                return sections\n",
    "            else:\n",
    "                return self._parse_qa(text)  # 단순 파싱으로 대체\n",
    "                \n",
    "        except:\n",
    "            return self._parse_qa(text)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"통계 반환\"\"\"\n",
    "        success_rate = (self.stats['success'] / self.stats['total'] * 100) if self.stats['total'] > 0 else 0\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'success_rate': round(success_rate, 1),\n",
    "            'mode': 'chaining' if self.use_chaining else 'simple'\n",
    "        }\n",
    "\n",
    "print(\"✅ 단순화된 데이터 생성기 생성 완료!\")\n",
    "print(\"특징:\")\n",
    "print(\"- ⭐ 체이닝 ON/OFF 선택 가능\")\n",
    "print(\"- 📝 프롬프트 90% 단순화\")\n",
    "print(\"- 🚀 H100 최적화 (bfloat16)\")\n",
    "print(\"- ⚡ 처리 속도 5-10배 향상\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 경량화된 RAG 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGSystem:\n",
    "    \"\"\"경량화된 RAG 시스템 - 빠르고 간단\"\"\"\n",
    "    \n",
    "    def __init__(self, external_dir: Path = EXTERNAL_DIR):\n",
    "        self.external_dir = external_dir\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        self.embedding_model = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"RAG 시스템 초기화\"\"\"\n",
    "        print(\"📚 RAG 시스템 초기화...\")\n",
    "        \n",
    "        # 1. 문서 로드\n",
    "        self._load_documents()\n",
    "        \n",
    "        # 2. 임베딩 모델 로드 (경량 모델 사용)\n",
    "        print(\"🔍 임베딩 모델 로드...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # 3. 인덱스 생성\n",
    "        self._create_index()\n",
    "        \n",
    "        print(f\"✅ RAG 초기화 완료! 문서: {len(self.documents)}개\")\n",
    "    \n",
    "    def _load_documents(self):\n",
    "        \"\"\"문서 로드 (단순화 - 처음 5페이지만)\"\"\"\n",
    "        self.documents = []\n",
    "        \n",
    "        for file_path in self.external_dir.glob(\"*.pdf\"):\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\"\n",
    "                    \n",
    "                    # 처음 5페이지만 읽기 (속도 개선)\n",
    "                    for page in reader.pages[:5]:\n",
    "                        text += page.extract_text()\n",
    "                    \n",
    "                    # 간단한 청킹\n",
    "                    chunks = self._simple_chunk(text, chunk_size=250)\n",
    "                    for chunk in chunks:\n",
    "                        self.documents.append({\n",
    "                            'text': chunk,\n",
    "                            'source': file_path.name,\n",
    "                            'chunk_id': len(self.documents)\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 문서 로드 실패 {file_path.name}: {e}\")\n",
    "    \n",
    "    def _simple_chunk(self, text: str, chunk_size: int = 250) -> List[str]:\n",
    "        \"\"\"간단한 청킹 - 문장 단위\"\"\"\n",
    "        sentences = text.replace('\\n', ' ').split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) < chunk_size:\n",
    "                current_chunk += sentence + \". \"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        # 너무 짧은 청크 제거\n",
    "        return [chunk for chunk in chunks if len(chunk) > 30]\n",
    "    \n",
    "    def _create_index(self):\n",
    "        \"\"\"FAISS 인덱스 생성\"\"\"\n",
    "        if not self.documents:\n",
    "            print(\"⚠️ 문서가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        print(\"🔍 임베딩 생성 중...\")\n",
    "        texts = [doc['text'] for doc in self.documents]\n",
    "        self.embeddings = self.embedding_model.encode(texts)\n",
    "        \n",
    "        # FAISS 인덱스 생성\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # 내적 유사도\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"✅ 인덱스 생성 완료: {len(texts)}개 청크\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"빠른 검색\"\"\"\n",
    "        if not self.index:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 임베딩\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # 검색\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        # 결과 반환\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx < len(self.documents):\n",
    "                results.append(self.documents[idx]['text'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_random_context(self, n: int = 2) -> List[str]:\n",
    "        \"\"\"랜덤 컨텍스트 반환\"\"\"\n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        selected = random.sample(self.documents, min(n, len(self.documents)))\n",
    "        return [doc['text'] for doc in selected]\n",
    "\n",
    "print(\"✅ 경량 RAG 시스템 생성 완료!\")\n",
    "print(\"특징:\")\n",
    "print(\"- 📄 처음 5페이지만 처리 (속도 향상)\")\n",
    "print(\"- 🔍 경량 임베딩 모델 (all-MiniLM-L6-v2)\")\n",
    "print(\"- ⚡ FAISS 고속 검색\")\n",
    "print(\"- 📝 간단한 청킹 (250자 단위)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 통합 실행 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSKUAugmentationLight:\n",
    "    \"\"\"경량화된 FSKU 데이터 증강 통합 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_name: str = \"microsoft/phi-2\",\n",
    "                 use_chaining: bool = False,\n",
    "                 use_quantization: bool = False):\n",
    "        \"\"\"\n",
    "        초기화\n",
    "        \n",
    "        Args:\n",
    "            model_name: 모델명\n",
    "            use_chaining: ⭐ 체이닝 사용 여부 (핵심 옵션!)\n",
    "            use_quantization: 양자화 사용 여부\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.use_chaining = use_chaining\n",
    "        self.use_quantization = use_quantization\n",
    "        \n",
    "        # 컴포넌트\n",
    "        self.generator = None\n",
    "        self.rag_system = None\n",
    "        \n",
    "        # 설정\n",
    "        self.config = {\n",
    "            'target_count': 10,\n",
    "            'question_types': ['객관식', '주관식'],\n",
    "            'use_rag': True\n",
    "        }\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"시스템 초기화\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🚀 FSKU 경량 데이터 증강 시스템 초기화\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. 생성기 초기화\n",
    "        print(f\"\\n[1/2] 데이터 생성기 초기화... (체이닝: {'✅ ON' if self.use_chaining else '❌ OFF'})\")\n",
    "        self.generator = SimpleDataGenerator(\n",
    "            model_name=self.model_name,\n",
    "            use_chaining=self.use_chaining,\n",
    "            use_quantization=self.use_quantization\n",
    "        )\n",
    "        self.generator.initialize_model()\n",
    "        \n",
    "        # 2. RAG 시스템 초기화\n",
    "        if self.config['use_rag']:\n",
    "            print(\"\\n[2/2] RAG 시스템 초기화...\")\n",
    "            self.rag_system = SimpleRAGSystem()\n",
    "            self.rag_system.initialize()\n",
    "        else:\n",
    "            print(\"\\n[2/2] ⚠️ RAG 시스템 비활성화\")\n",
    "        \n",
    "        print(\"\\n✅ 모든 시스템 초기화 완료!\")\n",
    "    \n",
    "    def run(self, target_count: int = None, test_mode: bool = False):\n",
    "        \"\"\"데이터 생성 실행\"\"\"\n",
    "        \n",
    "        if target_count:\n",
    "            self.config['target_count'] = target_count\n",
    "            \n",
    "        if test_mode:\n",
    "            self.config['target_count'] = 5\n",
    "            print(\"⚠️ 테스트 모드: 5개만 생성합니다.\")\n",
    "        \n",
    "        print(f\"\\n🎯 목표: {self.config['target_count']}개 생성\")\n",
    "        print(f\"📈 모드: {'🔗 체이닝' if self.use_chaining else '⚡ 단순'}\")\n",
    "        print(f\"🔍 RAG: {'✅ 사용' if self.config['use_rag'] else '❌ 미사용'}\")\n",
    "        \n",
    "        # 컨텍스트 준비\n",
    "        contexts = self._prepare_contexts()\n",
    "        \n",
    "        # 생성 실행\n",
    "        print(f\"\\n📝 생성 시작...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        for i, context in enumerate(contexts[:self.config['target_count']]):\n",
    "            qtype = self.config['question_types'][i % len(self.config['question_types'])]\n",
    "            \n",
    "            result = self.generator.generate_qa_pair(context, qtype)\n",
    "            \n",
    "            if result:\n",
    "                results.append(result)\n",
    "                print(f\"✅ {i+1}/{self.config['target_count']} 성공 ({result['method']}, {result['time_taken']:.1f}초)\")\n",
    "            else:\n",
    "                print(f\"❌ {i+1}/{self.config['target_count']} 실패\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # 결과 출력\n",
    "        if results:\n",
    "            print(f\"\\n🎉 생성 완료!\")\n",
    "            print(f\"📊 결과: {len(results)}/{self.config['target_count']}개 성공\")\n",
    "            print(f\"⏱️ 소요 시간: {total_time:.1f}초 (평균: {total_time/len(results):.1f}초/개)\")\n",
    "            \n",
    "            # 통계 출력\n",
    "            stats = self.generator.get_stats()\n",
    "            print(f\"📈 성공률: {stats['success_rate']}%\")\n",
    "            print(f\"🔧 모드: {stats['mode']}\")\n",
    "            \n",
    "            # 샘플 출력\n",
    "            print(f\"\\n📋 생성 샘플:\")\n",
    "            for i, result in enumerate(results[:2]):\n",
    "                print(f\"\\n[샘플 {i+1}] ({result['method']})\")\n",
    "                print(f\"📝 문제: {result['question'][:80]}...\")\n",
    "                print(f\"💡 답: {result['answer'][:40]}...\")\n",
    "                \n",
    "            # 결과 저장\n",
    "            output_file = self._save_results(results)\n",
    "            print(f\"💾 저장 위치: {output_file}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ 생성 결과가 없습니다.\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _prepare_contexts(self) -> List[str]:\n",
    "        \"\"\"컨텍스트 준비\"\"\"\n",
    "        contexts = []\n",
    "        \n",
    "        if self.config['use_rag'] and self.rag_system:\n",
    "            # RAG 검색\n",
    "            topics = [\"개인정보보호\", \"전자금융거래\", \"금융보안\", \"자금세탁방지\", \"신용정보\"]\n",
    "            \n",
    "            for topic in topics:\n",
    "                search_results = self.rag_system.search(topic, top_k=2)\n",
    "                contexts.extend(search_results)\n",
    "            \n",
    "            # 랜덤 컨텍스트 추가\n",
    "            random_contexts = self.rag_system.get_random_context(n=3)\n",
    "            contexts.extend(random_contexts)\n",
    "        else:\n",
    "            # 기본 컨텍스트 (RAG 없이)\n",
    "            default_contexts = [\n",
    "                \"개인정보 처리자는 개인정보를 처리할 목적을 명확히 하여야 하며, 그 목적에 필요한 범위에서 최소한으로 개인정보를 처리하여야 한다.\",\n",
    "                \"금융기관은 전자금융거래 시 충분한 보안대책을 수립·시행하여야 하며, 이용자로부터 이용자를 식별할 수 있는 정보를 요구할 수 있다.\",\n",
    "                \"금융회사는 자금세탁방지 및 테러자금조달금지에 관한 법률에 따라 고객확인의무를 이행하여야 한다.\",\n",
    "                \"신용정보회사는 신용정보주체의 동의를 받지 아니하고는 개인신용정보를 제3자에게 제공하거나 목적 외의 용도로 이용할 수 없다.\",\n",
    "                \"금융회사는 내부통제기준을 마련하여 이사회의 승인을 받고 이를 성실히 이행하여야 한다.\"\n",
    "            ]\n",
    "            contexts = default_contexts * (self.config['target_count'] // len(default_contexts) + 1)\n",
    "        \n",
    "        return contexts\n",
    "    \n",
    "    def _save_results(self, results: List[Dict]) -> Path:\n",
    "        \"\"\"결과 저장\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"fsku_light_{timestamp}.json\"\n",
    "        output_file = OUTPUT_DIR / filename\n",
    "        \n",
    "        # 메타데이터 추가\n",
    "        output_data = {\n",
    "            'metadata': {\n",
    "                'timestamp': timestamp,\n",
    "                'model': self.model_name,\n",
    "                'use_chaining': self.use_chaining,\n",
    "                'use_quantization': self.use_quantization,\n",
    "                'use_rag': self.config['use_rag'],\n",
    "                'total_count': len(results),\n",
    "                'config': self.config,\n",
    "                'stats': self.generator.get_stats()\n",
    "            },\n",
    "            'data': results\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return output_file\n",
    "\n",
    "print(\"✅ 통합 실행 시스템 생성 완료!\")\n",
    "print(\"특징:\")\n",
    "print(\"- ⚡ 단순/체이닝 모드 선택\")\n",
    "print(\"- 📊 실시간 통계 및 샘플 출력\")\n",
    "print(\"- 💾 자동 결과 저장\")\n",
    "print(\"- 🔧 유연한 설정 옵션\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 실행 테스트 - 체이닝 옵션 비교\n",
    "\n",
    "### 옵션 1: 단순 모드 (빠름, 1-2분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 🧪 테스트 1: 단순 생성 모드 (추천: 빠른 테스트용)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🧪 테스트 1: 단순 생성 모드\")\n",
    "print(\"=\"*50)\n",
    "print(\"특징: 1회 LLM 호출, 매우 빠름, H100에서 10개 생성 시 1-2분 소요\")\n",
    "print()\n",
    "\n",
    "# 시스템 생성\n",
    "system_simple = FSKUAugmentationLight(\n",
    "    model_name=\"microsoft/phi-2\",  # 빠른 모델\n",
    "    use_chaining=False,             # ⭐ 체이닝 OFF - 빠름!\n",
    "    use_quantization=False          # H100은 양자화 불필요\n",
    ")\n",
    "\n",
    "# RAG 비활성화 (더 빠른 테스트를 위해)\n",
    "system_simple.config['use_rag'] = False\n",
    "\n",
    "# 초기화\n",
    "system_simple.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 모드 실행\n",
    "results_simple = system_simple.run(target_count=5, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 옵션 2: 체이닝 모드 (고품질, 시간 더 오래)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 🧪 테스트 2: 체이닝 생성 모드 (고품질 원할 때)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n🧪 테스트 2: 체이닝 생성 모드\")\n",
    "print(\"=\"*50)\n",
    "print(\"특징: 다중 LLM 호출, 검증 단계 포함, 고품질, 시간 2-3배 더 소요\")\n",
    "print()\n",
    "\n",
    "# 시스템 생성\n",
    "system_chaining = FSKUAugmentationLight(\n",
    "    model_name=\"microsoft/phi-2\",  # 같은 모델 사용\n",
    "    use_chaining=True,              # ⭐ 체이닝 ON - 고품질!\n",
    "    use_quantization=False\n",
    ")\n",
    "\n",
    "# RAG 비활성화 (테스트용)\n",
    "system_chaining.config['use_rag'] = False\n",
    "\n",
    "# 초기화 (이미 로드된 모델 재사용 가능)\n",
    "system_chaining.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체이닝 모드 실행 (더 적은 수로 테스트)\n",
    "results_chaining = system_chaining.run(target_count=3, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 성능 비교 및 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 📊 성능 비교 분석\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📊 성능 비교 분석\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 결과가 있는 경우에만 비교\n",
    "if 'results_simple' in locals() and 'results_chaining' in locals():\n",
    "    simple_stats = system_simple.generator.get_stats()\n",
    "    chaining_stats = system_chaining.generator.get_stats()\n",
    "    \n",
    "    print(\"🔥 단순 모드 (빠름):\")\n",
    "    if results_simple:\n",
    "        avg_time_simple = sum(r['time_taken'] for r in results_simple) / len(results_simple)\n",
    "        print(f\"  - ✅ 성공률: {simple_stats['success_rate']}%\")\n",
    "        print(f\"  - ⚡ 평균 시간: {avg_time_simple:.2f}초/개\")\n",
    "        print(f\"  - 📝 생성 방식: 1회 LLM 호출\")\n",
    "        print(f\"  - 🎯 예상 10개 시간: {avg_time_simple * 10:.1f}초\")\n",
    "    \n",
    "    print(\"\\n🔗 체이닝 모드 (고품질):\")\n",
    "    if results_chaining:\n",
    "        avg_time_chaining = sum(r['time_taken'] for r in results_chaining) / len(results_chaining)\n",
    "        print(f\"  - ✅ 성공률: {chaining_stats['success_rate']}%\")\n",
    "        print(f\"  - ⏱️ 평균 시간: {avg_time_chaining:.2f}초/개\")\n",
    "        print(f\"  - 📝 생성 방식: 다중 LLM 호출 + 검증\")\n",
    "        print(f\"  - 🎯 예상 10개 시간: {avg_time_chaining * 10:.1f}초\")\n",
    "    \n",
    "    print(\"\\n💡 추천 사용법:\")\n",
    "    print(\"  - 🚀 빠른 테스트/프로토타이핑: 단순 모드\")\n",
    "    print(\"  - 🏆 최종 고품질 데이터 생성: 체이닝 모드\")\n",
    "    print(\"  - ⚖️ 절충안: 단순 모드로 대량 생성 후 일부만 체이닝으로 개선\")\n",
    "    \n",
    "    # 속도 개선 정도 계산\n",
    "    if results_simple and results_chaining:\n",
    "        speed_improvement = avg_time_chaining / avg_time_simple\n",
    "        print(f\"\\n📈 성능 개선: 체이닝 대비 단순 모드가 {speed_improvement:.1f}배 빠름\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ 비교할 결과가 없습니다. 위의 셀들을 먼저 실행하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 실제 사용 예제 (추천 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 🎯 실제 사용 예제 - H100 환경에서 추천 설정\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🎯 H100 환경 추천 설정\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 추천 설정 1: 빠른 대량 생성용\n",
    "print(\"\\n📋 설정 1: 빠른 대량 생성 (1000개 생성 시)\")\n",
    "recommended_fast = FSKUAugmentationLight(\n",
    "    model_name=\"microsoft/phi-2\",      # 빠른 모델\n",
    "    use_chaining=False,                 # 단순 모드\n",
    "    use_quantization=False              # H100은 양자화 불필요\n",
    ")\n",
    "print(\"✅ 예상 소요시간: 15-20분 (1000개)\")\n",
    "print(\"✅ 장점: 매우 빠름, 대량 생성 가능\")\n",
    "print(\"⚠️ 단점: 품질이 체이닝 대비 다소 떨어질 수 있음\")\n",
    "\n",
    "# 추천 설정 2: 고품질 생성용\n",
    "print(\"\\n📋 설정 2: 고품질 생성 (100개 고품질 생성 시)\")\n",
    "recommended_quality = FSKUAugmentationLight(\n",
    "    model_name=\"upstage/SOLAR-10.7B-v1.0\",  # 더 좋은 한국어 모델\n",
    "    use_chaining=True,                        # 체이닝 모드\n",
    "    use_quantization=False                    # H100은 충분한 메모리\n",
    ")\n",
    "print(\"✅ 예상 소요시간: 20-30분 (100개)\")\n",
    "print(\"✅ 장점: 최고 품질, 검증 단계 포함\")\n",
    "print(\"⚠️ 단점: 시간이 더 오래 걸림\")\n",
    "\n",
    "# 추천 설정 3: 절충안\n",
    "print(\"\\n📋 설정 3: 절충안 (500개 생성 시)\")\n",
    "recommended_balanced = FSKUAugmentationLight(\n",
    "    model_name=\"beomi/llama-2-ko-7b\",   # 한국어 특화 모델\n",
    "    use_chaining=False,                  # 단순 모드로 빠르게\n",
    "    use_quantization=False               # H100 최적화\n",
    ")\n",
    "print(\"✅ 예상 소요시간: 10-15분 (500개)\")\n",
    "print(\"✅ 장점: 한국어 특화 + 적당한 속도\")\n",
    "print(\"✅ 균형: 품질과 속도의 절충안\")\n",
    "\n",
    "print(\"\\n🎯 결론:\")\n",
    "print(\"1. 테스트/프로토타이핑: phi-2 + 단순 모드\")\n",
    "print(\"2. 대량 생성: llama-2-ko-7b + 단순 모드\") \n",
    "print(\"3. 최고 품질: SOLAR-10.7B + 체이닝 모드\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 최종 실행 (원하는 설정 선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 🏁 최종 실행 - 원하는 설정으로 실행하세요!\n",
    "# =============================================================================\n",
    "\n",
    "# 🔧 여기서 설정을 변경하세요!\n",
    "FINAL_MODEL = \"microsoft/phi-2\"        # 모델 선택\n",
    "FINAL_USE_CHAINING = False              # ⭐ True: 고품질, False: 빠름\n",
    "FINAL_TARGET_COUNT = 10                 # 생성할 개수\n",
    "FINAL_USE_RAG = True                    # RAG 사용 여부\n",
    "\n",
    "print(f\"🚀 최종 실행 설정:\")\n",
    "print(f\"  - 모델: {FINAL_MODEL}\")\n",
    "print(f\"  - 체이닝: {'ON (고품질)' if FINAL_USE_CHAINING else 'OFF (빠름)'}\")\n",
    "print(f\"  - 생성 개수: {FINAL_TARGET_COUNT}개\")\n",
    "print(f\"  - RAG: {'사용' if FINAL_USE_RAG else '미사용'}\")\n",
    "print()\n",
    "\n",
    "# 시스템 생성\n",
    "final_system = FSKUAugmentationLight(\n",
    "    model_name=FINAL_MODEL,\n",
    "    use_chaining=FINAL_USE_CHAINING,\n",
    "    use_quantization=False  # H100은 양자화 불필요\n",
    ")\n",
    "\n",
    "# RAG 설정\n",
    "final_system.config['use_rag'] = FINAL_USE_RAG\n",
    "\n",
    "# 초기화\n",
    "final_system.initialize()\n",
    "\n",
    "# 실행\n",
    "print(\"\\n🎬 최종 실행 시작!\")\n",
    "final_results = final_system.run(target_count=FINAL_TARGET_COUNT)\n",
    "\n",
    "print(\"\\n🎉 최종 실행 완료!\")\n",
    "if final_results:\n",
    "    print(f\"📊 최종 결과: {len(final_results)}개 성공적으로 생성됨\")\n",
    "    final_stats = final_system.generator.get_stats()\n",
    "    print(f\"📈 최종 성공률: {final_stats['success_rate']}%\")\n",
    "else:\n",
    "    print(\"❌ 생성 실패. 설정을 확인하고 다시 시도하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}