#!/usr/bin/env python3
"""
ì²´ì´ë‹ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ë°ì´í„° ìƒì„±ê¸°

ê¸°ì¡´ DataGeneratorë¥¼ í™•ì¥í•˜ì—¬ ë‹¤ë‹¨ê³„ ìƒì„±/ê²€ì¦/ê°œì„  í”„ë¡œì„¸ìŠ¤ êµ¬í˜„
"""

import json
import time
import re
from typing import Dict, List, Optional, Any
import logging
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ê¸°ì¡´ DataGenerator ìƒì†
try:
    from .main import DataGenerator
except ImportError:
    from main import DataGenerator

logger = logging.getLogger(__name__)


class ChainDataGenerator(DataGenerator):
    """ì²´ì´ë‹ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self, config: Dict[str, Any]):
        """ì´ˆê¸°í™”"""
        super().__init__(config)
        self.chain_steps = config.get('chain_steps', ['generate'])
        self.validation_model = None
        self.validation_tokenizer = None
        
        # ê²€ì¦ìš© ëª¨ë¸ ë¡œë“œ
        if config.get('validation_model') and 'validate' in self.chain_steps:
            self._load_validation_model()
    
    def _load_validation_model(self):
        """ê²€ì¦ìš© ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ê²€ì¦ ëª¨ë¸ ë¡œë”©: {self.config['validation_model']}")
        
        self.validation_tokenizer = AutoTokenizer.from_pretrained(
            self.config['validation_model'],
            trust_remote_code=True
        )
        
        if self.validation_tokenizer.pad_token is None:
            self.validation_tokenizer.pad_token = self.validation_tokenizer.eos_token
        
        # ë¦¬ì†ŒìŠ¤ ì œì•½ ì—†ëŠ” í™˜ê²½ì´ë¯€ë¡œ ì „ì²´ ì •ë°€ë„ ì‚¬ìš©
        self.validation_model = AutoModelForCausalLM.from_pretrained(
            self.config['validation_model'],
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        logger.info("ê²€ì¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def generate_with_chain(self, prompt: str, context: str) -> Dict[str, Any]:
        """ì²´ì´ë‹ì„ í†µí•œ ë¬¸ì œ ìƒì„±"""
        result = {
            'prompt': prompt,
            'context': context,
            'chain_history': []
        }
        
        current_output = None
        
        for step in self.chain_steps:
            if step == 'generate':
                # 1ë‹¨ê³„: ì´ˆê¸° ìƒì„±
                current_output = self._generate_with_llm(prompt, self.config['temperature'])
                result['initial_output'] = current_output
                
            elif step == 'validate' and self.validation_model:
                # 2ë‹¨ê³„: ê²€ì¦
                validation_result = self._validate_output(current_output)
                result['validation_score'] = validation_result['score']
                result['validation_feedback'] = validation_result['feedback']
                
                if validation_result['score'] < self.config.get('min_quality_score', 80):
                    current_output = validation_result['suggestion']
                    
            elif step == 'refine':
                # 3ë‹¨ê³„: ê°œì„ 
                refined_output = self._refine_output(
                    current_output, 
                    result.get('validation_feedback', '')
                )
                current_output = refined_output
                
            elif step == 'final_check':
                # 4ë‹¨ê³„: ìµœì¢… ê²€ì¦
                final_score = self.quality_checker.evaluate(current_output)
                result['final_score'] = final_score
            
            result['chain_history'].append({
                'step': step,
                'output': current_output[:200] + '...' if len(current_output) > 200 else current_output
            })
        
        result['final_output'] = current_output
        return result
    
    def _validate_output(self, output: str) -> Dict[str, Any]:
        """ê²€ì¦ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶œë ¥ í‰ê°€"""
        validation_prompt = f"""ë‹¤ìŒ ê¸ˆìœµ ì‹œí—˜ ë¬¸ì œë¥¼ í‰ê°€í•˜ê³  ê°œì„ ì ì„ ì œì‹œí•˜ì„¸ìš”.

ë¬¸ì œ:
{output}

í‰ê°€ ê¸°ì¤€:
1. ëª…í™•ì„± (1-10): ë¬¸ì œê°€ ëª…í™•í•˜ê³  ëª¨í˜¸í•˜ì§€ ì•Šì€ê°€?
2. ì •í™•ì„± (1-10): ê¸ˆìœµ ì§€ì‹ì´ ì •í™•í•œê°€?
3. ë‚œì´ë„ (1-10): ì ì ˆí•œ ë‚œì´ë„ì¸ê°€?
4. í˜•ì‹ (1-10): ë¬¸ì œ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?

ì¶œë ¥ í˜•ì‹:
[ì ìˆ˜] ì´ì : XX/40
[í”¼ë“œë°±] ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„
[ì œì•ˆ] ê°œì„ ëœ ë¬¸ì œ
"""
        
        # ê²€ì¦ ëª¨ë¸ë¡œ í‰ê°€
        inputs = self.validation_tokenizer(
            validation_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.validation_model.device)
        
        with torch.no_grad():
            outputs = self.validation_model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True
            )
        
        validation_output = self.validation_tokenizer.decode(outputs[0], skip_special_tokens=True)
        validation_output = validation_output.replace(validation_prompt, '').strip()
        
        # ê²°ê³¼ íŒŒì‹±
        score_match = re.search(r'ì´ì [:\s]*(\d+)', validation_output)
        score = int(score_match.group(1)) if score_match else 20
        
        feedback_match = re.search(r'\[í”¼ë“œë°±\](.*?)\[ì œì•ˆ\]', validation_output, re.DOTALL)
        feedback = feedback_match.group(1).strip() if feedback_match else ''
        
        suggestion_match = re.search(r'\[ì œì•ˆ\](.*)', validation_output, re.DOTALL)
        suggestion = suggestion_match.group(1).strip() if suggestion_match else output
        
        return {
            'score': (score / 40) * 100,  # 100ì  ë§Œì ìœ¼ë¡œ ë³€í™˜
            'feedback': feedback,
            'suggestion': suggestion
        }
    
    def _refine_output(self, output: str, feedback: str) -> str:
        """í”¼ë“œë°±ì„ ë°˜ì˜í•œ ì¶œë ¥ ê°œì„ """
        refine_prompt = f"""ë‹¤ìŒ ê¸ˆìœµ ë¬¸ì œë¥¼ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ê°œì„ í•˜ì„¸ìš”.

ì›ë³¸ ë¬¸ì œ:
{output}

í”¼ë“œë°±:
{feedback}

ê°œì„ ëœ ë¬¸ì œ (ë” ëª…í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ):
"""
        
        refined = self._generate_with_llm(refine_prompt, temperature=0.7)
        return refined
    
    def generate_questions_batch(self, chunks: List[Dict], batch_size: int = 32) -> List[Dict]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì œ ìƒì„±"""
        questions = []
        
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i+batch_size]
            batch_prompts = []
            
            for chunk in batch_chunks:
                prompt = self.prompt_template.format(
                    concept=chunk.get('keywords', ['ê¸ˆìœµ'])[0] if chunk.get('keywords') else 'ê¸ˆìœµ',
                    context=chunk['content']
                )
                batch_prompts.append(prompt)
            
            # ë°°ì¹˜ ì²˜ë¦¬ (í–¥í›„ ë³‘ë ¬í™” ê°€ëŠ¥)
            for prompt, chunk in zip(batch_prompts, batch_chunks):
                try:
                    if self.config.get('use_chaining'):
                        result = self.generate_with_chain(prompt, chunk['content'])
                        if result.get('final_score', 0) >= self.config.get('min_quality_score', 80):
                            question = self._parse_question_result(result)
                            questions.append(question)
                    else:
                        # ê¸°ì¡´ ë°©ì‹
                        generated = self._generate_with_llm(prompt, self.config['temperature'])
                        question = self._extract_question_answer(generated)
                        questions.append(question)
                except Exception as e:
                    logger.error(f"ìƒì„± ì˜¤ë¥˜: {e}")
                    continue
        
        return questions
    
    def _parse_question_result(self, result: Dict) -> Dict:
        """ì²´ì´ë‹ ê²°ê³¼ë¥¼ ë¬¸ì œ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±"""
        final_output = result['final_output']
        
        # ê¸°ì¡´ íŒŒì‹± ë©”ì„œë“œ ì‚¬ìš©
        question = self._clean_question(final_output)
        answer = self._extract_answer(final_output)
        
        return {
            'id': f"CHAIN_{int(time.time()*1000) % 1000000:06d}",
            'question': question,
            'answer': answer,
            'metadata': {
                'chain_steps': len(result['chain_history']),
                'initial_score': result.get('validation_score', 0),
                'final_score': result.get('final_score', 0),
                'context_used': bool(result.get('context'))
            }
        }
    
    def _extract_question_answer(self, generated: str) -> Dict:
        """ê¸°ì¡´ ë°©ì‹ì˜ ì§ˆë¬¸/ë‹µë³€ ì¶”ì¶œ"""
        question = self._clean_question(generated)
        answer = self._extract_answer(generated)
        
        return {
            'id': f"GEN_{int(time.time()*1000) % 1000000:06d}",
            'question': question,
            'answer': answer
        }


def generate_chain_data(config: Dict[str, Any]) -> str:
    """ì²´ì´ë‹ ë°ì´í„° ìƒì„± ì‹¤í–‰"""
    print("="*60)
    print("ğŸ”— ì²´ì´ë‹ ë°ì´í„° ìƒì„± ì‹œì‘")
    print(f"ëª¨ë¸: {config.get('model_name')}")
    print(f"ê²€ì¦ ëª¨ë¸: {config.get('validation_model', 'None')}")
    print(f"ì²´ì¸ ë‹¨ê³„: {config.get('chain_steps', ['generate'])}")
    print("="*60)
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = ChainDataGenerator(config)
    
    # ë¬¸ì œ ìƒì„±
    start_time = time.time()
    questions = generator.generate_questions(
        num_questions=config.get('num_questions', 100),
        min_quality_score=config.get('min_quality_score', 80),
        temperature=config.get('temperature', 0.7)
    )
    elapsed_time = time.time() - start_time
    
    # ê²°ê³¼ ì €ì¥
    output_file = generator.save_results(questions)
    
    print(f"\nâœ… ì²´ì´ë‹ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
    print(f"ğŸ“Š ìƒì„±ëœ ë¬¸ì œ: {len(questions)}ê°œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    
    # ì²´ì´ë‹ í†µê³„
    chain_questions = [q for q in questions if 'metadata' in q and 'chain_steps' in q['metadata']]
    if chain_questions:
        avg_improvement = sum(
            q['metadata'].get('final_score', 0) - q['metadata'].get('initial_score', 0)
            for q in chain_questions
        ) / len(chain_questions)
        print(f"ğŸ“ˆ í‰ê·  í’ˆì§ˆ ê°œì„ : +{avg_improvement:.1f}ì ")
    
    return output_file


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_config = {
        'model_name': 'Qwen/Qwen2.5-7B-Instruct',
        'validation_model': 'meta-llama/Llama-3-7B-Instruct',
        'use_rag': True,
        'use_chaining': True,
        'chain_steps': ['generate', 'validate', 'refine', 'final_check'],
        'num_questions': 10,
        'min_quality_score': 80
    }
    
    generate_chain_data(test_config)