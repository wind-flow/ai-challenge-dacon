#!/usr/bin/env python3
"""
ë°ì´í„° ìƒì„± ë©”ì¸ ëª¨ë“ˆ

í•™ìŠµ ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ í†µí•© ì‹œìŠ¤í…œ
RAGë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ê¸ˆìœµ ë¬¸ì œ ìƒì„±
"""

import json
import time
import re
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import logging

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from tqdm import tqdm

# ë¡œì»¬ ëª¨ë“ˆ
try:
    from generate_data.quality_checker import QualityChecker
    from rag.retriever import DocumentRetriever
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ì‹œ
    try:
        from .quality_checker import QualityChecker
    except ImportError:
        from quality_checker import QualityChecker
    
    import sys
    sys.path.append(str(Path(__file__).parent.parent))
    from rag.retriever import DocumentRetriever

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataGenerator:
    """
    í•™ìŠµ ë°ì´í„° ìƒì„± í´ë˜ìŠ¤
    
    ì™¸ë¶€ ë°ì´í„° ê¸°ë°˜ ê°œë… ì¶”ì¶œ â†’ RAG ê²€ìƒ‰ â†’ LLM ìƒì„± â†’ í’ˆì§ˆ ê²€ì¦
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
                - model_name: ì‚¬ìš©í•  LLM ëª¨ë¸
                - use_rag: RAG ì‚¬ìš© ì—¬ë¶€
                - use_quantization: 4bit ì–‘ìí™” ì‚¬ìš©
                - prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê²½ë¡œ
        """
        self.config = config
        self.model = None
        self.tokenizer = None
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA > MPS > CPU)
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"  # Mac GPU
        else:
            self.device = "cpu"
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.quality_checker = QualityChecker()
        
        if config.get('use_rag', False):
            self.retriever = DocumentRetriever()
        else:
            self.retriever = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        self.prompt_template = self._load_prompt_template()
        
        logger.info(f"ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ (Device: {self.device})")
    
    def _load_prompt_template(self) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        template_path = Path(self.config.get('prompt_template', 'prompts/default.txt'))
        
        if template_path.exists():
            with open(template_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            # ê¸°ë³¸ í…œí”Œë¦¿
            return """ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì œ: {concept}
ì°¸ê³ ìë£Œ: {context}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ FSKU í‰ê°€ìš© ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- í•œêµ­ ê¸ˆìœµ ì‹¤ë¬´ì™€ ê´€ë ¨ëœ ë‚´ìš©
- ëª…í™•í•˜ê³  ëª¨í˜¸í•˜ì§€ ì•Šì€ ë¬¸ì œ
- ê°ê´€ì‹ ë˜ëŠ” ì£¼ê´€ì‹ í˜•íƒœ

ë¬¸ì œ:"""
    
    def load_model(self):
        """LLM ëª¨ë¸ ë¡œë“œ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
        model_name = self.config['model_name']
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print("   (ì²« ì‹¤í–‰ì‹œ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")
        
        try:
            # í† í¬ë‚˜ì´ì €
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ì„¤ì •
            if self.config.get('use_quantization', True) and self.device == "cuda":  # ì–‘ìí™”ëŠ” CUDAë§Œ ì§€ì›
                # 4bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
                print("âœ… 4bit ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                # MPSë‚˜ CPU ì‚¬ìš©
                if self.device == "mps":
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=torch.float16,
                        trust_remote_code=True
                    )
                    self.model = self.model.to(self.device)
                    print(f"âœ… MPS(Mac GPU) ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=torch.float32,
                        trust_remote_code=True
                    )
                    print(f"âœ… CPU ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ í•´ê²° ë°©ë²•:")
            print("   1. python download_models.py ì‹¤í–‰")
            print("   2. ì¸í„°ë„· ì—°ê²° í™•ì¸")
            print("   3. GPU ë©”ëª¨ë¦¬ í™•ì¸ (nvidia-smi)")
            raise
    
    def generate_questions(self, num_questions: int = 100, min_quality_score: int = 70, temperature: float = 0.7) -> List[Dict]:
        """
        ë¬¸ì œ ìƒì„±
        
        Args:
            num_questions: ìƒì„±í•  ë¬¸ì œ ìˆ˜
            min_quality_score: ìµœì†Œ í’ˆì§ˆ ì ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            
        Returns:
            ìƒì„±ëœ ë¬¸ì œ ë¦¬ìŠ¤íŠ¸
        """
        # ëª¨ë¸ ë¡œë“œ
        if self.model is None:
            self.load_model()
        
        # RAG ì¤€ë¹„ í™•ì¸
        if self.retriever:
            print(f"âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
        else:
            print("âš ï¸ RAG ì‚¬ìš© ì•ˆ í•¨")
        
        # ë¬¸ì œ ìƒì„±
        generated_questions = []
        
        failed_attempts = 0
        max_retries = 3
        
        with tqdm(total=num_questions, desc="ë¬¸ì œ ìƒì„± ì¤‘") as pbar:
            while len(generated_questions) < num_questions and failed_attempts < num_questions * 2:
                try:
                    # RAGì—ì„œ ëœë¤ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    context = ""
                    concept = ""
                    
                    if self.retriever:
                        try:
                            # ëœë¤ ì²­í¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                            context = self.retriever.get_random_chunks(n=2)
                            
                            # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ëª…ì‚¬ ì°¾ê¸° (ì°¸ê³ ìš©)
                            nouns = re.findall(r'[\uac00-\ud7a3]{2,10}', context)
                            if nouns:
                                concept = nouns[0]  # ì²« ë²ˆì§¸ ëª…ì‚¬ë¥¼ ì°¸ê³ ë¡œ ì‚¬ìš©
                        except Exception as e:
                            logger.debug(f"RAG ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                    
                    if not context:
                        # RAGê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°
                        context = "ê¸ˆìœµ ê´€ë ¨ ì¼ë°˜ ì§€ì‹"
                        concept = "ê¸ˆìœµ"
                    
                    # í”„ë¡¬í”„íŠ¸ ìƒì„±
                    prompt = self.prompt_template.format(
                        concept=concept,
                        context=context if context else "ì—†ìŒ"
                    )
                    
                    # LLMìœ¼ë¡œ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                    question = None
                    for retry in range(max_retries):
                        try:
                            question = self._generate_with_llm(prompt, temperature)
                            break
                        except Exception as e:
                            if retry < max_retries - 1:
                                logger.debug(f"ìƒì„± ì¬ì‹œë„ {retry+1}/{max_retries}: {e}")
                                time.sleep(1)
                            else:
                                raise
                    
                    if not question:
                        failed_attempts += 1
                        continue
                    
                    # í’ˆì§ˆ í‰ê°€ (ê°œë… ì—†ì´)
                    quality_score = self.quality_checker.evaluate(question)
                    
                    # ê²°ê³¼ ì €ì¥ (ì§ˆë¬¸ê³¼ ë‹µë§Œ)
                    # ì§ˆë¬¸ì—ì„œ ë‹µ ì¶”ì¶œ ì‹œë„
                    answer = self._extract_answer(question)
                    clean_question = self._clean_question(question)
                    
                    result = {
                        'id': f"GEN_{len(generated_questions)+1:05d}",
                        'question': clean_question,
                        'answer': answer
                    }
                    
                    # ë©”íƒ€ë°ì´í„°ëŠ” ë³„ë„ ì €ì¥
                    metadata = {
                        'id': result['id'],
                        'concept': concept,
                        'quality_score': quality_score,
                        'context_used': bool(context),
                        'context': context[:500] if context else "",
                        'timestamp': datetime.now().isoformat(),
                        'model': self.config['model_name'],
                        'full_generated': question
                    }
                    
                    # ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (í´ë˜ìŠ¤ ë ˆë²¨ ë³€ìˆ˜ í•„ìš”)
                    if not hasattr(self, 'metadata_list'):
                        self.metadata_list = []
                    self.metadata_list.append(metadata)
                    
                    # í’ˆì§ˆ ê¸°ì¤€ í†µê³¼í•œ ê²ƒë§Œ ì¶”ê°€
                    if quality_score >= min_quality_score:
                        generated_questions.append(result)
                        pbar.update(1)
                    else:
                        logger.debug(f"í’ˆì§ˆ ë¯¸ë‹¬: {quality_score:.1f}ì ")
                        failed_attempts += 1
                        
                except Exception as e:
                    logger.error(f"ë¬¸ì œ ìƒì„± ì˜¤ë¥˜: {e}")
                    failed_attempts += 1
                    continue
        
        logger.info(f"ì´ {len(generated_questions)}ê°œ ë¬¸ì œ ìƒì„± ì™„ë£Œ")
        return generated_questions
    
    def _generate_with_llm(self, prompt: str, temperature: float = 0.7) -> str:
        """LLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        )
        
        if self.device in ["cuda", "mps"]:
            inputs = inputs.to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=temperature,
                top_p=self.config.get('top_p', 0.9),
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ì œê±°
        if prompt in generated:
            generated = generated.replace(prompt, "").strip()
        
        return generated
    
    def _extract_answer(self, generated_text: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë‹µë³€ ì°¾ê¸°
        patterns = [
            r'\[ANSWER\]([\s\S]*?)\[',
            r'\[ë‹µë³€\]([\s\S]*?)\[',
            r'\[ë‹µ\]([\s\S]*?)\[', 
            r'ì •ë‹µ[:ï¼š]\s*([^\n]+)',
            r'ë‹µ[:ï¼š]\s*([^\n]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, generated_text, re.IGNORECASE | re.MULTILINE)
            if match:
                return match.group(1).strip()
        
        # ê°ê´€ì‹ íŒ¨í„´ (1ë²ˆ, 2ë²ˆ ë“±)
        if any(num in generated_text for num in ['1)', '2)', '3)', '4)', '5)']):
            answer_match = re.search(r'ì •ë‹µ.*?([1-5])[ë²ˆ\)]', generated_text)
            if answer_match:
                return answer_match.group(1) + "ë²ˆ"
        
        return "ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨"
    
    def _clean_question(self, generated_text: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ë§Œ ì¶”ì¶œ"""
        # ì§ˆë¬¸ íŒ¨í„´
        patterns = [
            r'\[QUESTION\]([\s\S]*?)\[',
            r'\[ë¬¸ì œ\]([\s\S]*?)\[',
            r'ë¬¸ì œ[:ï¼š]\s*([^\[]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, generated_text, re.IGNORECASE | re.MULTILINE)
            if match:
                question = match.group(1).strip()
                # ì„ íƒì§€ ì œê±°
                if '\n1)' in question:
                    question = question[:question.find('\n1)')].strip()
                return question
        
        # ì²˜ìŒë¶€í„° ë¬¼ìŒí‘œê¹Œì§€ ì°¾ê¸°
        question_end = generated_text.find('?')
        if question_end > 0:
            return generated_text[:question_end+1].strip()
        
        # ì²˜ìŒ 200ìë§Œ ë°˜í™˜ (í´ë°±)
        return generated_text[:200].strip()
    
    def save_results(self, questions: List[Dict], output_file: str = None):
        """ê²°ê³¼ ì €ì¥"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"generated_data_{timestamp}.json"
        
        output_path = Path("results") / output_file
        output_path.parent.mkdir(exist_ok=True)
        
        result_data = {
            'metadata': {
                'total_questions': len(questions),
                'generation_time': datetime.now().isoformat(),
                'config': self.config,
                'quality_stats': self._calculate_quality_stats(questions)
            },
            'questions': questions
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_path}")
        return str(output_path)
    
    def _calculate_quality_stats(self, questions: List[Dict]) -> Dict:
        """í’ˆì§ˆ í†µê³„ ê³„ì‚°"""
        if not questions or not hasattr(self, 'metadata_list'):
            return {'avg_score': 0, 'min_score': 0, 'max_score': 0, 'total': 0}
        
        scores = [m['quality_score'] for m in self.metadata_list if 'quality_score' in m]
        
        if not scores:
            return {'avg_score': 0, 'min_score': 0, 'max_score': 0, 'total': 0}
        
        return {
            'avg_score': sum(scores) / len(scores),
            'min_score': min(scores),
            'max_score': max(scores),
            'total': len(scores)
        }
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    def generate_batch(self, prompts: List[str], temperature: float = 0.7) -> List[str]:
        """ë°°ì¹˜ ìƒì„± (í–¥í›„ êµ¬í˜„)"""
        # TODO: ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
        results = []
        for prompt in prompts:
            result = self._generate_with_llm(prompt, temperature)
            results.append(result)
        return results


def generate_data(config: Dict[str, Any] = None) -> str:
    """
    ë°ì´í„° ìƒì„± ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if config is None:
        # ê¸°ë³¸ ì„¤ì •
        config = {
            'model_name': 'upstage/SOLAR-10.7B-v1.0',
            'use_rag': True,
            'use_quantization': True,
            'num_questions': 100,
            'min_quality': 70,
            'temperature': 0.7,
            'prompt_template': 'prompts/cot.txt'
        }
    
    print("="*60)
    print("ğŸ“Š ë°ì´í„° ìƒì„± ì‹œì‘")
    print(f"ëª¨ë¸: {config.get('model_name', 'upstage/SOLAR-10.7B-v1.0')}")
    print(f"ë¬¸ì œ ìˆ˜: {config.get('num_questions', 100)}ê°œ")
    print(f"RAG ì‚¬ìš©: {'ì˜ˆ' if config.get('use_rag', True) else 'ì•„ë‹ˆì˜¤'}")
    print("="*60)
    
    try:
        # ìƒì„±ê¸° ì´ˆê¸°í™”
        generator_config = {
            'model_name': config.get('model_name', 'upstage/SOLAR-10.7B-v1.0'),
            'use_rag': config.get('use_rag', True),
            'use_quantization': config.get('use_quantization', True),
            'prompt_template': config.get('prompt_template', 'prompts/cot.txt'),
            'top_p': config.get('top_p', 0.9),
            'temperature': config.get('temperature', 0.7)
        }
        generator = DataGenerator(generator_config)
        
        # ë¬¸ì œ ìƒì„±
        start_time = time.time()
        questions = generator.generate_questions(
            num_questions=config.get('num_questions', 100),
            min_quality_score=config.get('min_quality', 70),
            temperature=config.get('temperature', 0.7)
        )
        elapsed_time = time.time() - start_time
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        generator.cleanup()
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if 'generator' in locals():
            generator.cleanup()
        raise
    
    # ê²°ê³¼ ì €ì¥ (JSONL í˜•ì‹)
    output_dir = Path("data/augmented")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"train_data_{timestamp}.jsonl"
    metadata_file = output_dir / f"metadata_{timestamp}.json"
    
    # ì§ˆë¬¸ê³¼ ë‹µë§Œ JSONLë¡œ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        for question in questions:
            f.write(json.dumps(question, ensure_ascii=False) + '\n')
    
    # ë©”íƒ€ë°ì´í„°ëŠ” ë³„ë„ JSON íŒŒì¼ë¡œ ì €ì¥
    if hasattr(generator, 'metadata_list'):
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump({
                'generation_info': {
                    'timestamp': timestamp,
                    'model': config.get('model_name'),
                    'total_questions': len(questions),
                    'config': config
                },
                'metadata': generator.metadata_list
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata_file}")
    
    # ê²°ê³¼ ì¶œë ¥
    print("="*60)
    print("âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ í›ˆë ¨ ë°ì´í„°: {output_file}")
    if 'metadata_file' in locals():
        print(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata_file}")
    print(f"ğŸ“Š ìƒì„±ëœ ë¬¸ì œ: {len(questions)}ê°œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    print("="*60)
    
    # ìƒ˜í”Œ ì¶œë ¥
    if questions and len(questions) > 0:
        print("\nğŸ” ìƒì„± ì˜ˆì‹œ (3ê°œ):")
        for i, q in enumerate(questions[:3], 1):
            print(f"\n[{i}] ì§ˆë¬¸: {q['question'][:100]}..." if len(q['question']) > 100 else f"\n[{i}] ì§ˆë¬¸: {q['question']}")
            print(f"    ë‹µë³€: {q['answer'][:50]}..." if len(q['answer']) > 50 else f"    ë‹µë³€: {q['answer']}")
    
    return str(output_file)




if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    config = {
        'model_name': 'beomi/SOLAR-10.7B-v1.0',
        'use_rag': True,
        'num_questions': 10,  # í…ŒìŠ¤íŠ¸ìš© ì ì€ ìˆ˜
        'prompt_template': 'prompts/cot.txt'
    }
    
    generate_data(config)