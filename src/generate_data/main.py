#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FSKU ë°ì´í„° ì¦ê°• ì‹œìŠ¤í…œ - ê°œì„ ëœ ë²„ì „
- ëª¨ë¸ ì‚¬ì „ ë¡œë”© âœ…
- ë™ì‘ ê²€ì¦ í¬í•¨ âœ…
- ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” âœ…
"""

import warnings
warnings.filterwarnings('ignore')

import os
import sys
import json
import time
import pickle
import traceback
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple
from collections import defaultdict
import random
import logging
import argparse

# ë°ì´í„° ì²˜ë¦¬
import numpy as np
import pandas as pd
from tqdm import tqdm

# ë”¥ëŸ¬ë‹ ë° NLP
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    set_seed
)

# ë²¡í„° ê²€ìƒ‰ ë° ì„ë² ë”© (RAGìš©)
from sentence_transformers import SentenceTransformer
import faiss

# ë¬¸ì„œ ì²˜ë¦¬
import PyPDF2

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('fsku_generation.log')
    ]
)
logger = logging.getLogger(__name__)

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
set_seed(42)
random.seed(42)
np.random.seed(42)

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent.parent  # ai-dacon í´ë”
EXTERNAL_DIR = BASE_DIR / "data" / "external"
OUTPUT_DIR = BASE_DIR / "data" / "augmented"
CACHE_DIR = BASE_DIR / "data" / "cache"
MODEL_CACHE_DIR = BASE_DIR / "models" / "cache"

# ë””ë ‰í† ë¦¬ ìƒì„±
for dir_path in [OUTPUT_DIR, CACHE_DIR, MODEL_CACHE_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)


class ModelManager:
    """
    ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤ - ì‚¬ì „ ë¡œë”© ë° ìºì‹±
    """
    _instance = None
    _models = {}
    _tokenizers = {}
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelManager, cls).__new__(cls)
        return cls._instance
    
    def load_model(self, 
                   model_name: str, 
                   use_quantization: bool = False,
                   force_reload: bool = False) -> Tuple[Any, Any]:
        """
        ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ìºì‹± ì§€ì›)
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            use_quantization: ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
            force_reload: ê°•ì œ ì¬ë¡œë“œ
            
        Returns:
            (model, tokenizer) íŠœí”Œ
        """
        cache_key = f"{model_name}_{use_quantization}"
        
        # ìºì‹œ í™•ì¸
        if not force_reload and cache_key in self._models:
            logger.info(f"ğŸ“¦ ìºì‹œì—ì„œ ëª¨ë¸ ë¡œë“œ: {model_name}")
            return self._models[cache_key], self._tokenizers[cache_key]
        
        logger.info(f"ğŸš€ ìƒˆë¡œìš´ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_name}")
        start_time = time.time()
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=MODEL_CACHE_DIR
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                tokenizer.pad_token_id = tokenizer.eos_token_id
            
            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            model_kwargs = {
                "trust_remote_code": True,
                "device_map": "auto",
                "cache_dir": MODEL_CACHE_DIR
            }
            
            # GPU ìµœì í™”
            if torch.cuda.is_available():
                # H100/A100ì€ bfloat16, ë‚˜ë¨¸ì§€ëŠ” float16
                compute_capability = torch.cuda.get_device_capability()
                if compute_capability[0] >= 8:  # Ampere ì´ìƒ
                    model_kwargs["torch_dtype"] = torch.bfloat16
                    logger.info("ğŸ”¥ bfloat16 ì‚¬ìš© (H100/A100)")
                else:
                    model_kwargs["torch_dtype"] = torch.float16
                    logger.info("ğŸ”¥ float16 ì‚¬ìš©")
            
            # ì–‘ìí™” ì„¤ì •
            if use_quantization:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=model_kwargs.get("torch_dtype", torch.float16),
                    bnb_4bit_use_double_quant=True
                )
                model_kwargs["quantization_config"] = bnb_config
                logger.info("ğŸ’ 4bit ì–‘ìí™” í™œì„±í™”")
            
            # ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                **model_kwargs
            )
            
            # ìºì‹œì— ì €ì¥
            self._models[cache_key] = model
            self._tokenizers[cache_key] = tokenizer
            
            load_time = time.time() - start_time
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            if torch.cuda.is_available():
                memory_gb = torch.cuda.memory_allocated() / 1024**3
                max_memory_gb = torch.cuda.max_memory_allocated() / 1024**3
                logger.info(f"âœ… ë¡œë“œ ì™„ë£Œ! (ì‹œê°„: {load_time:.1f}ì´ˆ)")
                logger.info(f"ğŸ’¾ í˜„ì¬ ë©”ëª¨ë¦¬: {memory_gb:.2f}GB / ìµœëŒ€: {max_memory_gb:.2f}GB")
            else:
                logger.info(f"âœ… CPU ë¡œë“œ ì™„ë£Œ! ({load_time:.1f}ì´ˆ)")
            
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        self._models.clear()
        self._tokenizers.clear()
        torch.cuda.empty_cache()
        logger.info("ğŸ§¹ ëª¨ë¸ ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ")


class SimpleDataGenerator:
    """
    ê°œì„ ëœ FSKU ë°ì´í„° ìƒì„±ê¸°
    - ëª¨ë¸ ì‚¬ì „ ë¡œë”© âœ…
    - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” âœ…
    - í†µê³„ ì¶”ì  ê°œì„  âœ…
    """
    
    def __init__(self, 
                 model_name: str = "microsoft/phi-2",
                 use_chaining: bool = False,
                 use_quantization: bool = False):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
            use_chaining: CoT ì²´ì´ë‹ ì‚¬ìš© ì—¬ë¶€
            use_quantization: 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
        """
        self.model_name = model_name
        self.use_chaining = use_chaining
        self.use_quantization = use_quantization
        
        # ëª¨ë¸ ë§¤ë‹ˆì €
        self.model_manager = ModelManager()
        self.model = None
        self.tokenizer = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.simple_prompt = """ì£¼ì œ: {question_type}

ì°¸ê³  ë‚´ìš©:
{context}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {question_type} ë¬¸ì œë¥¼ 1ê°œ ë§Œë“œì„¸ìš”.

ë¬¸ì œ:
ì •ë‹µ:"""
        
        self.chaining_prompt = """ê¸ˆìœµ ì „ë¬¸ê°€ë¡œì„œ FSKU ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ìš”êµ¬ì‚¬í•­:
- ë¬¸ì œ ìœ í˜•: {question_type}
- FSKU ì‹¤ì œ ì‹œí—˜ ìˆ˜ì¤€
- ëª…í™•í•˜ê³  ì •í™•í•œ í‘œí˜„

ë¬¸ì œ:
ì •ë‹µ:
í•´ì„¤:"""
        
        # í†µê³„
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'errors': defaultdict(int),
            'generation_times': []
        }
        
    def initialize(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ì‚¬ì „ ë¡œë”©)"""
        logger.info("=" * 60)
        logger.info("ğŸš€ ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”")
        logger.info(f"ğŸ“‹ ì„¤ì •: ëª¨ë¸={self.model_name}, ì²´ì´ë‹={self.use_chaining}, ì–‘ìí™”={self.use_quantization}")
        logger.info("=" * 60)
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            self.model, self.tokenizer = self.model_manager.load_model(
                self.model_name,
                self.use_quantization
            )
            
            # ë™ì‘ ê²€ì¦
            if not self.verify_model():
                raise RuntimeError("ëª¨ë¸ ë™ì‘ ê²€ì¦ ì‹¤íŒ¨")
            
            logger.info("âœ… ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def verify_model(self) -> bool:
        """ëª¨ë¸ ë™ì‘ ê²€ì¦"""
        logger.info("ğŸ” ëª¨ë¸ ë™ì‘ ê²€ì¦ ì¤‘...")
        
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„±
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            test_output = self.generate_text(test_prompt, max_tokens=10)
            
            if test_output and len(test_output) > 0:
                logger.info(f"âœ… ëª¨ë¸ ê²€ì¦ ì„±ê³µ: '{test_output[:30]}...'")
                return True
            else:
                logger.error("âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: ì¶œë ¥ì´ ë¹„ì–´ìˆìŒ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def generate_text(self, prompt: str, max_tokens: int = 150) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.model or not self.tokenizer:
            raise ValueError("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        try:
            # í† í°í™”
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=800
            )
            
            # GPUë¡œ ì´ë™
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            generated = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated.strip()
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            self.stats['errors']['generation'] += 1
            return ""
    
    def generate_qa_pair(self, context: str, question_type: str = "ê°ê´€ì‹") -> Optional[Dict]:
        """QA ìŒ ìƒì„±"""
        self.stats['total'] += 1
        start_time = time.time()
        
        try:
            if self.use_chaining:
                result = self._generate_with_chaining(context, question_type)
            else:
                result = self._generate_simple(context, question_type)
            
            if result:
                generation_time = time.time() - start_time
                result['generation_time'] = generation_time
                self.stats['generation_times'].append(generation_time)
                self.stats['success'] += 1
                return result
            else:
                self.stats['failed'] += 1
                return None
                
        except Exception as e:
            logger.error(f"QA ìƒì„± ì˜¤ë¥˜: {e}")
            self.stats['failed'] += 1
            self.stats['errors']['qa_generation'] += 1
            return None
    
    def _generate_simple(self, context: str, question_type: str) -> Optional[Dict]:
        """ë‹¨ìˆœ ìƒì„± (1íšŒ í˜¸ì¶œ)"""
        prompt = self.simple_prompt.format(
            context=context[:400],
            question_type=question_type
        )
        
        generated = self.generate_text(prompt)
        if not generated:
            return None
        
        qa_pair = self._parse_qa(generated)
        if not qa_pair:
            self.stats['errors']['parsing'] += 1
            return None
        
        return {
            'question': qa_pair['question'],
            'answer': qa_pair['answer'],
            'context': context,
            'question_type': question_type,
            'method': 'simple',
            'timestamp': datetime.now().isoformat()
        }
    
    def _generate_with_chaining(self, context: str, question_type: str) -> Optional[Dict]:
        """ì²´ì´ë‹ ìƒì„± (ë‹¤ì¤‘ í˜¸ì¶œ)"""
        # 1. ì´ˆê¸° ìƒì„±
        prompt = self.chaining_prompt.format(
            context=context[:600],
            question_type=question_type
        )
        generated = self.generate_text(prompt, max_tokens=200)
        
        if not generated:
            return None
        
        qa_pair = self._parse_qa_detailed(generated)
        if not qa_pair:
            self.stats['errors']['parsing'] += 1
            return None
        
        # 2. ê²€ì¦
        verification_prompt = f"""ë‹¤ìŒ ë¬¸ì œë¥¼ ê²€í† í•˜ì„¸ìš”:

ë¬¸ì œ: {qa_pair['question']}
ì •ë‹µ: {qa_pair['answer']}

ë¬¸ì œì ì´ ìˆìœ¼ë©´ ì§€ì í•˜ê³ , ì—†ìœ¼ë©´ "ì í•©"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”:"""
        
        feedback = self.generate_text(verification_prompt, max_tokens=100)
        
        return {
            'question': qa_pair['question'],
            'answer': qa_pair['answer'],
            'explanation': qa_pair.get('explanation', ''),
            'context': context,
            'question_type': question_type,
            'method': 'chaining',
            'feedback': feedback,
            'timestamp': datetime.now().isoformat()
        }
    
    def _parse_qa(self, text: str) -> Optional[Dict]:
        """ê°„ë‹¨í•œ QA íŒŒì‹±"""
        try:
            # ë‹¤ì–‘í•œ êµ¬ë¶„ì ì‹œë„
            for q_marker in ['ë¬¸ì œ:', 'ì§ˆë¬¸:', 'Q:', 'Question:']:
                if q_marker in text:
                    text = text.split(q_marker, 1)[1]
                    break
            
            for a_marker in ['ì •ë‹µ:', 'ë‹µ:', 'A:', 'Answer:']:
                if a_marker in text:
                    parts = text.split(a_marker, 1)
                    question = parts[0].strip()
                    answer = parts[1].strip() if len(parts) > 1 else ""
                    
                    # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    if len(question) > 10 and len(answer) > 0:
                        return {'question': question, 'answer': answer}
            
            # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„
            lines = text.strip().split('\n')
            if len(lines) >= 2:
                return {'question': lines[0].strip(), 'answer': lines[1].strip()}
            
            return None
            
        except Exception as e:
            logger.error(f"íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_qa_detailed(self, text: str) -> Optional[Dict]:
        """ìƒì„¸ QA íŒŒì‹± (í•´ì„¤ í¬í•¨)"""
        try:
            result = {'question': '', 'answer': '', 'explanation': ''}
            
            # ì„¹ì…˜ë³„ ì¶”ì¶œ
            sections = text.split('\n\n')
            for section in sections:
                section_lower = section.lower()
                if 'ë¬¸ì œ' in section_lower or 'question' in section_lower:
                    result['question'] = section.split(':', 1)[-1].strip()
                elif 'ì •ë‹µ' in section_lower or 'answer' in section_lower:
                    result['answer'] = section.split(':', 1)[-1].strip()
                elif 'í•´ì„¤' in section_lower or 'explanation' in section_lower:
                    result['explanation'] = section.split(':', 1)[-1].strip()
            
            # ìµœì†Œ ì¡°ê±´ í™•ì¸
            if result['question'] and result['answer']:
                return result
            
            # ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ íŒŒì‹± ì‹œë„
            return self._parse_qa(text)
            
        except Exception as e:
            logger.error(f"ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return self._parse_qa(text)
    
    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        success_rate = (self.stats['success'] / self.stats['total'] * 100) if self.stats['total'] > 0 else 0
        avg_time = np.mean(self.stats['generation_times']) if self.stats['generation_times'] else 0
        
        return {
            'total': self.stats['total'],
            'success': self.stats['success'],
            'failed': self.stats['failed'],
            'success_rate': round(success_rate, 1),
            'avg_generation_time': round(avg_time, 2),
            'mode': 'chaining' if self.use_chaining else 'simple',
            'errors': dict(self.stats['errors'])
        }


class SimpleRAGSystem:
    """ê²½ëŸ‰í™”ëœ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, external_dir: Path = EXTERNAL_DIR):
        self.external_dir = external_dir
        self.documents = []
        self.embeddings = None
        self.index = None
        self.embedding_model = None
        
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        self.cache_file = CACHE_DIR / "rag_cache.pkl"
    
    def initialize(self):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("ğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        
        # ìºì‹œ í™•ì¸
        if self.cache_file.exists():
            try:
                logger.info("ğŸ“¦ ìºì‹œì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ...")
                with open(self.cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                    self.documents = cache_data['documents']
                    self.embeddings = cache_data['embeddings']
                    
                # ì„ë² ë”© ëª¨ë¸ì€ í•­ìƒ ë¡œë“œ
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                
                # FAISS ì¸ë±ìŠ¤ ì¬ìƒì„±
                self._rebuild_index()
                
                logger.info(f"âœ… ìºì‹œì—ì„œ ë¡œë“œ ì™„ë£Œ! ë¬¸ì„œ: {len(self.documents)}ê°œ")
                return
                
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ìƒˆë¡œ ìƒì„±
        self._load_documents()
        
        if not self.documents:
            logger.warning("âš ï¸ ì™¸ë¶€ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. RAG ë¹„í™œì„±í™”")
            return
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ” ì„ë² ë”© ëª¨ë¸ ë¡œë“œ...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        self._create_index()
        
        # ìºì‹œ ì €ì¥
        self._save_cache()
        
        logger.info(f"âœ… RAG ì´ˆê¸°í™” ì™„ë£Œ! ë¬¸ì„œ: {len(self.documents)}ê°œ")
    
    def _load_documents(self):
        """ë¬¸ì„œ ë¡œë“œ"""
        self.documents = []
        pdf_files = list(self.external_dir.glob("*.pdf"))
        
        if not pdf_files:
            logger.warning(f"âš ï¸ {self.external_dir}ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ“„ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë¡œë“œ ì¤‘...")
        
        for file_path in tqdm(pdf_files, desc="ë¬¸ì„œ ë¡œë“œ"):
            try:
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    text = ""
                    
                    # ì²˜ìŒ 10í˜ì´ì§€ë§Œ ì½ê¸°
                    max_pages = min(10, len(reader.pages))
                    for page_num in range(max_pages):
                        text += reader.pages[page_num].extract_text()
                    
                    # ì²­í‚¹
                    chunks = self._simple_chunk(text, chunk_size=300)
                    for chunk in chunks:
                        self.documents.append({
                            'text': chunk,
                            'source': file_path.name,
                            'chunk_id': len(self.documents)
                        })
                        
            except Exception as e:
                logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨ {file_path.name}: {e}")
    
    def _simple_chunk(self, text: str, chunk_size: int = 300) -> List[str]:
        """ê°„ë‹¨í•œ ì²­í‚¹"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = text.replace('\n', ' ').split('. ')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk and len(current_chunk) > 50:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk and len(current_chunk) > 50:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _create_index(self):
        """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        if not self.documents:
            return
        
        logger.info("ğŸ” ì„ë² ë”© ìƒì„± ì¤‘...")
        texts = [doc['text'] for doc in self.documents]
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        batch_size = 32
        self.embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="ì„ë² ë”© ìƒì„±"):
            batch = texts[i:i+batch_size]
            batch_embeddings = self.embedding_model.encode(batch)
            self.embeddings.extend(batch_embeddings)
        
        self.embeddings = np.array(self.embeddings)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        self._rebuild_index()
        
        logger.info(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ ì²­í¬")
    
    def _rebuild_index(self):
        """FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        if self.embeddings is None or len(self.embeddings) == 0:
            return
            
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(self.embeddings.astype('float32'))
    
    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        try:
            cache_data = {
                'documents': self.documents,
                'embeddings': self.embeddings
            }
            with open(self.cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {self.cache_file}")
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def search(self, query: str, top_k: int = 3) -> List[str]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.index or not self.embedding_model:
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embedding_model.encode([query])
            
            # ê²€ìƒ‰
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            # ê²°ê³¼ ë°˜í™˜
            results = []
            for idx in indices[0]:
                if 0 <= idx < len(self.documents):
                    results.append(self.documents[idx]['text'])
            
            return results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_random_context(self, n: int = 2) -> List[str]:
        """ëœë¤ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        if not self.documents:
            return []
        
        selected = random.sample(self.documents, min(n, len(self.documents)))
        return [doc['text'] for doc in selected]


class FSKUAugmentationSystem:
    """
    FSKU ë°ì´í„° ì¦ê°• í†µí•© ì‹œìŠ¤í…œ
    - ëª¨ë¸ ì‚¬ì „ ë¡œë”© âœ…
    - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” âœ…
    - ë™ì‘ ê²€ì¦ í¬í•¨ âœ…
    """
    
    def __init__(self,
                 model_name: str = "microsoft/phi-2",
                 use_chaining: bool = False,
                 use_quantization: bool = False,
                 use_rag: bool = True):
        """ì´ˆê¸°í™”"""
        self.model_name = model_name
        self.use_chaining = use_chaining
        self.use_quantization = use_quantization
        self.use_rag = use_rag
        
        # ì»´í¬ë„ŒíŠ¸
        self.generator = None
        self.rag_system = None
        
        # ì„¤ì •
        self.config = {
            'question_types': ['ê°ê´€ì‹', 'ì£¼ê´€ì‹', 'ë‹¨ë‹µí˜•', 'ì„œìˆ í˜•'],
            'default_contexts': [
                "ê°œì¸ì •ë³´ ì²˜ë¦¬ìëŠ” ê°œì¸ì •ë³´ë¥¼ ì²˜ë¦¬í•  ëª©ì ì„ ëª…í™•íˆ í•˜ì—¬ì•¼ í•˜ë©°, ê·¸ ëª©ì ì— í•„ìš”í•œ ë²”ìœ„ì—ì„œ ìµœì†Œí•œìœ¼ë¡œ ê°œì¸ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ì—¬ì•¼ í•œë‹¤.",
                "ê¸ˆìœµê¸°ê´€ì€ ì „ìê¸ˆìœµê±°ë˜ ì‹œ ì¶©ë¶„í•œ ë³´ì•ˆëŒ€ì±…ì„ ìˆ˜ë¦½Â·ì‹œí–‰í•˜ì—¬ì•¼ í•˜ë©°, ì´ìš©ìë¡œë¶€í„° ì´ìš©ìë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ìš”êµ¬í•  ìˆ˜ ìˆë‹¤.",
                "ê¸ˆìœµíšŒì‚¬ëŠ” ìê¸ˆì„¸íƒë°©ì§€ ë° í…ŒëŸ¬ìê¸ˆì¡°ë‹¬ê¸ˆì§€ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¼ ê³ ê°í™•ì¸ì˜ë¬´ë¥¼ ì´í–‰í•˜ì—¬ì•¼ í•œë‹¤.",
                "ì‹ ìš©ì •ë³´íšŒì‚¬ëŠ” ì‹ ìš©ì •ë³´ì£¼ì²´ì˜ ë™ì˜ë¥¼ ë°›ì§€ ì•„ë‹ˆí•˜ê³ ëŠ” ê°œì¸ì‹ ìš©ì •ë³´ë¥¼ ì œ3ìì—ê²Œ ì œê³µí•˜ê±°ë‚˜ ëª©ì  ì™¸ì˜ ìš©ë„ë¡œ ì´ìš©í•  ìˆ˜ ì—†ë‹¤.",
                "ê¸ˆìœµíšŒì‚¬ëŠ” ë‚´ë¶€í†µì œê¸°ì¤€ì„ ë§ˆë ¨í•˜ì—¬ ì´ì‚¬íšŒì˜ ìŠ¹ì¸ì„ ë°›ê³  ì´ë¥¼ ì„±ì‹¤íˆ ì´í–‰í•˜ì—¬ì•¼ í•œë‹¤."
            ]
        }
        
        # ì´ˆê¸°í™” ìƒíƒœ
        self.initialized = False
    
    def initialize(self) -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ëª¨ë¸ ì‚¬ì „ ë¡œë”©)"""
        logger.info("=" * 60)
        logger.info("ğŸš€ FSKU ë°ì´í„° ì¦ê°• ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info("=" * 60)
        
        try:
            # 1. ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
            logger.info(f"\n[1/2] ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”...")
            logger.info(f"  - ëª¨ë¸: {self.model_name}")
            logger.info(f"  - ì²´ì´ë‹: {'âœ… ON' if self.use_chaining else 'âŒ OFF'}")
            logger.info(f"  - ì–‘ìí™”: {'âœ… ON' if self.use_quantization else 'âŒ OFF'}")
            
            self.generator = SimpleDataGenerator(
                model_name=self.model_name,
                use_chaining=self.use_chaining,
                use_quantization=self.use_quantization
            )
            
            if not self.generator.initialize():
                raise RuntimeError("ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 2. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if self.use_rag:
                logger.info("\n[2/2] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
                self.rag_system = SimpleRAGSystem()
                self.rag_system.initialize()
            else:
                logger.info("\n[2/2] RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™”")
            
            self.initialized = True
            logger.info("\nâœ… ëª¨ë“  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            self.initialized = False
            return False
    
    def verify_system(self) -> bool:
        """ì‹œìŠ¤í…œ ë™ì‘ ê²€ì¦"""
        if not self.initialized:
            logger.error("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info("\nğŸ” ì‹œìŠ¤í…œ ë™ì‘ ê²€ì¦ ì‹œì‘...")
        
        try:
            # 1. ê°„ë‹¨í•œ QA ìƒì„± í…ŒìŠ¤íŠ¸
            test_context = "í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê¸ˆìœµ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤."
            test_result = self.generator.generate_qa_pair(test_context, "ê°ê´€ì‹")
            
            if not test_result:
                logger.error("QA ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            logger.info(f"âœ… QA ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"  - ë¬¸ì œ: {test_result['question'][:50]}...")
            logger.info(f"  - ë‹µë³€: {test_result['answer'][:30]}...")
            
            # 2. RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (í™œì„±í™”ëœ ê²½ìš°)
            if self.use_rag and self.rag_system:
                test_results = self.rag_system.search("ê°œì¸ì •ë³´", top_k=1)
                if test_results:
                    logger.info(f"âœ… RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(test_results)}ê°œ ê²°ê³¼")
                else:
                    logger.warning("âš ï¸ RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            logger.info("âœ… ì‹œìŠ¤í…œ ê²€ì¦ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def run(self, target_count: int = 10, output_file: str = None) -> List[Dict]:
        """ë°ì´í„° ìƒì„± ì‹¤í–‰"""
        if not self.initialized:
            logger.error("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            return []
        
        logger.info(f"\nğŸ¯ ë°ì´í„° ìƒì„± ì‹œì‘")
        logger.info(f"  - ëª©í‘œ: {target_count}ê°œ")
        logger.info(f"  - ëª¨ë“œ: {'ğŸ”— ì²´ì´ë‹' if self.use_chaining else 'âš¡ ë‹¨ìˆœ'}")
        logger.info(f"  - RAG: {'âœ… ì‚¬ìš©' if self.use_rag else 'âŒ ë¯¸ì‚¬ìš©'}")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        contexts = self._prepare_contexts(target_count)
        
        # ìƒì„± ì‹¤í–‰
        start_time = time.time()
        results = []
        
        with tqdm(total=target_count, desc="ë°ì´í„° ìƒì„±") as pbar:
            for i, context in enumerate(contexts[:target_count]):
                qtype = self.config['question_types'][i % len(self.config['question_types'])]
                
                result = self.generator.generate_qa_pair(context, qtype)
                
                if result:
                    results.append(result)
                    pbar.update(1)
                    pbar.set_postfix({'ì„±ê³µ': len(results), 'ì‹¤íŒ¨': i + 1 - len(results)})
                else:
                    pbar.update(1)
                    pbar.set_postfix({'ì„±ê³µ': len(results), 'ì‹¤íŒ¨': i + 1 - len(results)})
        
        total_time = time.time() - start_time
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_results(results, target_count, total_time)
        
        # ê²°ê³¼ ì €ì¥
        if results:
            saved_file = self._save_results(results, output_file)
            logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {saved_file}")
        
        return results
    
    def _prepare_contexts(self, count: int) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        contexts = []
        
        if self.use_rag and self.rag_system and self.rag_system.documents:
            # RAG ê²€ìƒ‰
            topics = ["ê°œì¸ì •ë³´ë³´í˜¸", "ì „ìê¸ˆìœµê±°ë˜", "ê¸ˆìœµë³´ì•ˆ", "ìê¸ˆì„¸íƒë°©ì§€", 
                     "ì‹ ìš©ì •ë³´", "ë‚´ë¶€í†µì œ", "ì •ë³´ë³´ì•ˆ", "ì‚¬ì´ë²„ë³´ì•ˆ"]
            
            for topic in topics:
                search_results = self.rag_system.search(topic, top_k=3)
                contexts.extend(search_results)
            
            # ëœë¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            random_contexts = self.rag_system.get_random_context(n=count // 2)
            contexts.extend(random_contexts)
        
        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        contexts.extend(self.config['default_contexts'])
        
        # í•„ìš”í•œ ë§Œí¼ ë°˜ë³µ
        while len(contexts) < count:
            contexts.extend(contexts[:count - len(contexts)])
        
        # ì…”í”Œ
        random.shuffle(contexts)
        
        return contexts
    
    def _print_results(self, results: List[Dict], target_count: int, total_time: float):
        """ê²°ê³¼ ì¶œë ¥"""
        if results:
            success_rate = len(results) / target_count * 100
            avg_time = total_time / len(results)
            
            logger.info(f"\nğŸ‰ ìƒì„± ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ê²°ê³¼:")
            logger.info(f"  - ì„±ê³µ: {len(results)}/{target_count}ê°œ ({success_rate:.1f}%)")
            logger.info(f"  - ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
            logger.info(f"  - í‰ê·  ì‹œê°„: {avg_time:.1f}ì´ˆ/ê°œ")
            
            # í†µê³„
            stats = self.generator.get_stats()
            logger.info(f"\nğŸ“ˆ í†µê³„:")
            logger.info(f"  - ì„±ê³µë¥ : {stats['success_rate']}%")
            logger.info(f"  - í‰ê·  ìƒì„± ì‹œê°„: {stats['avg_generation_time']}ì´ˆ")
            logger.info(f"  - ëª¨ë“œ: {stats['mode']}")
            
            if stats['errors']:
                logger.info(f"  - ì˜¤ë¥˜: {stats['errors']}")
            
            # ìƒ˜í”Œ ì¶œë ¥
            logger.info(f"\nğŸ“‹ ìƒì„± ìƒ˜í”Œ:")
            for i, result in enumerate(results[:3]):
                logger.info(f"\n[ìƒ˜í”Œ {i+1}] ({result.get('method', 'unknown')})")
                logger.info(f"  ë¬¸ì œ: {result['question'][:100]}...")
                logger.info(f"  ë‹µë³€: {result['answer'][:50]}...")
                if 'explanation' in result and result['explanation']:
                    logger.info(f"  í•´ì„¤: {result['explanation'][:50]}...")
        else:
            logger.error("âŒ ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def _save_results(self, results: List[Dict], output_file: str = None) -> Path:
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if output_file:
            output_path = Path(output_file)
        else:
            filename = f"fsku_data_{timestamp}.json"
            output_path = OUTPUT_DIR / filename
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        output_data = {
            'metadata': {
                'timestamp': timestamp,
                'model': self.model_name,
                'use_chaining': self.use_chaining,
                'use_quantization': self.use_quantization,
                'use_rag': self.use_rag,
                'total_count': len(results),
                'stats': self.generator.get_stats()
            },
            'data': results
        }
        
        # ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="FSKU ë°ì´í„° ì¦ê°• ì‹œìŠ¤í…œ")
    
    # ê¸°ë³¸ ì˜µì…˜
    parser.add_argument('--model', type=str, default="microsoft/phi-2",
                       help="ì‚¬ìš©í•  ëª¨ë¸ (default: microsoft/phi-2)")
    parser.add_argument('--count', type=int, default=10,
                       help="ìƒì„±í•  ë°ì´í„° ê°œìˆ˜ (default: 10)")
    parser.add_argument('--output', type=str, default=None,
                       help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ")
    
    # ëª¨ë“œ ì˜µì…˜
    parser.add_argument('--chaining', action='store_true',
                       help="ì²´ì´ë‹ ëª¨ë“œ ì‚¬ìš© (ê³ í’ˆì§ˆ)")
    parser.add_argument('--quantization', action='store_true',
                       help="4bit ì–‘ìí™” ì‚¬ìš©")
    parser.add_argument('--no-rag', action='store_true',
                       help="RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™”")
    
    # í…ŒìŠ¤íŠ¸ ì˜µì…˜
    parser.add_argument('--test', action='store_true',
                       help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (5ê°œë§Œ ìƒì„±)")
    parser.add_argument('--verify', action='store_true',
                       help="ì‹œìŠ¤í…œ ê²€ì¦ë§Œ ìˆ˜í–‰")
    
    args = parser.parse_args()
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.test:
        args.count = 5
        logger.info("âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œë§Œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # ì‹œìŠ¤í…œ ìƒì„±
    logger.info("ğŸš€ FSKU ë°ì´í„° ì¦ê°• ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info(f"ì„¤ì •: ëª¨ë¸={args.model}, ì²´ì´ë‹={args.chaining}, ì–‘ìí™”={args.quantization}")
    
    system = FSKUAugmentationSystem(
        model_name=args.model,
        use_chaining=args.chaining,
        use_quantization=args.quantization,
        use_rag=not args.no_rag
    )
    
    # ì´ˆê¸°í™” (ëª¨ë¸ ì‚¬ì „ ë¡œë”©)
    if not system.initialize():
        logger.error("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return 1
    
    # ì‹œìŠ¤í…œ ê²€ì¦
    if not system.verify_system():
        logger.error("ì‹œìŠ¤í…œ ê²€ì¦ ì‹¤íŒ¨")
        return 1
    
    # ê²€ì¦ë§Œ ìˆ˜í–‰
    if args.verify:
        logger.info("âœ… ì‹œìŠ¤í…œ ê²€ì¦ ì™„ë£Œ. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return 0
    
    # ë°ì´í„° ìƒì„±
    results = system.run(
        target_count=args.count,
        output_file=args.output
    )
    
    if results:
        logger.info(f"âœ… ì„±ê³µì ìœ¼ë¡œ {len(results)}ê°œ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        return 0
    else:
        logger.error("âŒ ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
        return 1


if __name__ == "__main__":
    exit(main())