#!/usr/bin/env python3
"""
í–¥ìƒëœ RAG (Retrieval Augmented Generation) ë¬¸ì„œ ê²€ìƒ‰ ëª¨ë“ˆ

PDF ë¬¸ì„œ ì²˜ë¦¬, ì²­í‚¹, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›
"""

import json
import re
import pickle
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
import logging
from collections import defaultdict, Counter
import numpy as np

logger = logging.getLogger(__name__)


class DocumentRetriever:
    """í–¥ìƒëœ ë¬¸ì„œ ê²€ìƒ‰ ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str = "data", use_embedding: bool = True, use_cache: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            use_embedding: ë²¡í„° ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)
            use_cache: ìºì‹œëœ ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€
        """
        self.data_dir = Path(data_dir)
        self.documents = []
        self.chunks = []  # ì²­í‚¹ëœ ë¬¸ì„œ
        self.chunk_index = defaultdict(list)  # í‚¤ì›Œë“œ -> ì²­í¬ ì¸ë±ìŠ¤
        self.use_embedding = use_embedding
        self.embeddings = None
        self.embedding_model = None
        self.faiss_index = None
        self.index_path = self.data_dir / "vectordb" / "index.pkl"
        
        # PDF ë¡œë”ì™€ ì²­ì»¤ ì´ˆê¸°í™”
        try:
            from pdf_loader import DocumentLoader
            from chunker import DocumentChunker
        except ImportError:
            from .pdf_loader import DocumentLoader
            from .chunker import DocumentChunker
        
        self.loader = DocumentLoader()
        # Colab ì„¤ì •ê³¼ ë™ì¼í•˜ê²Œ ìµœì í™”
        self.chunker = DocumentChunker(chunk_size=300, chunk_overlap=50)
        
        # ìºì‹œ í™•ì¸ ë° ë¡œë“œ
        if use_cache and self.index_path.exists():
            print("ğŸ“‚ ìºì‹œëœ ì¸ë±ìŠ¤ ë°œê²¬...")
            if self.load_index():
                print("âœ… ìºì‹œì—ì„œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
                # ì„ë² ë”© ì´ˆê¸°í™” (ì„ íƒì )
                if use_embedding and self.embeddings is None:
                    self._initialize_embeddings()
                return
        
        # ìºì‹œê°€ ì—†ê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨ì‹œ ìƒˆë¡œ ìƒì„±
        print("ğŸ”„ ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        self._load_and_process_documents()
        self._build_index()
        
        # ì¸ë±ìŠ¤ ì €ì¥
        self.save_index()
        
        # ì„ë² ë”© ì´ˆê¸°í™” (ì„ íƒì )
        if use_embedding:
            self._initialize_embeddings()
    
    def _load_and_process_documents(self):
        """ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹ ì²˜ë¦¬"""
        external_dir = self.data_dir / "external"
        
        if not external_dir.exists():
            logger.warning(f"ì™¸ë¶€ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {external_dir}")
            return
        
        print("ğŸ“š ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘...")
        
        # ëª¨ë“  ë¬¸ì„œ ë¡œë“œ
        self.documents = self.loader.load_directory(external_dir)
        
        # ê° ë¬¸ì„œë¥¼ ì²­í‚¹
        for doc in self.documents:
            doc_chunks = self.chunker.chunk_document(doc)
            
            # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
            for chunk in doc_chunks:
                chunk['source'] = doc['metadata']['source']
                chunk['doc_type'] = doc['metadata']['type']
            
            self.chunks.extend(doc_chunks)
        
        print(f"âœ… {len(self.documents)}ê°œ ë¬¸ì„œì—ì„œ {len(self.chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    def _build_index(self):
        """ì²­í¬ ì¸ë±ìŠ¤ êµ¬ì¶• (BM25ìš©)"""
        print("ğŸ”¨ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        for idx, chunk in enumerate(self.chunks):
            content = chunk['content'].lower()
            
            # ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£]+|[a-zA-Z]+|\d+', content)
            
            # ê° ë‹¨ì–´ì— ëŒ€í•´ ì²­í¬ ì¸ë±ìŠ¤ ì €ì¥
            for word in set(words):
                if len(word) >= 2:
                    self.chunk_index[word].append(idx)
            
            # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if 'keywords' in chunk:
                for keyword in chunk['keywords']:
                    self.chunk_index[keyword.lower()].append(idx)
        
        print(f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.chunk_index)}ê°œ í‚¤ì›Œë“œ")
    
    def _initialize_embeddings(self):
        """ë²¡í„° ì„ë² ë”© ì´ˆê¸°í™” - í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ìš°ì„ """
        try:
            from sentence_transformers import SentenceTransformer
            
            # Colabê³¼ ë™ì¼í•œ í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©
            model_name = "jhgan/ko-sbert-nli"
            print(f"ğŸ¤– í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
            
            self.embedding_model = SentenceTransformer(model_name)
            print("âœ… í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë“  ì²­í¬ ì„ë² ë”© (ì •ê·œí™” í¬í•¨)
            texts = [chunk['content'] for chunk in self.chunks]
            self.embeddings = self.embedding_model.encode(
                texts, 
                normalize_embeddings=True,  # Colabê³¼ ë™ì¼
                show_progress_bar=True
            )
            
            print(f"âœ… {len(self.embeddings)}ê°œ ì²­í¬ ì„ë² ë”© ì™„ë£Œ")
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶• (ì„ íƒì )
            self._build_faiss_index()
            
        except ImportError:
            logger.warning("sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            logger.warning("pip install sentence-transformers")
            self.use_embedding = False
    
    def _build_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            import faiss
            
            # ì„ë² ë”© ì°¨ì›
            d = self.embeddings.shape[1]
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„±
            self.faiss_index = faiss.IndexFlatIP(d)  # ë‚´ì  ìœ ì‚¬ë„
            
            # ì •ê·œí™”
            faiss.normalize_L2(self.embeddings)
            
            # ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.faiss_index.add(self.embeddings)
            
            print(f"âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            
        except ImportError:
            logger.warning("faissê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            logger.warning("pip install faiss-cpu")
    
    def search(self, query: str, top_k: int = 3, method: str = "similarity") -> str:
        """
        ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ì²­í¬ ìˆ˜
            method: ê²€ìƒ‰ ë°©ë²• (bm25, embedding, hybrid)
            
        Returns:
            ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
        """
        if not self.chunks:
            return ""
        
        if method == "similarity" and self.use_embedding:
            # ê¸°ë³¸: ì„ë² ë”© ìœ ì‚¬ë„ ê²€ìƒ‰ (Colab ë°©ì‹)
            results = self._embedding_search(query, top_k * 2)
        elif method == "hybrid" and self.use_embedding:
            results = self._hybrid_search(query, top_k * 2)
        elif method == "bm25":
            results = self._bm25_search(query, top_k * 2)
        else:
            # í´ë°±: BM25
            results = self._bm25_search(query, top_k * 2)
        
        # ìƒìœ„ kê°œ ì„ íƒ ë° ì¤‘ë³µ ì œê±°
        seen_content = set()
        final_results = []
        
        for chunk_idx, score in results[:top_k*2]:
            chunk = self.chunks[chunk_idx]
            content_hash = hash(chunk['content'][:100])  # ì•ë¶€ë¶„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                final_results.append((chunk, score))
                
                if len(final_results) >= top_k:
                    break
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        contexts = []
        for chunk, score in final_results:
            source = chunk.get('source', 'Unknown')
            content = chunk['content']
            
            # ì†ŒìŠ¤ ì •ë³´ í¬í•¨
            context = f"[ì¶œì²˜: {source}]\n{content}"
            contexts.append(context)
        
        return "\n\n---\n\n".join(contexts)
    
    def _bm25_search(self, query: str, top_k: int) -> List[Tuple[int, float]]:
        """BM25 ê¸°ë°˜ ê²€ìƒ‰"""
        query_lower = query.lower()
        query_words = re.findall(r'[ê°€-í£]+|[a-zA-Z]+|\d+', query_lower)
        
        # BM25 íŒŒë¼ë¯¸í„°
        k1 = 1.2
        b = 0.75
        
        # ë¬¸ì„œ ê¸¸ì´ í‰ê· 
        avg_len = np.mean([len(chunk['content']) for chunk in self.chunks])
        
        # ê° ì²­í¬ì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°
        scores = defaultdict(float)
        
        for word in query_words:
            if word in self.chunk_index:
                # IDF ê³„ì‚°
                df = len(self.chunk_index[word])
                idf = np.log((len(self.chunks) - df + 0.5) / (df + 0.5) + 1)
                
                # ê° ë¬¸ì„œì— ëŒ€í•œ TF ê³„ì‚°
                for chunk_idx in self.chunk_index[word]:
                    chunk = self.chunks[chunk_idx]
                    tf = chunk['content'].lower().count(word)
                    doc_len = len(chunk['content'])
                    
                    # BM25 ì ìˆ˜
                    score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_len))
                    scores[chunk_idx] += score
        
        # ì •ë ¬
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_scores[:top_k]
    
    def _embedding_search(self, query: str, top_k: int) -> List[Tuple[int, float]]:
        """ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰"""
        if not self.use_embedding or self.embedding_model is None:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© (ì •ê·œí™” í¬í•¨)
        query_embedding = self.embedding_model.encode(
            [query],
            normalize_embeddings=True
        )
        
        if hasattr(self, 'faiss_index'):
            # FAISS ì‚¬ìš©
            import faiss
            faiss.normalize_L2(query_embedding)
            scores, indices = self.faiss_index.search(query_embedding, top_k)
            results = [(int(idx), float(score)) for idx, score in zip(indices[0], scores[0])]
        else:
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            top_indices = np.argsort(similarities)[::-1][:top_k]
            results = [(int(idx), float(similarities[idx])) for idx in top_indices]
        
        return results
    
    def _hybrid_search(self, query: str, top_k: int) -> List[Tuple[int, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ì„ë² ë”©)"""
        # BM25 ê²€ìƒ‰
        bm25_results = self._bm25_search(query, top_k)
        bm25_scores = {idx: score for idx, score in bm25_results}
        
        # ì„ë² ë”© ê²€ìƒ‰
        embedding_results = self._embedding_search(query, top_k)
        embedding_scores = {idx: score for idx, score in embedding_results}
        
        # ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        all_indices = set(bm25_scores.keys()) | set(embedding_scores.keys())
        
        hybrid_scores = {}
        for idx in all_indices:
            # ê°€ì¤‘ í‰ê·  (BM25: 0.2, ì„ë² ë”©: 0.8) - ì„ë² ë”© ë¹„ì¤‘ ì¦ê°€
            bm25_score = bm25_scores.get(idx, 0)
            embedding_score = embedding_scores.get(idx, 0)
            
            # ì •ê·œí™”
            max_bm25 = max(bm25_scores.values()) if bm25_scores else 1
            max_embedding = max(embedding_scores.values()) if embedding_scores else 1
            
            normalized_bm25 = bm25_score / max_bm25 if max_bm25 > 0 else 0
            normalized_embedding = embedding_score / max_embedding if max_embedding > 0 else 0
            
            # ê°€ì¤‘ ê²°í•©
            hybrid_scores[idx] = 0.2 * normalized_bm25 + 0.8 * normalized_embedding
        
        # ì •ë ¬
        sorted_scores = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_scores[:top_k]
    
    def get_random_chunks(self, n: int = 3) -> str:
        """
        ëœë¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸° (ê°œë… ì¶”ì¶œ ëŒ€ì²´ìš©)
        
        Args:
            n: ê°€ì ¸ì˜¬ ì²­í¬ ìˆ˜
            
        Returns:
            ëœë¤ ì²­í¬ë“¤ì˜ í…ìŠ¤íŠ¸
        """
        if not self.chunks:
            return ""
        
        import random
        selected_chunks = random.sample(self.chunks, min(n, len(self.chunks)))
        
        contexts = []
        for chunk in selected_chunks:
            source = chunk.get('source', 'Unknown')
            content = chunk['content']
            context = f"[ì¶œì²˜: {source}]\n{content}"
            contexts.append(context)
        
        return "\n\n---\n\n".join(contexts)
    
    def get_statistics(self) -> Dict:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        stats = {
            'total_documents': len(self.documents),
            'total_chunks': len(self.chunks),
            'total_keywords': len(self.chunk_index),
            'sources': list(set(chunk.get('source', 'Unknown') for chunk in self.chunks)),
            'avg_chunk_size': np.mean([chunk.get('tokens', 0) for chunk in self.chunks]) if self.chunks else 0,
            'use_embedding': self.use_embedding
        }
        
        # ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„
        doc_types = Counter(chunk.get('doc_type', 'Unknown') for chunk in self.chunks)
        stats['document_types'] = dict(doc_types)
        
        return stats
    
    def save_index(self, path: str = "data/vectordb/index.pkl"):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        save_path = Path(path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        save_data = {
            'chunks': self.chunks,
            'chunk_index': dict(self.chunk_index),
            'embeddings': self.embeddings
        }
        
        with open(save_path, 'wb') as f:
            pickle.dump(save_data, f)
        
        print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    def load_index(self, path: str = "data/vectordb/index.pkl"):
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        load_path = Path(path)
        
        if not load_path.exists():
            logger.warning(f"ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {load_path}")
            return False
        
        with open(load_path, 'rb') as f:
            save_data = pickle.load(f)
        
        self.chunks = save_data['chunks']
        self.chunk_index = defaultdict(list, save_data['chunk_index'])
        self.embeddings = save_data['embeddings']
        
        print(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.chunks)}ê°œ ì²­í¬")
        return True


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("="*60)
    print("RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # RAG ì´ˆê¸°í™” (ì„ë² ë”© ì—†ì´)
    retriever = DocumentRetriever(use_embedding=False)
    
    # í†µê³„ ì¶œë ¥
    stats = retriever.get_statistics()
    print(f"\nğŸ“Š í†µê³„:")
    print(f"- ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
    print(f"- ì²­í¬ ìˆ˜: {stats['total_chunks']}")
    print(f"- í‚¤ì›Œë“œ ìˆ˜: {stats['total_keywords']}")
    print(f"- ë¬¸ì„œ íƒ€ì…: {stats['document_types']}")
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    test_queries = [
        "ì „ìê¸ˆìœµê±°ë˜",
        "ê°œì¸ì •ë³´ë³´í˜¸",
        "ì•”í˜¸ê¸°ìˆ "
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ê²€ìƒ‰: '{query}'")
        results = retriever.search(query, top_k=2)
        
        if results:
            print(f"ê²°ê³¼:\n{results[:200]}...")
        else:
            print("ê²°ê³¼ ì—†ìŒ")
    
    # ì¸ë±ìŠ¤ ì €ì¥
    retriever.save_index()