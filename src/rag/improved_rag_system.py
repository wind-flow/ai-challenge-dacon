#!/usr/bin/env python3
"""
ê°œì„ ëœ RAG ì‹œìŠ¤í…œ - ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™”
ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ê³¼ ë‹¨ê¸° ê³¼ì œ êµ¬í˜„
"""

import os
import gc
import json
import pickle
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

import torch
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
import faiss

# ============================================================
# 1. ë¡œê¹… ì‹œìŠ¤í…œ ê°œì„  (ì¦‰ì‹œ ì ìš©)
# ============================================================

# êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •
def setup_logging(log_file: str = 'fsku_processing.log'):
    """ê°œì„ ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    
    # ë¡œê·¸ í¬ë§· ì •ì˜
    log_format = '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    
    # ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # ì„±ëŠ¥ ë¡œê±° ë³„ë„ ì„¤ì •
    perf_logger = logging.getLogger('performance')
    perf_handler = logging.FileHandler('performance.log', encoding='utf-8')
    perf_handler.setFormatter(logging.Formatter(log_format))
    perf_logger.addHandler(perf_handler)
    
    return logging.getLogger(__name__)

logger = setup_logging()

# ============================================================
# 2. ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ (ì¦‰ì‹œ ì ìš©)
# ============================================================

class MemoryManager:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ìë™ ì •ë¦¬"""
    
    def __init__(self, threshold_gb: float = 20.0):
        self.threshold_gb = threshold_gb
        self.cleanup_count = 0
        
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        import psutil
        
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            'rss_gb': memory_info.rss / (1024**3),  # ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            'vms_gb': memory_info.vms / (1024**3),  # ê°€ìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            'percent': process.memory_percent(),
            'available_gb': psutil.virtual_memory().available / (1024**3)
        }
    
    def cleanup(self, force: bool = False):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        memory = self.get_memory_usage()
        
        if force or memory['rss_gb'] > self.threshold_gb:
            logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘ (í˜„ì¬: {memory['rss_gb']:.2f}GB)")
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # PyTorch CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (Mac)
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
                torch.mps.synchronize()
            
            self.cleanup_count += 1
            
            # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
            memory_after = self.get_memory_usage()
            freed = memory['rss_gb'] - memory_after['rss_gb']
            logger.info(f"âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ (í•´ì œ: {freed:.2f}GB, í˜„ì¬: {memory_after['rss_gb']:.2f}GB)")
            
            return freed
        return 0
    
    def monitor_decorator(self, func):
        """í•¨ìˆ˜ ì‹¤í–‰ ì „í›„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
        def wrapper(*args, **kwargs):
            # ì‹¤í–‰ ì „ ë©”ëª¨ë¦¬
            before = self.get_memory_usage()
            logger.debug(f"ë©”ëª¨ë¦¬ ì‚¬ìš© ì „: {before['rss_gb']:.2f}GB")
            
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ì‹¤í–‰ í›„ ë©”ëª¨ë¦¬
            after = self.get_memory_usage()
            used = after['rss_gb'] - before['rss_gb']
            
            if used > 1.0:  # 1GB ì´ìƒ ì¦ê°€ì‹œ ê²½ê³ 
                logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ê¸‰ì¦: {used:.2f}GB (í•¨ìˆ˜: {func.__name__})")
            
            # ìë™ ì •ë¦¬
            self.cleanup()
            
            return result
        return wrapper

# ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
memory_manager = MemoryManager(threshold_gb=20.0)

# ============================================================
# 3. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” (ë‹¨ê¸° ê³¼ì œ)
# ============================================================

@dataclass
class BatchConfig:
    """ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •"""
    batch_size: int = 32
    max_batch_size: int = 128
    min_batch_size: int = 4
    adaptive: bool = True
    memory_threshold_gb: float = 18.0

class AdaptiveBatchProcessor:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬"""
    
    def __init__(self, config: BatchConfig = None):
        self.config = config or BatchConfig()
        self.current_batch_size = self.config.batch_size
        self.performance_history = []
        
    def get_optimal_batch_size(self) -> int:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if not self.config.adaptive:
            return self.current_batch_size
        
        memory = memory_manager.get_memory_usage()
        available_gb = memory['available_gb']
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if available_gb < 4:
            self.current_batch_size = self.config.min_batch_size
        elif available_gb < 8:
            self.current_batch_size = min(16, self.config.batch_size)
        elif available_gb < 16:
            self.current_batch_size = self.config.batch_size
        else:
            self.current_batch_size = min(
                self.config.max_batch_size,
                self.config.batch_size * 2
            )
        
        logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ê¸° ì¡°ì •: {self.current_batch_size} (ê°€ìš© ë©”ëª¨ë¦¬: {available_gb:.2f}GB)")
        return self.current_batch_size
    
    def process_batch(self, items: List[Any], process_func, **kwargs) -> List[Any]:
        """ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ with ë©”ëª¨ë¦¬ ê´€ë¦¬"""
        batch_size = self.get_optimal_batch_size()
        results = []
        
        # ì§„í–‰ë¥  í‘œì‹œ
        total_batches = (len(items) + batch_size - 1) // batch_size
        
        with tqdm(total=len(items), desc="ë°°ì¹˜ ì²˜ë¦¬") as pbar:
            for i in range(0, len(items), batch_size):
                batch = items[i:i + batch_size]
                
                # ë°°ì¹˜ ì²˜ë¦¬
                try:
                    batch_results = process_func(batch, **kwargs)
                    results.extend(batch_results)
                    
                    # ì„±ëŠ¥ ê¸°ë¡
                    self.performance_history.append({
                        'batch_size': len(batch),
                        'memory_used': memory_manager.get_memory_usage()['rss_gb'],
                        'timestamp': datetime.now()
                    })
                    
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê³  ì¬ì‹œë„
                    if batch_size > self.config.min_batch_size:
                        self.current_batch_size = max(
                            self.config.min_batch_size,
                            batch_size // 2
                        )
                        logger.info(f"ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {self.current_batch_size}")
                    raise
                
                # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (10 ë°°ì¹˜ë§ˆë‹¤)
                if (i // batch_size) % 10 == 0:
                    memory_manager.cleanup()
                
                pbar.update(len(batch))
        
        return results

# ============================================================
# 4. ë¹„ë™ê¸° PDF ì²˜ë¦¬ (ë‹¨ê¸° ê³¼ì œ)
# ============================================================

class AsyncPDFProcessor:
    """ë¹„ë™ê¸° PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
    async def process_pdf_async(self, pdf_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ PDF ë¹„ë™ê¸° ì²˜ë¦¬"""
        loop = asyncio.get_event_loop()
        
        try:
            # CPU ì§‘ì•½ì  ì‘ì—…ì€ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
            result = await loop.run_in_executor(
                self.executor,
                self._process_pdf_sync,
                pdf_path
            )
            return result
            
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨ ({pdf_path}): {e}")
            return {'error': str(e), 'path': str(pdf_path)}
    
    def _process_pdf_sync(self, pdf_path: Path) -> Dict[str, Any]:
        """ë™ê¸° PDF ì²˜ë¦¬ (ì‹¤ì œ ë¡œì§)"""
        import PyPDF2
        
        logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
        
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = []
                for page_num in range(len(reader.pages)):
                    page = reader.pages[page_num]
                    text_content.append(page.extract_text())
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = {
                    'title': reader.metadata.get('/Title', 'Unknown'),
                    'pages': len(reader.pages),
                    'file_size': pdf_path.stat().st_size,
                    'processed_at': datetime.now().isoformat()
                }
                
                return {
                    'path': str(pdf_path),
                    'text': '\n'.join(text_content),
                    'metadata': metadata
                }
                
        except Exception as e:
            logger.error(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")
            raise
    
    async def process_pdfs_batch(self, pdf_paths: List[Path]) -> List[Dict]:
        """ì—¬ëŸ¬ PDF ë™ì‹œ ì²˜ë¦¬"""
        logger.info(f"ğŸš€ {len(pdf_paths)}ê°œ PDF ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘")
        
        # ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        tasks = [self.process_pdf_async(path) for path in pdf_paths]
        
        # ë™ì‹œ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ë¦¬
        successful = [r for r in results if not isinstance(r, Exception) and 'error' not in r]
        failed = [r for r in results if isinstance(r, Exception) or 'error' in r]
        
        logger.info(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {len(successful)}ê°œ, ì‹¤íŒ¨ {len(failed)}ê°œ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        memory_manager.cleanup()
        
        return results

# ============================================================
# 5. ê°œì„ ëœ RAG ì‹œìŠ¤í…œ (í†µí•©)
# ============================================================

class ImprovedRAGSystem:
    """ì„±ëŠ¥ ìµœì í™”ëœ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 model_name: str = "jhgan/ko-sroberta-multitask",
                 cache_dir: str = "data/cache",
                 use_async: bool = True):
        
        self.model_name = model_name
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.use_async = use_async
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.memory_manager = memory_manager
        self.batch_processor = AdaptiveBatchProcessor()
        self.pdf_processor = AsyncPDFProcessor()
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €
        self.tokenizer = None
        self.model = None
        self.device = self._get_device()
        
        # ë²¡í„° ì €ì¥ì†Œ
        self.index = None
        self.documents = []
        
        logger.info(f"âœ… ImprovedRAGSystem ì´ˆê¸°í™” ì™„ë£Œ (Device: {self.device})")
    
    def _get_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    @memory_manager.monitor_decorator
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ with ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
        logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”©: {self.model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            
            # ë””ë°”ì´ìŠ¤ ì´ë™
            if self.device != "cpu":
                self.model = self.model.to(self.device)
            
            # í‰ê°€ ëª¨ë“œ
            self.model.eval()
            
            logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    @lru_cache(maxsize=1000)
    def get_embedding(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ìºì‹±)"""
        with torch.no_grad():
            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            
            if self.device != "cpu":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            outputs = self.model(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
            
            return embeddings.cpu().numpy()[0]
    
    def process_documents_batch(self, texts: List[str]) -> np.ndarray:
        """ë°°ì¹˜ ë‹¨ìœ„ ë¬¸ì„œ ì²˜ë¦¬"""
        
        def batch_embed(batch_texts):
            embeddings = []
            for text in batch_texts:
                emb = self.get_embedding(text)
                embeddings.append(emb)
            return embeddings
        
        # ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬
        all_embeddings = self.batch_processor.process_batch(
            texts,
            batch_embed
        )
        
        return np.array(all_embeddings)
    
    async def build_index_async(self, pdf_dir: Path):
        """ë¹„ë™ê¸° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        logger.info("ğŸ”¨ ë¹„ë™ê¸° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        
        # PDF íŒŒì¼ ëª©ë¡
        pdf_files = list(pdf_dir.glob("*.pdf"))
        
        if not pdf_files:
            logger.warning("PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ë¹„ë™ê¸° PDF ì²˜ë¦¬
        pdf_results = await self.pdf_processor.process_pdfs_batch(pdf_files)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
        all_chunks = []
        for result in pdf_results:
            if 'text' in result:
                chunks = self._chunk_text(result['text'])
                all_chunks.extend(chunks)
        
        logger.info(f"ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
        embeddings = self.process_documents_batch(all_chunks)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings.astype('float32'))
        
        # ë¬¸ì„œ ì €ì¥
        self.documents = all_chunks
        
        # ìºì‹œ ì €ì¥
        self._save_cache()
        
        logger.info(f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def _chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """í…ìŠ¤íŠ¸ ì²­í‚¹"""
        chunks = []
        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            if chunk:
                chunks.append(chunk)
        return chunks
    
    def _save_cache(self):
        """ì¸ë±ìŠ¤ ìºì‹œ ì €ì¥"""
        cache_path = self.cache_dir / "index_cache.pkl"
        
        cache_data = {
            'index': faiss.serialize_index(self.index),
            'documents': self.documents,
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'num_documents': len(self.documents),
                'model': self.model_name
            }
        }
        
        with open(cache_path, 'wb') as f:
            pickle.dump(cache_data, f)
        
        logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_path}")
    
    def search(self, query: str, k: int = 5) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        if self.index is None:
            logger.error("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.get_embedding(query)
        query_embedding = np.array([query_embedding]).astype('float32')
        
        # ê²€ìƒ‰
        distances, indices = self.index.search(query_embedding, k)
        
        # ê²°ê³¼ ì •ë¦¬
        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.documents):
                results.append({
                    'rank': i + 1,
                    'text': self.documents[idx],
                    'distance': float(dist),
                    'similarity': 1 / (1 + float(dist))
                })
        
        return results
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # ëª¨ë¸ ì •ë¦¬
        if self.model is not None:
            del self.model
            self.model = None
        
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        # ì¸ë±ìŠ¤ ì •ë¦¬
        if self.index is not None:
            del self.index
            self.index = None
        
        # ë©”ëª¨ë¦¬ ê°•ì œ ì •ë¦¬
        memory_manager.cleanup(force=True)
        
        logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ============================================================
# 6. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°
# ============================================================

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë³´ê³ """
    
    def __init__(self):
        self.metrics = []
        
    def record(self, operation: str, duration: float, memory_used: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics.append({
            'operation': operation,
            'duration': duration,
            'memory_gb': memory_used,
            'timestamp': datetime.now()
        })
    
    def report(self) -> Dict:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        if not self.metrics:
            return {}
        
        report = {
            'total_operations': len(self.metrics),
            'total_duration': sum(m['duration'] for m in self.metrics),
            'avg_duration': np.mean([m['duration'] for m in self.metrics]),
            'max_memory': max(m['memory_gb'] for m in self.metrics),
            'avg_memory': np.mean([m['memory_gb'] for m in self.metrics])
        }
        
        logger.info("ğŸ“Š ì„±ëŠ¥ ë³´ê³ ì„œ:")
        for key, value in report.items():
            logger.info(f"  {key}: {value:.2f}")
        
        return report

# ============================================================
# 7. ì‚¬ìš© ì˜ˆì‹œ
# ============================================================

async def main_example():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = ImprovedRAGSystem()
    
    # ëª¨ë¸ ë¡œë“œ
    rag.load_model()
    
    # PDF ë””ë ‰í† ë¦¬ ì„¤ì •
    pdf_dir = Path("data/external")
    
    # ë¹„ë™ê¸° ì¸ë±ìŠ¤ êµ¬ì¶•
    if rag.use_async:
        await rag.build_index_async(pdf_dir)
    else:
        # ë™ê¸° ë°©ì‹ í´ë°±
        logger.info("ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì „í™˜")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query = "ê¸ˆìœµ ìƒí’ˆ ê°€ì… ì¡°ê±´ì€?"
    results = rag.search(query, k=3)
    
    logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
    for result in results:
        logger.info(f"  [{result['rank']}] ìœ ì‚¬ë„: {result['similarity']:.4f}")
        logger.info(f"      {result['text'][:100]}...")
    
    # ì •ë¦¬
    rag.cleanup()
    
    # ì„±ëŠ¥ ë³´ê³ 
    monitor = PerformanceMonitor()
    monitor.report()

if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main_example())