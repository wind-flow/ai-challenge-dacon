{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 FSKU 데이터 생성 파이프라인\n",
    "\n",
    "고성능 환경에서 금융보안 데이터를 대량 생성하고 체이닝으로 품질을 극대화하는 노트북입니다.\n",
    "\n",
    "## 📋 목차\n",
    "1. 환경 설정 및 패키지 설치\n",
    "2. 기존 모듈 임포트 및 설정\n",
    "3. 체이닝 기능이 추가된 DataGenerator\n",
    "4. RAG 시스템 초기화\n",
    "5. 프롬프트 체이닝 템플릿\n",
    "6. 대규모 배치 생성\n",
    "7. 품질 모니터링 대시보드\n",
    "8. 메인 실행 및 데이터 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 환경 확인\n",
    "!nvidia-smi\n",
    "\n",
    "# 필수 패키지 설치 (고성능 환경용)\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q langchain sentence-transformers faiss-gpu\n",
    "!pip install -q PyPDF2 pdfplumber pandas tqdm matplotlib\n",
    "!pip install -q flash-attn --no-build-isolation  # Flash Attention 2\n",
    "!pip install -q datasets tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기존 모듈 임포트 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 프로젝트 경로 설정\n",
    "PROJECT_ROOT = Path('/workspace/ai-dacon')  # RunPod 환경에 맞게 수정\n",
    "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# 기존 모듈 임포트\n",
    "from rag.retriever import DocumentRetriever\n",
    "from rag.pdf_loader import DocumentLoader\n",
    "from rag.chunker import DocumentChunker\n",
    "from generate_data.main import DataGenerator\n",
    "from generate_data.quality_checker import QualityChecker\n",
    "\n",
    "print(\"✅ 모듈 임포트 완료\")\n",
    "print(f\"🔧 프로젝트 경로: {PROJECT_ROOT}\")\n",
    "print(f\"🔥 PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"💻 CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 GPU 개수: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   - GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정 정의 (리소스 제약 없음)\n",
    "config = {\n",
    "    # 모델 설정\n",
    "    'model_name': 'Qwen/Qwen2.5-72B-Instruct',  # 메인 생성 모델 (대형)\n",
    "    'validation_model': 'meta-llama/Llama-3-70B-Instruct',  # 검증용 모델\n",
    "    'use_quantization': False,  # 양자화 사용 안함 (전체 정밀도)\n",
    "    'device_map': 'auto',  # 멀티 GPU 자동 분배\n",
    "    \n",
    "    # 생성 설정\n",
    "    'num_questions': 10000,  # 목표 생성 수\n",
    "    'batch_size': 32,  # 대규모 배치\n",
    "    'temperature': 0.8,\n",
    "    'top_p': 0.9,\n",
    "    'max_new_tokens': 1024,\n",
    "    \n",
    "    # RAG 설정\n",
    "    'use_rag': True,\n",
    "    'search_method': 'hybrid',  # hybrid, similarity, bm25\n",
    "    'top_k_retrieval': 5,\n",
    "    \n",
    "    # 체이닝 설정\n",
    "    'use_chaining': True,\n",
    "    'chain_steps': ['generate', 'validate', 'refine', 'final_check'],\n",
    "    'min_quality_score': 80,\n",
    "    \n",
    "    # 저장 설정\n",
    "    'output_dir': PROJECT_ROOT / 'data' / 'generated',\n",
    "    'checkpoint_interval': 1000,  # 1000개마다 체크포인트\n",
    "    \n",
    "    # 모니터링\n",
    "    'enable_monitoring': True,\n",
    "    'update_interval': 100  # 100개마다 대시보드 업데이트\n",
    "}\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "config['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"📁 출력 디렉토리: {config['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 체이닝 기능이 추가된 DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainDataGenerator(DataGenerator):\n",
    "    \"\"\"체이닝 기능이 추가된 데이터 생성기\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"초기화\"\"\"\n",
    "        super().__init__(config)\n",
    "        self.chain_steps = config.get('chain_steps', ['generate'])\n",
    "        self.validation_model = None\n",
    "        self.validation_tokenizer = None\n",
    "        \n",
    "        # 검증용 모델 로드\n",
    "        if config.get('validation_model') and 'validate' in self.chain_steps:\n",
    "            self._load_validation_model()\n",
    "    \n",
    "    def _load_validation_model(self):\n",
    "        \"\"\"검증용 모델 로드\"\"\"\n",
    "        print(f\"🔄 검증 모델 로딩: {self.config['validation_model']}\")\n",
    "        \n",
    "        self.validation_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config['validation_model'],\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if self.validation_tokenizer.pad_token is None:\n",
    "            self.validation_tokenizer.pad_token = self.validation_tokenizer.eos_token\n",
    "        \n",
    "        # 전체 정밀도로 로드 (리소스 제약 없음)\n",
    "        self.validation_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config['validation_model'],\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"✅ 검증 모델 로드 완료\")\n",
    "    \n",
    "    def generate_with_chain(self, prompt: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"체이닝을 통한 문제 생성\"\"\"\n",
    "        result = {\n",
    "            'prompt': prompt,\n",
    "            'context': context,\n",
    "            'chain_history': []\n",
    "        }\n",
    "        \n",
    "        current_output = None\n",
    "        \n",
    "        for step in self.chain_steps:\n",
    "            if step == 'generate':\n",
    "                # 1단계: 초기 생성\n",
    "                current_output = self._generate_with_llm(prompt, self.config['temperature'])\n",
    "                result['initial_output'] = current_output\n",
    "                \n",
    "            elif step == 'validate' and self.validation_model:\n",
    "                # 2단계: 검증\n",
    "                validation_result = self._validate_output(current_output)\n",
    "                result['validation_score'] = validation_result['score']\n",
    "                result['validation_feedback'] = validation_result['feedback']\n",
    "                \n",
    "                if validation_result['score'] < self.config['min_quality_score']:\n",
    "                    current_output = validation_result['suggestion']\n",
    "                    \n",
    "            elif step == 'refine':\n",
    "                # 3단계: 개선\n",
    "                refined_output = self._refine_output(current_output, result.get('validation_feedback', ''))\n",
    "                current_output = refined_output\n",
    "                \n",
    "            elif step == 'final_check':\n",
    "                # 4단계: 최종 검증\n",
    "                final_score = self.quality_checker.evaluate(current_output)\n",
    "                result['final_score'] = final_score\n",
    "            \n",
    "            result['chain_history'].append({\n",
    "                'step': step,\n",
    "                'output': current_output[:200] + '...' if len(current_output) > 200 else current_output\n",
    "            })\n",
    "        \n",
    "        result['final_output'] = current_output\n",
    "        return result\n",
    "    \n",
    "    def _validate_output(self, output: str) -> Dict[str, Any]:\n",
    "        \"\"\"검증 모델을 사용한 출력 평가\"\"\"\n",
    "        validation_prompt = f\"\"\"다음 금융 시험 문제를 평가하고 개선점을 제시하세요.\n",
    "\n",
    "문제:\n",
    "{output}\n",
    "\n",
    "평가 기준:\n",
    "1. 명확성 (1-10): 문제가 명확하고 모호하지 않은가?\n",
    "2. 정확성 (1-10): 금융 지식이 정확한가?\n",
    "3. 난이도 (1-10): 적절한 난이도인가?\n",
    "4. 형식 (1-10): 문제 형식이 올바른가?\n",
    "\n",
    "출력 형식:\n",
    "[점수] 총점: XX/40\n",
    "[피드백] 개선이 필요한 부분\n",
    "[제안] 개선된 문제\n",
    "\"\"\"\n",
    "        \n",
    "        # 검증 모델로 평가\n",
    "        inputs = self.validation_tokenizer(\n",
    "            validation_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(self.validation_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.validation_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        validation_output = self.validation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        validation_output = validation_output.replace(validation_prompt, '').strip()\n",
    "        \n",
    "        # 결과 파싱\n",
    "        import re\n",
    "        score_match = re.search(r'총점[:\\s]*(\\d+)', validation_output)\n",
    "        score = int(score_match.group(1)) if score_match else 20\n",
    "        \n",
    "        feedback_match = re.search(r'\\[피드백\\](.*?)\\[제안\\]', validation_output, re.DOTALL)\n",
    "        feedback = feedback_match.group(1).strip() if feedback_match else ''\n",
    "        \n",
    "        suggestion_match = re.search(r'\\[제안\\](.*)', validation_output, re.DOTALL)\n",
    "        suggestion = suggestion_match.group(1).strip() if suggestion_match else output\n",
    "        \n",
    "        return {\n",
    "            'score': (score / 40) * 100,  # 100점 만점으로 변환\n",
    "            'feedback': feedback,\n",
    "            'suggestion': suggestion\n",
    "        }\n",
    "    \n",
    "    def _refine_output(self, output: str, feedback: str) -> str:\n",
    "        \"\"\"피드백을 반영한 출력 개선\"\"\"\n",
    "        refine_prompt = f\"\"\"다음 금융 문제를 피드백을 참고하여 개선하세요.\n",
    "\n",
    "원본 문제:\n",
    "{output}\n",
    "\n",
    "피드백:\n",
    "{feedback}\n",
    "\n",
    "개선된 문제 (더 명확하고 전문적으로):\n",
    "\"\"\"\n",
    "        \n",
    "        refined = self._generate_with_llm(refine_prompt, temperature=0.7)\n",
    "        return refined\n",
    "    \n",
    "    def generate_questions_batch(self, chunks: List[Dict], batch_size: int = 32) -> List[Dict]:\n",
    "        \"\"\"배치 단위로 문제 생성\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch_chunks = chunks[i:i+batch_size]\n",
    "            batch_prompts = []\n",
    "            \n",
    "            for chunk in batch_chunks:\n",
    "                prompt = self.prompt_template.format(\n",
    "                    concept=chunk.get('keywords', ['금융'])[0] if chunk.get('keywords') else '금융',\n",
    "                    context=chunk['content']\n",
    "                )\n",
    "                batch_prompts.append(prompt)\n",
    "            \n",
    "            # 배치 처리 (현재는 순차 처리, 향후 병렬화 가능)\n",
    "            for prompt, chunk in zip(batch_prompts, batch_chunks):\n",
    "                try:\n",
    "                    if self.config.get('use_chaining'):\n",
    "                        result = self.generate_with_chain(prompt, chunk['content'])\n",
    "                        if result['final_score'] >= self.config['min_quality_score']:\n",
    "                            question = self._parse_question_result(result)\n",
    "                            questions.append(question)\n",
    "                    else:\n",
    "                        # 기존 방식\n",
    "                        generated = self._generate_with_llm(prompt, self.config['temperature'])\n",
    "                        question = self._extract_question_answer(generated)\n",
    "                        questions.append(question)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ 생성 오류: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def _parse_question_result(self, result: Dict) -> Dict:\n",
    "        \"\"\"체이닝 결과를 문제 형식으로 파싱\"\"\"\n",
    "        final_output = result['final_output']\n",
    "        \n",
    "        # 기존 파싱 메서드 사용\n",
    "        question = self._clean_question(final_output)\n",
    "        answer = self._extract_answer(final_output)\n",
    "        \n",
    "        return {\n",
    "            'id': f\"CHAIN_{int(time.time()*1000) % 1000000:06d}\",\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'metadata': {\n",
    "                'chain_steps': len(result['chain_history']),\n",
    "                'initial_score': result.get('validation_score', 0),\n",
    "                'final_score': result.get('final_score', 0),\n",
    "                'context_used': bool(result.get('context'))\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _extract_question_answer(self, generated: str) -> Dict:\n",
    "        \"\"\"기존 방식의 질문/답변 추출\"\"\"\n",
    "        question = self._clean_question(generated)\n",
    "        answer = self._extract_answer(generated)\n",
    "        \n",
    "        return {\n",
    "            'id': f\"GEN_{int(time.time()*1000) % 1000000:06d}\",\n",
    "            'question': question,\n",
    "            'answer': answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG 시스템 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 시스템 초기화 (기존 retriever 활용)\n",
    "print(\"🔄 RAG 시스템 초기화 중...\")\n",
    "\n",
    "# 문서 경로 확인\n",
    "data_dir = PROJECT_ROOT / 'data'\n",
    "external_dir = data_dir / 'external'\n",
    "\n",
    "if not external_dir.exists():\n",
    "    print(f\"⚠️ 외부 데이터 디렉토리가 없습니다: {external_dir}\")\n",
    "    print(\"PDF 파일을 업로드하거나 경로를 수정하세요.\")\n",
    "else:\n",
    "    # DocumentRetriever 초기화\n",
    "    retriever = DocumentRetriever(\n",
    "        data_dir=str(data_dir),\n",
    "        use_embedding=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    # 통계 출력\n",
    "    stats = retriever.get_statistics()\n",
    "    print(f\"\\n📊 RAG 통계:\")\n",
    "    print(f\"  - 문서 수: {stats['total_documents']}\")\n",
    "    print(f\"  - 청크 수: {stats['total_chunks']}\")\n",
    "    print(f\"  - 평균 청크 크기: {stats['avg_chunk_size']:.1f} 토큰\")\n",
    "    print(f\"  - 임베딩 사용: {stats['use_embedding']}\")\n",
    "    print(f\"  - 문서 타입: {stats['document_types']}\")\n",
    "\n",
    "\n",
    "def multi_method_search(retriever, query: str, methods: List[str] = None) -> str:\n",
    "    \"\"\"여러 검색 방법을 조합한 검색\"\"\"\n",
    "    if methods is None:\n",
    "        methods = ['similarity', 'bm25', 'hybrid']\n",
    "    \n",
    "    all_contexts = []\n",
    "    seen_chunks = set()\n",
    "    \n",
    "    for method in methods:\n",
    "        try:\n",
    "            context = retriever.search(query, top_k=3, method=method)\n",
    "            # 중복 제거\n",
    "            if context and context not in seen_chunks:\n",
    "                all_contexts.append(f\"[{method.upper()}]\\n{context}\")\n",
    "                seen_chunks.add(context[:100])  # 앞부분으로 중복 체크\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {method} 검색 실패: {e}\")\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(all_contexts)\n",
    "\n",
    "\n",
    "# 테스트 검색\n",
    "if 'retriever' in locals():\n",
    "    test_query = \"전자금융거래\"\n",
    "    test_result = multi_method_search(retriever, test_query, ['similarity', 'bm25'])\n",
    "    print(f\"\\n🔍 테스트 검색 '{test_query}':\")\n",
    "    print(test_result[:500] + \"...\" if len(test_result) > 500 else test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 프롬프트 체이닝 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "PROMPT_TEMPLATES = {\n",
    "    'initial': \"\"\"당신은 한국 금융보안원의 수석 전문가입니다.\n",
    "다음 금융 문서를 바탕으로 FSKU 평가에 적합한 고품질 시험 문제를 생성하세요.\n",
    "\n",
    "📌 문서 내용:\n",
    "{context}\n",
    "\n",
    "📋 문제 요구사항:\n",
    "1. 형식: 객관식(4-5지선다) 또는 주관식\n",
    "2. 난이도: 중상급 (금융 전문가 대상)\n",
    "3. 내용: 실무에 직접 적용 가능한 지식\n",
    "4. 명확성: 모호하지 않고 구체적인 표현\n",
    "\n",
    "🎯 생성할 문제 유형: {question_type}\n",
    "\n",
    "[문제]\n",
    "(여기에 문제 작성)\n",
    "\n",
    "[선택지] (객관식인 경우)\n",
    "1) \n",
    "2) \n",
    "3) \n",
    "4) \n",
    "5) \n",
    "\n",
    "[정답]\n",
    "(정답 번호 또는 서술형 답변)\n",
    "\n",
    "[해설]\n",
    "(정답의 근거와 오답 분석)\"\"\",\n",
    "    \n",
    "    'validation': \"\"\"금융 교육 전문가로서 다음 시험 문제를 평가해주세요.\n",
    "\n",
    "📝 평가할 문제:\n",
    "{question}\n",
    "\n",
    "🔍 평가 기준:\n",
    "1. 명확성 (1-10): 문제가 명확하고 이해하기 쉬운가?\n",
    "2. 정확성 (1-10): 금융 지식과 법규가 정확한가?\n",
    "3. 난이도 (1-10): 전문가 수준에 적절한가?\n",
    "4. 실무성 (1-10): 실제 업무에 도움이 되는가?\n",
    "\n",
    "💡 출력 형식:\n",
    "[점수] 총점: XX/40\n",
    "- 명확성: X/10\n",
    "- 정확성: X/10\n",
    "- 난이도: X/10\n",
    "- 실무성: X/10\n",
    "\n",
    "[피드백]\n",
    "개선이 필요한 구체적인 부분\n",
    "\n",
    "[제안]\n",
    "개선된 문제 전체\"\"\",\n",
    "    \n",
    "    'refinement': \"\"\"다음 피드백을 반영하여 금융 시험 문제를 개선하세요.\n",
    "\n",
    "📌 원본 문제:\n",
    "{original}\n",
    "\n",
    "💭 피드백:\n",
    "{feedback}\n",
    "\n",
    "✨ 개선 지침:\n",
    "- 더 명확하고 구체적인 표현 사용\n",
    "- 금융 전문 용어의 정확한 사용\n",
    "- 실무 상황을 반영한 사례 포함\n",
    "- 선택지 간 명확한 구분\n",
    "\n",
    "📝 개선된 문제:\n",
    "[문제]\n",
    "\n",
    "[선택지]\n",
    "\n",
    "[정답]\n",
    "\n",
    "[해설]\"\"\",\n",
    "    \n",
    "    'final_check': \"\"\"최종 품질 검증을 수행하세요.\n",
    "\n",
    "문제: {question}\n",
    "\n",
    "체크리스트:\n",
    "□ 문법적 오류가 없는가?\n",
    "□ 금융 용어가 정확한가?\n",
    "□ 법령 조항이 최신인가?\n",
    "□ 정답이 명확한가?\n",
    "□ 난이도가 적절한가?\n",
    "\n",
    "최종 평가: [통과/수정필요]\"\"\"\n",
    "}\n",
    "\n",
    "# 문제 유형 정의\n",
    "QUESTION_TYPES = [\n",
    "    \"정의형 (용어나 개념의 정의를 묻는 문제)\",\n",
    "    \"사례형 (실제 상황에서의 적용을 묻는 문제)\",\n",
    "    \"계산형 (수치 계산이 필요한 문제)\",\n",
    "    \"판단형 (옳고 그름을 판단하는 문제)\",\n",
    "    \"비교형 (여러 개념을 비교하는 문제)\",\n",
    "    \"법령형 (특정 법령 조항을 묻는 문제)\"\n",
    "]\n",
    "\n",
    "# 프롬프트 템플릿 저장\n",
    "prompt_dir = config['output_dir'] / 'prompts'\n",
    "prompt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name, template in PROMPT_TEMPLATES.items():\n",
    "    with open(prompt_dir / f'{name}_prompt.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(template)\n",
    "\n",
    "print(f\"✅ {len(PROMPT_TEMPLATES)}개 프롬프트 템플릿 생성 완료\")\n",
    "print(f\"📁 저장 위치: {prompt_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 품질 모니터링 대시보드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMonitor:\n",
    "    \"\"\"실시간 진행률 모니터링\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'generated': [],\n",
    "            'passed': [],\n",
    "            'quality_scores': [],\n",
    "            'generation_speed': [],\n",
    "            'chain_improvements': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "        self.last_update = self.start_time\n",
    "    \n",
    "    def update(self, new_questions: List[Dict], total_generated: int, total_passed: int):\n",
    "        \"\"\"메트릭 업데이트\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # 생성 속도 계산 (분당)\n",
    "        time_diff = (current_time - self.last_update) / 60\n",
    "        if time_diff > 0:\n",
    "            speed = len(new_questions) / time_diff\n",
    "        else:\n",
    "            speed = 0\n",
    "        \n",
    "        self.metrics['generated'].append(total_generated)\n",
    "        self.metrics['passed'].append(total_passed)\n",
    "        self.metrics['generation_speed'].append(speed)\n",
    "        self.metrics['timestamps'].append(current_time - self.start_time)\n",
    "        \n",
    "        # 품질 점수 추가\n",
    "        for q in new_questions:\n",
    "            if 'metadata' in q and 'final_score' in q['metadata']:\n",
    "                self.metrics['quality_scores'].append(q['metadata']['final_score'])\n",
    "                \n",
    "                # 체이닝 개선도\n",
    "                if 'initial_score' in q['metadata']:\n",
    "                    improvement = q['metadata']['final_score'] - q['metadata']['initial_score']\n",
    "                    self.metrics['chain_improvements'].append(improvement)\n",
    "        \n",
    "        self.last_update = current_time\n",
    "    \n",
    "    def plot_dashboard(self):\n",
    "        \"\"\"대시보드 플롯\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('FSKU 데이터 생성 모니터링 대시보드', fontsize=16)\n",
    "        \n",
    "        # 1. 생성 진행률\n",
    "        if self.metrics['generated']:\n",
    "            axes[0, 0].plot(self.metrics['timestamps'], self.metrics['generated'], 'b-', linewidth=2)\n",
    "            axes[0, 0].fill_between(self.metrics['timestamps'], 0, self.metrics['generated'], alpha=0.3)\n",
    "            axes[0, 0].set_title('누적 생성 문제 수')\n",
    "            axes[0, 0].set_xlabel('시간 (초)')\n",
    "            axes[0, 0].set_ylabel('문제 수')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 통과율\n",
    "        if self.metrics['generated'] and self.metrics['passed']:\n",
    "            pass_rates = [p/g*100 if g > 0 else 0 for p, g in zip(self.metrics['passed'], self.metrics['generated'])]\n",
    "            axes[0, 1].plot(self.metrics['timestamps'], pass_rates, 'g-', linewidth=2)\n",
    "            axes[0, 1].axhline(y=config['min_quality_score'], color='r', linestyle='--', label=f\"목표: {config['min_quality_score']}%\")\n",
    "            axes[0, 1].set_title('품질 통과율 (%)')\n",
    "            axes[0, 1].set_xlabel('시간 (초)')\n",
    "            axes[0, 1].set_ylabel('통과율 (%)')\n",
    "            axes[0, 1].set_ylim(0, 100)\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. 생성 속도\n",
    "        if self.metrics['generation_speed']:\n",
    "            axes[0, 2].plot(self.metrics['timestamps'], self.metrics['generation_speed'], 'r-', linewidth=2)\n",
    "            axes[0, 2].set_title('생성 속도 (문제/분)')\n",
    "            axes[0, 2].set_xlabel('시간 (초)')\n",
    "            axes[0, 2].set_ylabel('속도')\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. 품질 점수 분포\n",
    "        if self.metrics['quality_scores']:\n",
    "            axes[1, 0].hist(self.metrics['quality_scores'], bins=20, alpha=0.7, color='purple')\n",
    "            axes[1, 0].axvline(x=config['min_quality_score'], color='r', linestyle='--', label=f\"최소: {config['min_quality_score']}\")\n",
    "            axes[1, 0].set_title('품질 점수 분포')\n",
    "            axes[1, 0].set_xlabel('점수')\n",
    "            axes[1, 0].set_ylabel('빈도')\n",
    "            axes[1, 0].legend()\n",
    "        \n",
    "        # 5. 체이닝 개선도\n",
    "        if self.metrics['chain_improvements']:\n",
    "            axes[1, 1].hist(self.metrics['chain_improvements'], bins=20, alpha=0.7, color='orange')\n",
    "            avg_improvement = np.mean(self.metrics['chain_improvements'])\n",
    "            axes[1, 1].axvline(x=avg_improvement, color='g', linestyle='--', label=f\"평균: {avg_improvement:.1f}\")\n",
    "            axes[1, 1].set_title('체이닝 품질 개선도')\n",
    "            axes[1, 1].set_xlabel('개선 점수')\n",
    "            axes[1, 1].set_ylabel('빈도')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        # 6. 요약 통계\n",
    "        axes[1, 2].axis('off')\n",
    "        if self.metrics['generated']:\n",
    "            total_time = (time.time() - self.start_time) / 60  # 분\n",
    "            stats_text = f\"\"\"📊 실시간 통계\n",
    "            \n",
    "총 생성: {self.metrics['generated'][-1]:,}개\n",
    "통과: {self.metrics['passed'][-1]:,}개\n",
    "통과율: {self.metrics['passed'][-1]/self.metrics['generated'][-1]*100:.1f}%\n",
    "\n",
    "경과 시간: {total_time:.1f}분\n",
    "평균 속도: {self.metrics['generated'][-1]/total_time:.1f}개/분\n",
    "\n",
    "평균 품질: {np.mean(self.metrics['quality_scores']):.1f}점\n",
    "최고 품질: {max(self.metrics['quality_scores']):.1f}점\n",
    "최저 품질: {min(self.metrics['quality_scores']):.1f}점\n",
    "\"\"\"\n",
    "            axes[1, 2].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 예상 완료 시간\n",
    "        if self.metrics['generated'] and self.metrics['generated'][-1] > 0:\n",
    "            current_rate = self.metrics['generated'][-1] / (time.time() - self.start_time)\n",
    "            remaining = config['num_questions'] - self.metrics['generated'][-1]\n",
    "            eta_seconds = remaining / current_rate if current_rate > 0 else 0\n",
    "            eta_minutes = eta_seconds / 60\n",
    "            \n",
    "            print(f\"\\n⏱️ 예상 완료 시간: {eta_minutes:.1f}분 후\")\n",
    "            print(f\"🎯 목표 달성률: {self.metrics['generated'][-1]/config['num_questions']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 대규모 배치 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_large_dataset(config: Dict[str, Any], retriever: DocumentRetriever) -> List[Dict]:\n",
    "    \"\"\"대규모 데이터셋 생성\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"🚀 대규모 데이터 생성 시작\")\n",
    "    print(f\"📋 설정:\")\n",
    "    print(f\"  - 목표: {config['num_questions']:,}개\")\n",
    "    print(f\"  - 모델: {config['model_name']}\")\n",
    "    print(f\"  - 배치 크기: {config['batch_size']}\")\n",
    "    print(f\"  - 체이닝: {config['use_chaining']}\")\n",
    "    print(f\"  - 최소 품질: {config['min_quality_score']}점\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 생성기 초기화\n",
    "    generator = ChainDataGenerator(config)\n",
    "    monitor = ProgressMonitor()\n",
    "    \n",
    "    # 결과 저장\n",
    "    all_questions = []\n",
    "    all_metadata = []\n",
    "    checkpoint_counter = 0\n",
    "    \n",
    "    # 청크 가져오기\n",
    "    all_chunks = retriever.chunks\n",
    "    print(f\"\\n📚 사용 가능한 청크: {len(all_chunks)}개\")\n",
    "    \n",
    "    # 청크를 순환하며 사용\n",
    "    chunk_index = 0\n",
    "    total_generated = 0\n",
    "    total_passed = 0\n",
    "    \n",
    "    try:\n",
    "        with tqdm(total=config['num_questions'], desc=\"전체 진행률\") as pbar:\n",
    "            while len(all_questions) < config['num_questions']:\n",
    "                # 배치 크기만큼 청크 선택\n",
    "                batch_chunks = []\n",
    "                for _ in range(config['batch_size']):\n",
    "                    batch_chunks.append(all_chunks[chunk_index % len(all_chunks)])\n",
    "                    chunk_index += 1\n",
    "                \n",
    "                # 배치 생성\n",
    "                batch_start_time = time.time()\n",
    "                batch_questions = generator.generate_questions_batch(\n",
    "                    chunks=batch_chunks,\n",
    "                    batch_size=config['batch_size']\n",
    "                )\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                \n",
    "                # 품질 필터링\n",
    "                passed_questions = []\n",
    "                for q in batch_questions:\n",
    "                    total_generated += 1\n",
    "                    \n",
    "                    # 품질 검사\n",
    "                    if 'metadata' in q and q['metadata'].get('final_score', 0) >= config['min_quality_score']:\n",
    "                        passed_questions.append(q)\n",
    "                        total_passed += 1\n",
    "                    elif 'metadata' not in q:  # 기존 방식 생성\n",
    "                        # 간단한 품질 체크\n",
    "                        if len(q.get('question', '')) > 20 and len(q.get('answer', '')) > 0:\n",
    "                            passed_questions.append(q)\n",
    "                            total_passed += 1\n",
    "                \n",
    "                # 결과 추가\n",
    "                all_questions.extend(passed_questions)\n",
    "                pbar.update(len(passed_questions))\n",
    "                \n",
    "                # 모니터링 업데이트\n",
    "                monitor.update(passed_questions, total_generated, total_passed)\n",
    "                \n",
    "                # 대시보드 업데이트 (설정된 간격마다)\n",
    "                if total_generated % config['update_interval'] == 0 and config['enable_monitoring']:\n",
    "                    monitor.plot_dashboard()\n",
    "                \n",
    "                # 체크포인트 저장\n",
    "                if len(all_questions) % config['checkpoint_interval'] == 0 and len(all_questions) > 0:\n",
    "                    checkpoint_counter += 1\n",
    "                    checkpoint_file = config['output_dir'] / f'checkpoint_{checkpoint_counter}_{len(all_questions)}.jsonl'\n",
    "                    \n",
    "                    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                        for q in all_questions[-config['checkpoint_interval']:]:\n",
    "                            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "                    \n",
    "                    print(f\"\\n💾 체크포인트 저장: {checkpoint_file.name}\")\n",
    "                    print(f\"   배치 처리 시간: {batch_time:.1f}초\")\n",
    "                    print(f\"   배치 통과율: {len(passed_questions)/len(batch_questions)*100:.1f}%\")\n",
    "                \n",
    "                # 목표 달성 시 종료\n",
    "                if len(all_questions) >= config['num_questions']:\n",
    "                    all_questions = all_questions[:config['num_questions']]\n",
    "                    break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ 사용자에 의해 중단됨\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # 최종 대시보드 표시\n",
    "        if config['enable_monitoring']:\n",
    "            monitor.plot_dashboard()\n",
    "        \n",
    "        # 메모리 정리\n",
    "        generator.cleanup()\n",
    "    \n",
    "    print(f\"\\n✅ 생성 완료!\")\n",
    "    print(f\"  - 총 시도: {total_generated:,}개\")\n",
    "    print(f\"  - 통과: {len(all_questions):,}개\")\n",
    "    print(f\"  - 통과율: {len(all_questions)/total_generated*100:.1f}%\")\n",
    "    \n",
    "    return all_questions\n",
    "\n",
    "\n",
    "def save_final_dataset(questions: List[Dict], config: Dict[str, Any]):\n",
    "    \"\"\"최종 데이터셋 저장\"\"\"\n",
    "    print(\"\\n📁 최종 데이터셋 저장 중...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. 전체 데이터 JSONL 저장\n",
    "    full_file = config['output_dir'] / f'full_dataset_{timestamp}.jsonl'\n",
    "    with open(full_file, 'w', encoding='utf-8') as f:\n",
    "        for q in questions:\n",
    "            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "    print(f\"✅ 전체 데이터: {full_file.name}\")\n",
    "    \n",
    "    # 2. 훈련/검증/테스트 분할\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # 80/10/10 분할\n",
    "    train_val, test = train_test_split(questions, test_size=0.1, random_state=42)\n",
    "    train, val = train_test_split(train_val, test_size=0.111, random_state=42)  # 0.111 ≈ 10/90\n",
    "    \n",
    "    # 각각 저장\n",
    "    splits = {\n",
    "        'train': train,\n",
    "        'val': val,\n",
    "        'test': test\n",
    "    }\n",
    "    \n",
    "    for split_name, split_data in splits.items():\n",
    "        split_file = config['output_dir'] / f'{split_name}_{timestamp}.jsonl'\n",
    "        with open(split_file, 'w', encoding='utf-8') as f:\n",
    "            for q in split_data:\n",
    "                f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "        print(f\"✅ {split_name}: {split_file.name} ({len(split_data):,}개)\")\n",
    "    \n",
    "    # 3. 요약 리포트 생성\n",
    "    report = {\n",
    "        'generation_info': {\n",
    "            'timestamp': timestamp,\n",
    "            'total_questions': len(questions),\n",
    "            'config': config\n",
    "        },\n",
    "        'splits': {\n",
    "            'train': len(train),\n",
    "            'val': len(val),\n",
    "            'test': len(test)\n",
    "        },\n",
    "        'sample_questions': questions[:5]  # 샘플 5개\n",
    "    }\n",
    "    \n",
    "    report_file = config['output_dir'] / f'generation_report_{timestamp}.json'\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✅ 리포트: {report_file.name}\")\n",
    "    \n",
    "    # 4. 샘플 출력\n",
    "    print(\"\\n📝 생성된 문제 샘플:\")\n",
    "    for i, q in enumerate(questions[:3], 1):\n",
    "        print(f\"\\n[{i}] ID: {q['id']}\")\n",
    "        print(f\"질문: {q['question'][:100]}...\" if len(q['question']) > 100 else f\"질문: {q['question']}\")\n",
    "        print(f\"답변: {q['answer'][:50]}...\" if len(q['answer']) > 50 else f\"답변: {q['answer']}\")\n",
    "        if 'metadata' in q:\n",
    "            print(f\"품질: {q['metadata'].get('final_score', 'N/A')}점\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 메인 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 실행\n",
    "if 'retriever' in locals():\n",
    "    # 데이터 생성 실행\n",
    "    questions = generate_large_dataset(config, retriever)\n",
    "    \n",
    "    # 결과 저장\n",
    "    if questions:\n",
    "        save_final_dataset(questions, config)\n",
    "else:\n",
    "    print(\"⚠️ RAG 시스템이 초기화되지 않았습니다.\")\n",
    "    print(\"Cell 4를 먼저 실행하여 RAG를 초기화하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 결과 분석 및 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 데이터 분석\n",
    "def analyze_generated_data(output_dir: Path):\n",
    "    \"\"\"생성된 데이터 분석\"\"\"\n",
    "    # 최신 파일 찾기\n",
    "    jsonl_files = list(output_dir.glob('full_dataset_*.jsonl'))\n",
    "    if not jsonl_files:\n",
    "        print(\"⚠️ 생성된 데이터 파일이 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    latest_file = max(jsonl_files, key=lambda f: f.stat().st_mtime)\n",
    "    print(f\"📊 분석 파일: {latest_file.name}\")\n",
    "    \n",
    "    # 데이터 로드\n",
    "    questions = []\n",
    "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            questions.append(json.loads(line))\n",
    "    \n",
    "    print(f\"\\n📈 데이터 통계:\")\n",
    "    print(f\"  - 총 문제 수: {len(questions):,}개\")\n",
    "    \n",
    "    # 문제 길이 분석\n",
    "    q_lengths = [len(q['question']) for q in questions]\n",
    "    a_lengths = [len(q['answer']) for q in questions]\n",
    "    \n",
    "    print(f\"\\n📏 문제 길이:\")\n",
    "    print(f\"  - 평균: {np.mean(q_lengths):.1f}자\")\n",
    "    print(f\"  - 최소: {min(q_lengths)}자\")\n",
    "    print(f\"  - 최대: {max(q_lengths)}자\")\n",
    "    \n",
    "    print(f\"\\n📏 답변 길이:\")\n",
    "    print(f\"  - 평균: {np.mean(a_lengths):.1f}자\")\n",
    "    print(f\"  - 최소: {min(a_lengths)}자\")\n",
    "    print(f\"  - 최대: {max(a_lengths)}자\")\n",
    "    \n",
    "    # 메타데이터 분석\n",
    "    has_metadata = sum(1 for q in questions if 'metadata' in q)\n",
    "    print(f\"\\n🏷️ 메타데이터:\")\n",
    "    print(f\"  - 메타데이터 포함: {has_metadata}개 ({has_metadata/len(questions)*100:.1f}%)\")\n",
    "    \n",
    "    if has_metadata > 0:\n",
    "        scores = [q['metadata'].get('final_score', 0) for q in questions if 'metadata' in q and 'final_score' in q['metadata']]\n",
    "        if scores:\n",
    "            print(f\"\\n🎯 품질 점수:\")\n",
    "            print(f\"  - 평균: {np.mean(scores):.1f}점\")\n",
    "            print(f\"  - 표준편차: {np.std(scores):.1f}\")\n",
    "            print(f\"  - 최소: {min(scores):.1f}점\")\n",
    "            print(f\"  - 최대: {max(scores):.1f}점\")\n",
    "    \n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 문제 길이 분포\n",
    "    axes[0].hist(q_lengths, bins=30, alpha=0.7, color='blue')\n",
    "    axes[0].set_title('문제 길이 분포')\n",
    "    axes[0].set_xlabel('글자 수')\n",
    "    axes[0].set_ylabel('빈도')\n",
    "    \n",
    "    # 답변 길이 분포\n",
    "    axes[1].hist(a_lengths, bins=30, alpha=0.7, color='green')\n",
    "    axes[1].set_title('답변 길이 분포')\n",
    "    axes[1].set_xlabel('글자 수')\n",
    "    axes[1].set_ylabel('빈도')\n",
    "    \n",
    "    # 품질 점수 분포\n",
    "    if scores:\n",
    "        axes[2].hist(scores, bins=20, alpha=0.7, color='red')\n",
    "        axes[2].set_title('품질 점수 분포')\n",
    "        axes[2].set_xlabel('점수')\n",
    "        axes[2].set_ylabel('빈도')\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, '품질 점수 없음', ha='center', va='center')\n",
    "        axes[2].set_xlim(0, 1)\n",
    "        axes[2].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 분석 실행\n",
    "analyze_generated_data(config['output_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 추가 유틸리티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 병합 유틸리티\n",
    "def merge_checkpoints(output_dir: Path) -> str:\n",
    "    \"\"\"체크포인트 파일들을 병합\"\"\"\n",
    "    checkpoint_files = sorted(output_dir.glob('checkpoint_*.jsonl'))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(\"체크포인트 파일이 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n🔀 {len(checkpoint_files)}개 체크포인트 병합 중...\")\n",
    "    \n",
    "    all_questions = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    for file in checkpoint_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                q = json.loads(line)\n",
    "                if q['id'] not in seen_ids:\n",
    "                    all_questions.append(q)\n",
    "                    seen_ids.add(q['id'])\n",
    "    \n",
    "    # 병합 파일 저장\n",
    "    merged_file = output_dir / f'merged_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.jsonl'\n",
    "    with open(merged_file, 'w', encoding='utf-8') as f:\n",
    "        for q in all_questions:\n",
    "            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"✅ 병합 완료: {merged_file.name}\")\n",
    "    print(f\"   총 {len(all_questions):,}개 문제 (중복 제거됨)\")\n",
    "    \n",
    "    return str(merged_file)\n",
    "\n",
    "\n",
    "# 품질 필터링 유틸리티\n",
    "def filter_by_quality(input_file: Path, min_score: float = 85.0) -> str:\n",
    "    \"\"\"품질 점수로 필터링\"\"\"\n",
    "    questions = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            q = json.loads(line)\n",
    "            if 'metadata' in q and q['metadata'].get('final_score', 0) >= min_score:\n",
    "                questions.append(q)\n",
    "    \n",
    "    # 필터링된 파일 저장\n",
    "    filtered_file = input_file.parent / f'filtered_{min_score}_{input_file.name}'\n",
    "    with open(filtered_file, 'w', encoding='utf-8') as f:\n",
    "        for q in questions:\n",
    "            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"✅ 필터링 완료: {filtered_file.name}\")\n",
    "    print(f\"   {len(questions):,}개 문제 (최소 {min_score}점)\")\n",
    "    \n",
    "    return str(filtered_file)\n",
    "\n",
    "\n",
    "# HuggingFace 형식 변환\n",
    "def convert_to_hf_format(input_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"HuggingFace datasets 형식으로 변환\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            q = json.loads(line)\n",
    "            data.append({\n",
    "                'instruction': q['question'],\n",
    "                'output': q['answer'],\n",
    "                'id': q['id']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Parquet 형식으로 저장 (HF 권장)\n",
    "    parquet_file = input_file.parent / f'{input_file.stem}.parquet'\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "    \n",
    "    print(f\"✅ HF 형식 변환 완료: {parquet_file.name}\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 유틸리티 메뉴\n",
    "print(\"\\n🛠️ 추가 유틸리티:\")\n",
    "print(\"1. merge_checkpoints(config['output_dir']) - 체크포인트 병합\")\n",
    "print(\"2. filter_by_quality(file_path, min_score) - 품질 필터링\")\n",
    "print(\"3. convert_to_hf_format(file_path) - HuggingFace 형식 변환\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}