{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ FSKU ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ê³ ì„±ëŠ¥ í™˜ê²½ì—ì„œ ê¸ˆìœµë³´ì•ˆ ë°ì´í„°ë¥¼ ëŒ€ëŸ‰ ìƒì„±í•˜ê³  ì²´ì´ë‹ìœ¼ë¡œ í’ˆì§ˆì„ ê·¹ëŒ€í™”í•˜ëŠ” ë…¸íŠ¸ë¶ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“‹ ëª©ì°¨\n",
    "1. í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "2. ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì„¤ì •\n",
    "3. ì²´ì´ë‹ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ DataGenerator\n",
    "4. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "5. í”„ë¡¬í”„íŠ¸ ì²´ì´ë‹ í…œí”Œë¦¿\n",
    "6. ëŒ€ê·œëª¨ ë°°ì¹˜ ìƒì„±\n",
    "7. í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ\n",
    "8. ë©”ì¸ ì‹¤í–‰ ë° ë°ì´í„° ì €ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™˜ê²½ í™•ì¸\n",
    "!nvidia-smi\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ê³ ì„±ëŠ¥ í™˜ê²½ìš©)\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q langchain sentence-transformers faiss-gpu\n",
    "!pip install -q PyPDF2 pdfplumber pandas tqdm matplotlib\n",
    "!pip install -q flash-attn --no-build-isolation  # Flash Attention 2\n",
    "!pip install -q datasets tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "PROJECT_ROOT = Path('/workspace/ai-dacon')  # RunPod í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •\n",
    "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from rag.retriever import DocumentRetriever\n",
    "from rag.pdf_loader import DocumentLoader\n",
    "from rag.chunker import DocumentChunker\n",
    "from generate_data.main import DataGenerator\n",
    "from generate_data.quality_checker import QualityChecker\n",
    "\n",
    "print(\"âœ… ëª¨ë“ˆ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"ğŸ”§ í”„ë¡œì íŠ¸ ê²½ë¡œ: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"ğŸ’» CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   - GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì • ì •ì˜ (ë¦¬ì†ŒìŠ¤ ì œì•½ ì—†ìŒ)\n",
    "config = {\n",
    "    # ëª¨ë¸ ì„¤ì •\n",
    "    'model_name': 'Qwen/Qwen2.5-72B-Instruct',  # ë©”ì¸ ìƒì„± ëª¨ë¸ (ëŒ€í˜•)\n",
    "    'validation_model': 'meta-llama/Llama-3-70B-Instruct',  # ê²€ì¦ìš© ëª¨ë¸\n",
    "    'use_quantization': False,  # ì–‘ìí™” ì‚¬ìš© ì•ˆí•¨ (ì „ì²´ ì •ë°€ë„)\n",
    "    'device_map': 'auto',  # ë©€í‹° GPU ìë™ ë¶„ë°°\n",
    "    \n",
    "    # ìƒì„± ì„¤ì •\n",
    "    'num_questions': 10000,  # ëª©í‘œ ìƒì„± ìˆ˜\n",
    "    'batch_size': 32,  # ëŒ€ê·œëª¨ ë°°ì¹˜\n",
    "    'temperature': 0.8,\n",
    "    'top_p': 0.9,\n",
    "    'max_new_tokens': 1024,\n",
    "    \n",
    "    # RAG ì„¤ì •\n",
    "    'use_rag': True,\n",
    "    'search_method': 'hybrid',  # hybrid, similarity, bm25\n",
    "    'top_k_retrieval': 5,\n",
    "    \n",
    "    # ì²´ì´ë‹ ì„¤ì •\n",
    "    'use_chaining': True,\n",
    "    'chain_steps': ['generate', 'validate', 'refine', 'final_check'],\n",
    "    'min_quality_score': 80,\n",
    "    \n",
    "    # ì €ì¥ ì„¤ì •\n",
    "    'output_dir': PROJECT_ROOT / 'data' / 'generated',\n",
    "    'checkpoint_interval': 1000,  # 1000ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸\n",
    "    \n",
    "    # ëª¨ë‹ˆí„°ë§\n",
    "    'enable_monitoring': True,\n",
    "    'update_interval': 100  # 100ê°œë§ˆë‹¤ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸\n",
    "}\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "config['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {config['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì²´ì´ë‹ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainDataGenerator(DataGenerator):\n",
    "    \"\"\"ì²´ì´ë‹ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ë°ì´í„° ìƒì„±ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"ì´ˆê¸°í™”\"\"\"\n",
    "        super().__init__(config)\n",
    "        self.chain_steps = config.get('chain_steps', ['generate'])\n",
    "        self.validation_model = None\n",
    "        self.validation_tokenizer = None\n",
    "        \n",
    "        # ê²€ì¦ìš© ëª¨ë¸ ë¡œë“œ\n",
    "        if config.get('validation_model') and 'validate' in self.chain_steps:\n",
    "            self._load_validation_model()\n",
    "    \n",
    "    def _load_validation_model(self):\n",
    "        \"\"\"ê²€ì¦ìš© ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        print(f\"ğŸ”„ ê²€ì¦ ëª¨ë¸ ë¡œë”©: {self.config['validation_model']}\")\n",
    "        \n",
    "        self.validation_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config['validation_model'],\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if self.validation_tokenizer.pad_token is None:\n",
    "            self.validation_tokenizer.pad_token = self.validation_tokenizer.eos_token\n",
    "        \n",
    "        # ì „ì²´ ì •ë°€ë„ë¡œ ë¡œë“œ (ë¦¬ì†ŒìŠ¤ ì œì•½ ì—†ìŒ)\n",
    "        self.validation_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config['validation_model'],\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… ê²€ì¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    def generate_with_chain(self, prompt: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"ì²´ì´ë‹ì„ í†µí•œ ë¬¸ì œ ìƒì„±\"\"\"\n",
    "        result = {\n",
    "            'prompt': prompt,\n",
    "            'context': context,\n",
    "            'chain_history': []\n",
    "        }\n",
    "        \n",
    "        current_output = None\n",
    "        \n",
    "        for step in self.chain_steps:\n",
    "            if step == 'generate':\n",
    "                # 1ë‹¨ê³„: ì´ˆê¸° ìƒì„±\n",
    "                current_output = self._generate_with_llm(prompt, self.config['temperature'])\n",
    "                result['initial_output'] = current_output\n",
    "                \n",
    "            elif step == 'validate' and self.validation_model:\n",
    "                # 2ë‹¨ê³„: ê²€ì¦\n",
    "                validation_result = self._validate_output(current_output)\n",
    "                result['validation_score'] = validation_result['score']\n",
    "                result['validation_feedback'] = validation_result['feedback']\n",
    "                \n",
    "                if validation_result['score'] < self.config['min_quality_score']:\n",
    "                    current_output = validation_result['suggestion']\n",
    "                    \n",
    "            elif step == 'refine':\n",
    "                # 3ë‹¨ê³„: ê°œì„ \n",
    "                refined_output = self._refine_output(current_output, result.get('validation_feedback', ''))\n",
    "                current_output = refined_output\n",
    "                \n",
    "            elif step == 'final_check':\n",
    "                # 4ë‹¨ê³„: ìµœì¢… ê²€ì¦\n",
    "                final_score = self.quality_checker.evaluate(current_output)\n",
    "                result['final_score'] = final_score\n",
    "            \n",
    "            result['chain_history'].append({\n",
    "                'step': step,\n",
    "                'output': current_output[:200] + '...' if len(current_output) > 200 else current_output\n",
    "            })\n",
    "        \n",
    "        result['final_output'] = current_output\n",
    "        return result\n",
    "    \n",
    "    def _validate_output(self, output: str) -> Dict[str, Any]:\n",
    "        \"\"\"ê²€ì¦ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶œë ¥ í‰ê°€\"\"\"\n",
    "        validation_prompt = f\"\"\"ë‹¤ìŒ ê¸ˆìœµ ì‹œí—˜ ë¬¸ì œë¥¼ í‰ê°€í•˜ê³  ê°œì„ ì ì„ ì œì‹œí•˜ì„¸ìš”.\n",
    "\n",
    "ë¬¸ì œ:\n",
    "{output}\n",
    "\n",
    "í‰ê°€ ê¸°ì¤€:\n",
    "1. ëª…í™•ì„± (1-10): ë¬¸ì œê°€ ëª…í™•í•˜ê³  ëª¨í˜¸í•˜ì§€ ì•Šì€ê°€?\n",
    "2. ì •í™•ì„± (1-10): ê¸ˆìœµ ì§€ì‹ì´ ì •í™•í•œê°€?\n",
    "3. ë‚œì´ë„ (1-10): ì ì ˆí•œ ë‚œì´ë„ì¸ê°€?\n",
    "4. í˜•ì‹ (1-10): ë¬¸ì œ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?\n",
    "\n",
    "ì¶œë ¥ í˜•ì‹:\n",
    "[ì ìˆ˜] ì´ì : XX/40\n",
    "[í”¼ë“œë°±] ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„\n",
    "[ì œì•ˆ] ê°œì„ ëœ ë¬¸ì œ\n",
    "\"\"\"\n",
    "        \n",
    "        # ê²€ì¦ ëª¨ë¸ë¡œ í‰ê°€\n",
    "        inputs = self.validation_tokenizer(\n",
    "            validation_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(self.validation_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.validation_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        validation_output = self.validation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        validation_output = validation_output.replace(validation_prompt, '').strip()\n",
    "        \n",
    "        # ê²°ê³¼ íŒŒì‹±\n",
    "        import re\n",
    "        score_match = re.search(r'ì´ì [:\\s]*(\\d+)', validation_output)\n",
    "        score = int(score_match.group(1)) if score_match else 20\n",
    "        \n",
    "        feedback_match = re.search(r'\\[í”¼ë“œë°±\\](.*?)\\[ì œì•ˆ\\]', validation_output, re.DOTALL)\n",
    "        feedback = feedback_match.group(1).strip() if feedback_match else ''\n",
    "        \n",
    "        suggestion_match = re.search(r'\\[ì œì•ˆ\\](.*)', validation_output, re.DOTALL)\n",
    "        suggestion = suggestion_match.group(1).strip() if suggestion_match else output\n",
    "        \n",
    "        return {\n",
    "            'score': (score / 40) * 100,  # 100ì  ë§Œì ìœ¼ë¡œ ë³€í™˜\n",
    "            'feedback': feedback,\n",
    "            'suggestion': suggestion\n",
    "        }\n",
    "    \n",
    "    def _refine_output(self, output: str, feedback: str) -> str:\n",
    "        \"\"\"í”¼ë“œë°±ì„ ë°˜ì˜í•œ ì¶œë ¥ ê°œì„ \"\"\"\n",
    "        refine_prompt = f\"\"\"ë‹¤ìŒ ê¸ˆìœµ ë¬¸ì œë¥¼ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ê°œì„ í•˜ì„¸ìš”.\n",
    "\n",
    "ì›ë³¸ ë¬¸ì œ:\n",
    "{output}\n",
    "\n",
    "í”¼ë“œë°±:\n",
    "{feedback}\n",
    "\n",
    "ê°œì„ ëœ ë¬¸ì œ (ë” ëª…í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ):\n",
    "\"\"\"\n",
    "        \n",
    "        refined = self._generate_with_llm(refine_prompt, temperature=0.7)\n",
    "        return refined\n",
    "    \n",
    "    def generate_questions_batch(self, chunks: List[Dict], batch_size: int = 32) -> List[Dict]:\n",
    "        \"\"\"ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì œ ìƒì„±\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch_chunks = chunks[i:i+batch_size]\n",
    "            batch_prompts = []\n",
    "            \n",
    "            for chunk in batch_chunks:\n",
    "                prompt = self.prompt_template.format(\n",
    "                    concept=chunk.get('keywords', ['ê¸ˆìœµ'])[0] if chunk.get('keywords') else 'ê¸ˆìœµ',\n",
    "                    context=chunk['content']\n",
    "                )\n",
    "                batch_prompts.append(prompt)\n",
    "            \n",
    "            # ë°°ì¹˜ ì²˜ë¦¬ (í˜„ì¬ëŠ” ìˆœì°¨ ì²˜ë¦¬, í–¥í›„ ë³‘ë ¬í™” ê°€ëŠ¥)\n",
    "            for prompt, chunk in zip(batch_prompts, batch_chunks):\n",
    "                try:\n",
    "                    if self.config.get('use_chaining'):\n",
    "                        result = self.generate_with_chain(prompt, chunk['content'])\n",
    "                        if result['final_score'] >= self.config['min_quality_score']:\n",
    "                            question = self._parse_question_result(result)\n",
    "                            questions.append(question)\n",
    "                    else:\n",
    "                        # ê¸°ì¡´ ë°©ì‹\n",
    "                        generated = self._generate_with_llm(prompt, self.config['temperature'])\n",
    "                        question = self._extract_question_answer(generated)\n",
    "                        questions.append(question)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def _parse_question_result(self, result: Dict) -> Dict:\n",
    "        \"\"\"ì²´ì´ë‹ ê²°ê³¼ë¥¼ ë¬¸ì œ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±\"\"\"\n",
    "        final_output = result['final_output']\n",
    "        \n",
    "        # ê¸°ì¡´ íŒŒì‹± ë©”ì„œë“œ ì‚¬ìš©\n",
    "        question = self._clean_question(final_output)\n",
    "        answer = self._extract_answer(final_output)\n",
    "        \n",
    "        return {\n",
    "            'id': f\"CHAIN_{int(time.time()*1000) % 1000000:06d}\",\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'metadata': {\n",
    "                'chain_steps': len(result['chain_history']),\n",
    "                'initial_score': result.get('validation_score', 0),\n",
    "                'final_score': result.get('final_score', 0),\n",
    "                'context_used': bool(result.get('context'))\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _extract_question_answer(self, generated: str) -> Dict:\n",
    "        \"\"\"ê¸°ì¡´ ë°©ì‹ì˜ ì§ˆë¬¸/ë‹µë³€ ì¶”ì¶œ\"\"\"\n",
    "        question = self._clean_question(generated)\n",
    "        answer = self._extract_answer(generated)\n",
    "        \n",
    "        return {\n",
    "            'id': f\"GEN_{int(time.time()*1000) % 1000000:06d}\",\n",
    "            'question': question,\n",
    "            'answer': answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ê¸°ì¡´ retriever í™œìš©)\n",
    "print(\"ğŸ”„ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n",
    "\n",
    "# ë¬¸ì„œ ê²½ë¡œ í™•ì¸\n",
    "data_dir = PROJECT_ROOT / 'data'\n",
    "external_dir = data_dir / 'external'\n",
    "\n",
    "if not external_dir.exists():\n",
    "    print(f\"âš ï¸ ì™¸ë¶€ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {external_dir}\")\n",
    "    print(\"PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    # DocumentRetriever ì´ˆê¸°í™”\n",
    "    retriever = DocumentRetriever(\n",
    "        data_dir=str(data_dir),\n",
    "        use_embedding=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    stats = retriever.get_statistics()\n",
    "    print(f\"\\nğŸ“Š RAG í†µê³„:\")\n",
    "    print(f\"  - ë¬¸ì„œ ìˆ˜: {stats['total_documents']}\")\n",
    "    print(f\"  - ì²­í¬ ìˆ˜: {stats['total_chunks']}\")\n",
    "    print(f\"  - í‰ê·  ì²­í¬ í¬ê¸°: {stats['avg_chunk_size']:.1f} í† í°\")\n",
    "    print(f\"  - ì„ë² ë”© ì‚¬ìš©: {stats['use_embedding']}\")\n",
    "    print(f\"  - ë¬¸ì„œ íƒ€ì…: {stats['document_types']}\")\n",
    "\n",
    "\n",
    "def multi_method_search(retriever, query: str, methods: List[str] = None) -> str:\n",
    "    \"\"\"ì—¬ëŸ¬ ê²€ìƒ‰ ë°©ë²•ì„ ì¡°í•©í•œ ê²€ìƒ‰\"\"\"\n",
    "    if methods is None:\n",
    "        methods = ['similarity', 'bm25', 'hybrid']\n",
    "    \n",
    "    all_contexts = []\n",
    "    seen_chunks = set()\n",
    "    \n",
    "    for method in methods:\n",
    "        try:\n",
    "            context = retriever.search(query, top_k=3, method=method)\n",
    "            # ì¤‘ë³µ ì œê±°\n",
    "            if context and context not in seen_chunks:\n",
    "                all_contexts.append(f\"[{method.upper()}]\\n{context}\")\n",
    "                seen_chunks.add(context[:100])  # ì•ë¶€ë¶„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {method} ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(all_contexts)\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "if 'retriever' in locals():\n",
    "    test_query = \"ì „ìê¸ˆìœµê±°ë˜\"\n",
    "    test_result = multi_method_search(retriever, test_query, ['similarity', 'bm25'])\n",
    "    print(f\"\\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ '{test_query}':\")\n",
    "    print(test_result[:500] + \"...\" if len(test_result) > 500 else test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í”„ë¡¬í”„íŠ¸ ì²´ì´ë‹ í…œí”Œë¦¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "PROMPT_TEMPLATES = {\n",
    "    'initial': \"\"\"ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµë³´ì•ˆì›ì˜ ìˆ˜ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "ë‹¤ìŒ ê¸ˆìœµ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ FSKU í‰ê°€ì— ì í•©í•œ ê³ í’ˆì§ˆ ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ğŸ“Œ ë¬¸ì„œ ë‚´ìš©:\n",
    "{context}\n",
    "\n",
    "ğŸ“‹ ë¬¸ì œ ìš”êµ¬ì‚¬í•­:\n",
    "1. í˜•ì‹: ê°ê´€ì‹(4-5ì§€ì„ ë‹¤) ë˜ëŠ” ì£¼ê´€ì‹\n",
    "2. ë‚œì´ë„: ì¤‘ìƒê¸‰ (ê¸ˆìœµ ì „ë¬¸ê°€ ëŒ€ìƒ)\n",
    "3. ë‚´ìš©: ì‹¤ë¬´ì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ ì§€ì‹\n",
    "4. ëª…í™•ì„±: ëª¨í˜¸í•˜ì§€ ì•Šê³  êµ¬ì²´ì ì¸ í‘œí˜„\n",
    "\n",
    "ğŸ¯ ìƒì„±í•  ë¬¸ì œ ìœ í˜•: {question_type}\n",
    "\n",
    "[ë¬¸ì œ]\n",
    "(ì—¬ê¸°ì— ë¬¸ì œ ì‘ì„±)\n",
    "\n",
    "[ì„ íƒì§€] (ê°ê´€ì‹ì¸ ê²½ìš°)\n",
    "1) \n",
    "2) \n",
    "3) \n",
    "4) \n",
    "5) \n",
    "\n",
    "[ì •ë‹µ]\n",
    "(ì •ë‹µ ë²ˆí˜¸ ë˜ëŠ” ì„œìˆ í˜• ë‹µë³€)\n",
    "\n",
    "[í•´ì„¤]\n",
    "(ì •ë‹µì˜ ê·¼ê±°ì™€ ì˜¤ë‹µ ë¶„ì„)\"\"\",\n",
    "    \n",
    "    'validation': \"\"\"ê¸ˆìœµ êµìœ¡ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì‹œí—˜ ë¬¸ì œë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ğŸ“ í‰ê°€í•  ë¬¸ì œ:\n",
    "{question}\n",
    "\n",
    "ğŸ” í‰ê°€ ê¸°ì¤€:\n",
    "1. ëª…í™•ì„± (1-10): ë¬¸ì œê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?\n",
    "2. ì •í™•ì„± (1-10): ê¸ˆìœµ ì§€ì‹ê³¼ ë²•ê·œê°€ ì •í™•í•œê°€?\n",
    "3. ë‚œì´ë„ (1-10): ì „ë¬¸ê°€ ìˆ˜ì¤€ì— ì ì ˆí•œê°€?\n",
    "4. ì‹¤ë¬´ì„± (1-10): ì‹¤ì œ ì—…ë¬´ì— ë„ì›€ì´ ë˜ëŠ”ê°€?\n",
    "\n",
    "ğŸ’¡ ì¶œë ¥ í˜•ì‹:\n",
    "[ì ìˆ˜] ì´ì : XX/40\n",
    "- ëª…í™•ì„±: X/10\n",
    "- ì •í™•ì„±: X/10\n",
    "- ë‚œì´ë„: X/10\n",
    "- ì‹¤ë¬´ì„±: X/10\n",
    "\n",
    "[í”¼ë“œë°±]\n",
    "ê°œì„ ì´ í•„ìš”í•œ êµ¬ì²´ì ì¸ ë¶€ë¶„\n",
    "\n",
    "[ì œì•ˆ]\n",
    "ê°œì„ ëœ ë¬¸ì œ ì „ì²´\"\"\",\n",
    "    \n",
    "    'refinement': \"\"\"ë‹¤ìŒ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ê¸ˆìœµ ì‹œí—˜ ë¬¸ì œë¥¼ ê°œì„ í•˜ì„¸ìš”.\n",
    "\n",
    "ğŸ“Œ ì›ë³¸ ë¬¸ì œ:\n",
    "{original}\n",
    "\n",
    "ğŸ’­ í”¼ë“œë°±:\n",
    "{feedback}\n",
    "\n",
    "âœ¨ ê°œì„  ì§€ì¹¨:\n",
    "- ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í‘œí˜„ ì‚¬ìš©\n",
    "- ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ì˜ ì •í™•í•œ ì‚¬ìš©\n",
    "- ì‹¤ë¬´ ìƒí™©ì„ ë°˜ì˜í•œ ì‚¬ë¡€ í¬í•¨\n",
    "- ì„ íƒì§€ ê°„ ëª…í™•í•œ êµ¬ë¶„\n",
    "\n",
    "ğŸ“ ê°œì„ ëœ ë¬¸ì œ:\n",
    "[ë¬¸ì œ]\n",
    "\n",
    "[ì„ íƒì§€]\n",
    "\n",
    "[ì •ë‹µ]\n",
    "\n",
    "[í•´ì„¤]\"\"\",\n",
    "    \n",
    "    'final_check': \"\"\"ìµœì¢… í’ˆì§ˆ ê²€ì¦ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "ë¬¸ì œ: {question}\n",
    "\n",
    "ì²´í¬ë¦¬ìŠ¤íŠ¸:\n",
    "â–¡ ë¬¸ë²•ì  ì˜¤ë¥˜ê°€ ì—†ëŠ”ê°€?\n",
    "â–¡ ê¸ˆìœµ ìš©ì–´ê°€ ì •í™•í•œê°€?\n",
    "â–¡ ë²•ë ¹ ì¡°í•­ì´ ìµœì‹ ì¸ê°€?\n",
    "â–¡ ì •ë‹µì´ ëª…í™•í•œê°€?\n",
    "â–¡ ë‚œì´ë„ê°€ ì ì ˆí•œê°€?\n",
    "\n",
    "ìµœì¢… í‰ê°€: [í†µê³¼/ìˆ˜ì •í•„ìš”]\"\"\"\n",
    "}\n",
    "\n",
    "# ë¬¸ì œ ìœ í˜• ì •ì˜\n",
    "QUESTION_TYPES = [\n",
    "    \"ì •ì˜í˜• (ìš©ì–´ë‚˜ ê°œë…ì˜ ì •ì˜ë¥¼ ë¬»ëŠ” ë¬¸ì œ)\",\n",
    "    \"ì‚¬ë¡€í˜• (ì‹¤ì œ ìƒí™©ì—ì„œì˜ ì ìš©ì„ ë¬»ëŠ” ë¬¸ì œ)\",\n",
    "    \"ê³„ì‚°í˜• (ìˆ˜ì¹˜ ê³„ì‚°ì´ í•„ìš”í•œ ë¬¸ì œ)\",\n",
    "    \"íŒë‹¨í˜• (ì˜³ê³  ê·¸ë¦„ì„ íŒë‹¨í•˜ëŠ” ë¬¸ì œ)\",\n",
    "    \"ë¹„êµí˜• (ì—¬ëŸ¬ ê°œë…ì„ ë¹„êµí•˜ëŠ” ë¬¸ì œ)\",\n",
    "    \"ë²•ë ¹í˜• (íŠ¹ì • ë²•ë ¹ ì¡°í•­ì„ ë¬»ëŠ” ë¬¸ì œ)\"\n",
    "]\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì €ì¥\n",
    "prompt_dir = config['output_dir'] / 'prompts'\n",
    "prompt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name, template in PROMPT_TEMPLATES.items():\n",
    "    with open(prompt_dir / f'{name}_prompt.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(template)\n",
    "\n",
    "print(f\"âœ… {len(PROMPT_TEMPLATES)}ê°œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {prompt_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMonitor:\n",
    "    \"\"\"ì‹¤ì‹œê°„ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'generated': [],\n",
    "            'passed': [],\n",
    "            'quality_scores': [],\n",
    "            'generation_speed': [],\n",
    "            'chain_improvements': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "        self.last_update = self.start_time\n",
    "    \n",
    "    def update(self, new_questions: List[Dict], total_generated: int, total_passed: int):\n",
    "        \"\"\"ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # ìƒì„± ì†ë„ ê³„ì‚° (ë¶„ë‹¹)\n",
    "        time_diff = (current_time - self.last_update) / 60\n",
    "        if time_diff > 0:\n",
    "            speed = len(new_questions) / time_diff\n",
    "        else:\n",
    "            speed = 0\n",
    "        \n",
    "        self.metrics['generated'].append(total_generated)\n",
    "        self.metrics['passed'].append(total_passed)\n",
    "        self.metrics['generation_speed'].append(speed)\n",
    "        self.metrics['timestamps'].append(current_time - self.start_time)\n",
    "        \n",
    "        # í’ˆì§ˆ ì ìˆ˜ ì¶”ê°€\n",
    "        for q in new_questions:\n",
    "            if 'metadata' in q and 'final_score' in q['metadata']:\n",
    "                self.metrics['quality_scores'].append(q['metadata']['final_score'])\n",
    "                \n",
    "                # ì²´ì´ë‹ ê°œì„ ë„\n",
    "                if 'initial_score' in q['metadata']:\n",
    "                    improvement = q['metadata']['final_score'] - q['metadata']['initial_score']\n",
    "                    self.metrics['chain_improvements'].append(improvement)\n",
    "        \n",
    "        self.last_update = current_time\n",
    "    \n",
    "    def plot_dashboard(self):\n",
    "        \"\"\"ëŒ€ì‹œë³´ë“œ í”Œë¡¯\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('FSKU ë°ì´í„° ìƒì„± ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ', fontsize=16)\n",
    "        \n",
    "        # 1. ìƒì„± ì§„í–‰ë¥ \n",
    "        if self.metrics['generated']:\n",
    "            axes[0, 0].plot(self.metrics['timestamps'], self.metrics['generated'], 'b-', linewidth=2)\n",
    "            axes[0, 0].fill_between(self.metrics['timestamps'], 0, self.metrics['generated'], alpha=0.3)\n",
    "            axes[0, 0].set_title('ëˆ„ì  ìƒì„± ë¬¸ì œ ìˆ˜')\n",
    "            axes[0, 0].set_xlabel('ì‹œê°„ (ì´ˆ)')\n",
    "            axes[0, 0].set_ylabel('ë¬¸ì œ ìˆ˜')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. í†µê³¼ìœ¨\n",
    "        if self.metrics['generated'] and self.metrics['passed']:\n",
    "            pass_rates = [p/g*100 if g > 0 else 0 for p, g in zip(self.metrics['passed'], self.metrics['generated'])]\n",
    "            axes[0, 1].plot(self.metrics['timestamps'], pass_rates, 'g-', linewidth=2)\n",
    "            axes[0, 1].axhline(y=config['min_quality_score'], color='r', linestyle='--', label=f\"ëª©í‘œ: {config['min_quality_score']}%\")\n",
    "            axes[0, 1].set_title('í’ˆì§ˆ í†µê³¼ìœ¨ (%)')\n",
    "            axes[0, 1].set_xlabel('ì‹œê°„ (ì´ˆ)')\n",
    "            axes[0, 1].set_ylabel('í†µê³¼ìœ¨ (%)')\n",
    "            axes[0, 1].set_ylim(0, 100)\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. ìƒì„± ì†ë„\n",
    "        if self.metrics['generation_speed']:\n",
    "            axes[0, 2].plot(self.metrics['timestamps'], self.metrics['generation_speed'], 'r-', linewidth=2)\n",
    "            axes[0, 2].set_title('ìƒì„± ì†ë„ (ë¬¸ì œ/ë¶„)')\n",
    "            axes[0, 2].set_xlabel('ì‹œê°„ (ì´ˆ)')\n",
    "            axes[0, 2].set_ylabel('ì†ë„')\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. í’ˆì§ˆ ì ìˆ˜ ë¶„í¬\n",
    "        if self.metrics['quality_scores']:\n",
    "            axes[1, 0].hist(self.metrics['quality_scores'], bins=20, alpha=0.7, color='purple')\n",
    "            axes[1, 0].axvline(x=config['min_quality_score'], color='r', linestyle='--', label=f\"ìµœì†Œ: {config['min_quality_score']}\")\n",
    "            axes[1, 0].set_title('í’ˆì§ˆ ì ìˆ˜ ë¶„í¬')\n",
    "            axes[1, 0].set_xlabel('ì ìˆ˜')\n",
    "            axes[1, 0].set_ylabel('ë¹ˆë„')\n",
    "            axes[1, 0].legend()\n",
    "        \n",
    "        # 5. ì²´ì´ë‹ ê°œì„ ë„\n",
    "        if self.metrics['chain_improvements']:\n",
    "            axes[1, 1].hist(self.metrics['chain_improvements'], bins=20, alpha=0.7, color='orange')\n",
    "            avg_improvement = np.mean(self.metrics['chain_improvements'])\n",
    "            axes[1, 1].axvline(x=avg_improvement, color='g', linestyle='--', label=f\"í‰ê· : {avg_improvement:.1f}\")\n",
    "            axes[1, 1].set_title('ì²´ì´ë‹ í’ˆì§ˆ ê°œì„ ë„')\n",
    "            axes[1, 1].set_xlabel('ê°œì„  ì ìˆ˜')\n",
    "            axes[1, 1].set_ylabel('ë¹ˆë„')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        # 6. ìš”ì•½ í†µê³„\n",
    "        axes[1, 2].axis('off')\n",
    "        if self.metrics['generated']:\n",
    "            total_time = (time.time() - self.start_time) / 60  # ë¶„\n",
    "            stats_text = f\"\"\"ğŸ“Š ì‹¤ì‹œê°„ í†µê³„\n",
    "            \n",
    "ì´ ìƒì„±: {self.metrics['generated'][-1]:,}ê°œ\n",
    "í†µê³¼: {self.metrics['passed'][-1]:,}ê°œ\n",
    "í†µê³¼ìœ¨: {self.metrics['passed'][-1]/self.metrics['generated'][-1]*100:.1f}%\n",
    "\n",
    "ê²½ê³¼ ì‹œê°„: {total_time:.1f}ë¶„\n",
    "í‰ê·  ì†ë„: {self.metrics['generated'][-1]/total_time:.1f}ê°œ/ë¶„\n",
    "\n",
    "í‰ê·  í’ˆì§ˆ: {np.mean(self.metrics['quality_scores']):.1f}ì \n",
    "ìµœê³  í’ˆì§ˆ: {max(self.metrics['quality_scores']):.1f}ì \n",
    "ìµœì € í’ˆì§ˆ: {min(self.metrics['quality_scores']):.1f}ì \n",
    "\"\"\"\n",
    "            axes[1, 2].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„\n",
    "        if self.metrics['generated'] and self.metrics['generated'][-1] > 0:\n",
    "            current_rate = self.metrics['generated'][-1] / (time.time() - self.start_time)\n",
    "            remaining = config['num_questions'] - self.metrics['generated'][-1]\n",
    "            eta_seconds = remaining / current_rate if current_rate > 0 else 0\n",
    "            eta_minutes = eta_seconds / 60\n",
    "            \n",
    "            print(f\"\\nâ±ï¸ ì˜ˆìƒ ì™„ë£Œ ì‹œê°„: {eta_minutes:.1f}ë¶„ í›„\")\n",
    "            print(f\"ğŸ¯ ëª©í‘œ ë‹¬ì„±ë¥ : {self.metrics['generated'][-1]/config['num_questions']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëŒ€ê·œëª¨ ë°°ì¹˜ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_large_dataset(config: Dict[str, Any], retriever: DocumentRetriever) -> List[Dict]:\n",
    "    \"\"\"ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ ëŒ€ê·œëª¨ ë°ì´í„° ìƒì„± ì‹œì‘\")\n",
    "    print(f\"ğŸ“‹ ì„¤ì •:\")\n",
    "    print(f\"  - ëª©í‘œ: {config['num_questions']:,}ê°œ\")\n",
    "    print(f\"  - ëª¨ë¸: {config['model_name']}\")\n",
    "    print(f\"  - ë°°ì¹˜ í¬ê¸°: {config['batch_size']}\")\n",
    "    print(f\"  - ì²´ì´ë‹: {config['use_chaining']}\")\n",
    "    print(f\"  - ìµœì†Œ í’ˆì§ˆ: {config['min_quality_score']}ì \")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "    generator = ChainDataGenerator(config)\n",
    "    monitor = ProgressMonitor()\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    all_questions = []\n",
    "    all_metadata = []\n",
    "    checkpoint_counter = 0\n",
    "    \n",
    "    # ì²­í¬ ê°€ì ¸ì˜¤ê¸°\n",
    "    all_chunks = retriever.chunks\n",
    "    print(f\"\\nğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬: {len(all_chunks)}ê°œ\")\n",
    "    \n",
    "    # ì²­í¬ë¥¼ ìˆœí™˜í•˜ë©° ì‚¬ìš©\n",
    "    chunk_index = 0\n",
    "    total_generated = 0\n",
    "    total_passed = 0\n",
    "    \n",
    "    try:\n",
    "        with tqdm(total=config['num_questions'], desc=\"ì „ì²´ ì§„í–‰ë¥ \") as pbar:\n",
    "            while len(all_questions) < config['num_questions']:\n",
    "                # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ì²­í¬ ì„ íƒ\n",
    "                batch_chunks = []\n",
    "                for _ in range(config['batch_size']):\n",
    "                    batch_chunks.append(all_chunks[chunk_index % len(all_chunks)])\n",
    "                    chunk_index += 1\n",
    "                \n",
    "                # ë°°ì¹˜ ìƒì„±\n",
    "                batch_start_time = time.time()\n",
    "                batch_questions = generator.generate_questions_batch(\n",
    "                    chunks=batch_chunks,\n",
    "                    batch_size=config['batch_size']\n",
    "                )\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                \n",
    "                # í’ˆì§ˆ í•„í„°ë§\n",
    "                passed_questions = []\n",
    "                for q in batch_questions:\n",
    "                    total_generated += 1\n",
    "                    \n",
    "                    # í’ˆì§ˆ ê²€ì‚¬\n",
    "                    if 'metadata' in q and q['metadata'].get('final_score', 0) >= config['min_quality_score']:\n",
    "                        passed_questions.append(q)\n",
    "                        total_passed += 1\n",
    "                    elif 'metadata' not in q:  # ê¸°ì¡´ ë°©ì‹ ìƒì„±\n",
    "                        # ê°„ë‹¨í•œ í’ˆì§ˆ ì²´í¬\n",
    "                        if len(q.get('question', '')) > 20 and len(q.get('answer', '')) > 0:\n",
    "                            passed_questions.append(q)\n",
    "                            total_passed += 1\n",
    "                \n",
    "                # ê²°ê³¼ ì¶”ê°€\n",
    "                all_questions.extend(passed_questions)\n",
    "                pbar.update(len(passed_questions))\n",
    "                \n",
    "                # ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸\n",
    "                monitor.update(passed_questions, total_generated, total_passed)\n",
    "                \n",
    "                # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ (ì„¤ì •ëœ ê°„ê²©ë§ˆë‹¤)\n",
    "                if total_generated % config['update_interval'] == 0 and config['enable_monitoring']:\n",
    "                    monitor.plot_dashboard()\n",
    "                \n",
    "                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "                if len(all_questions) % config['checkpoint_interval'] == 0 and len(all_questions) > 0:\n",
    "                    checkpoint_counter += 1\n",
    "                    checkpoint_file = config['output_dir'] / f'checkpoint_{checkpoint_counter}_{len(all_questions)}.jsonl'\n",
    "                    \n",
    "                    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                        for q in all_questions[-config['checkpoint_interval']:]:\n",
    "                            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "                    \n",
    "                    print(f\"\\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_file.name}\")\n",
    "                    print(f\"   ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {batch_time:.1f}ì´ˆ\")\n",
    "                    print(f\"   ë°°ì¹˜ í†µê³¼ìœ¨: {len(passed_questions)/len(batch_questions)*100:.1f}%\")\n",
    "                \n",
    "                # ëª©í‘œ ë‹¬ì„± ì‹œ ì¢…ë£Œ\n",
    "                if len(all_questions) >= config['num_questions']:\n",
    "                    all_questions = all_questions[:config['num_questions']]\n",
    "                    break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # ìµœì¢… ëŒ€ì‹œë³´ë“œ í‘œì‹œ\n",
    "        if config['enable_monitoring']:\n",
    "            monitor.plot_dashboard()\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        generator.cleanup()\n",
    "    \n",
    "    print(f\"\\nâœ… ìƒì„± ì™„ë£Œ!\")\n",
    "    print(f\"  - ì´ ì‹œë„: {total_generated:,}ê°œ\")\n",
    "    print(f\"  - í†µê³¼: {len(all_questions):,}ê°œ\")\n",
    "    print(f\"  - í†µê³¼ìœ¨: {len(all_questions)/total_generated*100:.1f}%\")\n",
    "    \n",
    "    return all_questions\n",
    "\n",
    "\n",
    "def save_final_dataset(questions: List[Dict], config: Dict[str, Any]):\n",
    "    \"\"\"ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥\"\"\"\n",
    "    print(\"\\nğŸ“ ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. ì „ì²´ ë°ì´í„° JSONL ì €ì¥\n",
    "    full_file = config['output_dir'] / f'full_dataset_{timestamp}.jsonl'\n",
    "    with open(full_file, 'w', encoding='utf-8') as f:\n",
    "        for q in questions:\n",
    "            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "    print(f\"âœ… ì „ì²´ ë°ì´í„°: {full_file.name}\")\n",
    "    \n",
    "    # 2. í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # 80/10/10 ë¶„í• \n",
    "    train_val, test = train_test_split(questions, test_size=0.1, random_state=42)\n",
    "    train, val = train_test_split(train_val, test_size=0.111, random_state=42)  # 0.111 â‰ˆ 10/90\n",
    "    \n",
    "    # ê°ê° ì €ì¥\n",
    "    splits = {\n",
    "        'train': train,\n",
    "        'val': val,\n",
    "        'test': test\n",
    "    }\n",
    "    \n",
    "    for split_name, split_data in splits.items():\n",
    "        split_file = config['output_dir'] / f'{split_name}_{timestamp}.jsonl'\n",
    "        with open(split_file, 'w', encoding='utf-8') as f:\n",
    "            for q in split_data:\n",
    "                f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "        print(f\"âœ… {split_name}: {split_file.name} ({len(split_data):,}ê°œ)\")\n",
    "    \n",
    "    # 3. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    report = {\n",
    "        'generation_info': {\n",
    "            'timestamp': timestamp,\n",
    "            'total_questions': len(questions),\n",
    "            'config': config\n",
    "        },\n",
    "        'splits': {\n",
    "            'train': len(train),\n",
    "            'val': len(val),\n",
    "            'test': len(test)\n",
    "        },\n",
    "        'sample_questions': questions[:5]  # ìƒ˜í”Œ 5ê°œ\n",
    "    }\n",
    "    \n",
    "    report_file = config['output_dir'] / f'generation_report_{timestamp}.json'\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… ë¦¬í¬íŠ¸: {report_file.name}\")\n",
    "    \n",
    "    # 4. ìƒ˜í”Œ ì¶œë ¥\n",
    "    print(\"\\nğŸ“ ìƒì„±ëœ ë¬¸ì œ ìƒ˜í”Œ:\")\n",
    "    for i, q in enumerate(questions[:3], 1):\n",
    "        print(f\"\\n[{i}] ID: {q['id']}\")\n",
    "        print(f\"ì§ˆë¬¸: {q['question'][:100]}...\" if len(q['question']) > 100 else f\"ì§ˆë¬¸: {q['question']}\")\n",
    "        print(f\"ë‹µë³€: {q['answer'][:50]}...\" if len(q['answer']) > 50 else f\"ë‹µë³€: {q['answer']}\")\n",
    "        if 'metadata' in q:\n",
    "            print(f\"í’ˆì§ˆ: {q['metadata'].get('final_score', 'N/A')}ì \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë©”ì¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì¸ ì‹¤í–‰\n",
    "if 'retriever' in locals():\n",
    "    # ë°ì´í„° ìƒì„± ì‹¤í–‰\n",
    "    questions = generate_large_dataset(config, retriever)\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    if questions:\n",
    "        save_final_dataset(questions, config)\n",
    "else:\n",
    "    print(\"âš ï¸ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"Cell 4ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ RAGë¥¼ ì´ˆê¸°í™”í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ê²°ê³¼ ë¶„ì„ ë° í†µê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„±ëœ ë°ì´í„° ë¶„ì„\n",
    "def analyze_generated_data(output_dir: Path):\n",
    "    \"\"\"ìƒì„±ëœ ë°ì´í„° ë¶„ì„\"\"\"\n",
    "    # ìµœì‹  íŒŒì¼ ì°¾ê¸°\n",
    "    jsonl_files = list(output_dir.glob('full_dataset_*.jsonl'))\n",
    "    if not jsonl_files:\n",
    "        print(\"âš ï¸ ìƒì„±ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    latest_file = max(jsonl_files, key=lambda f: f.stat().st_mtime)\n",
    "    print(f\"ğŸ“Š ë¶„ì„ íŒŒì¼: {latest_file.name}\")\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    questions = []\n",
    "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            questions.append(json.loads(line))\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ ë°ì´í„° í†µê³„:\")\n",
    "    print(f\"  - ì´ ë¬¸ì œ ìˆ˜: {len(questions):,}ê°œ\")\n",
    "    \n",
    "    # ë¬¸ì œ ê¸¸ì´ ë¶„ì„\n",
    "    q_lengths = [len(q['question']) for q in questions]\n",
    "    a_lengths = [len(q['answer']) for q in questions]\n",
    "    \n",
    "    print(f\"\\nğŸ“ ë¬¸ì œ ê¸¸ì´:\")\n",
    "    print(f\"  - í‰ê· : {np.mean(q_lengths):.1f}ì\")\n",
    "    print(f\"  - ìµœì†Œ: {min(q_lengths)}ì\")\n",
    "    print(f\"  - ìµœëŒ€: {max(q_lengths)}ì\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ ë‹µë³€ ê¸¸ì´:\")\n",
    "    print(f\"  - í‰ê· : {np.mean(a_lengths):.1f}ì\")\n",
    "    print(f\"  - ìµœì†Œ: {min(a_lengths)}ì\")\n",
    "    print(f\"  - ìµœëŒ€: {max(a_lengths)}ì\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° ë¶„ì„\n",
    "    has_metadata = sum(1 for q in questions if 'metadata' in q)\n",
    "    print(f\"\\nğŸ·ï¸ ë©”íƒ€ë°ì´í„°:\")\n",
    "    print(f\"  - ë©”íƒ€ë°ì´í„° í¬í•¨: {has_metadata}ê°œ ({has_metadata/len(questions)*100:.1f}%)\")\n",
    "    \n",
    "    if has_metadata > 0:\n",
    "        scores = [q['metadata'].get('final_score', 0) for q in questions if 'metadata' in q and 'final_score' in q['metadata']]\n",
    "        if scores:\n",
    "            print(f\"\\nğŸ¯ í’ˆì§ˆ ì ìˆ˜:\")\n",
    "            print(f\"  - í‰ê· : {np.mean(scores):.1f}ì \")\n",
    "            print(f\"  - í‘œì¤€í¸ì°¨: {np.std(scores):.1f}\")\n",
    "            print(f\"  - ìµœì†Œ: {min(scores):.1f}ì \")\n",
    "            print(f\"  - ìµœëŒ€: {max(scores):.1f}ì \")\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # ë¬¸ì œ ê¸¸ì´ ë¶„í¬\n",
    "    axes[0].hist(q_lengths, bins=30, alpha=0.7, color='blue')\n",
    "    axes[0].set_title('ë¬¸ì œ ê¸¸ì´ ë¶„í¬')\n",
    "    axes[0].set_xlabel('ê¸€ì ìˆ˜')\n",
    "    axes[0].set_ylabel('ë¹ˆë„')\n",
    "    \n",
    "    # ë‹µë³€ ê¸¸ì´ ë¶„í¬\n",
    "    axes[1].hist(a_lengths, bins=30, alpha=0.7, color='green')\n",
    "    axes[1].set_title('ë‹µë³€ ê¸¸ì´ ë¶„í¬')\n",
    "    axes[1].set_xlabel('ê¸€ì ìˆ˜')\n",
    "    axes[1].set_ylabel('ë¹ˆë„')\n",
    "    \n",
    "    # í’ˆì§ˆ ì ìˆ˜ ë¶„í¬\n",
    "    if scores:\n",
    "        axes[2].hist(scores, bins=20, alpha=0.7, color='red')\n",
    "        axes[2].set_title('í’ˆì§ˆ ì ìˆ˜ ë¶„í¬')\n",
    "        axes[2].set_xlabel('ì ìˆ˜')\n",
    "        axes[2].set_ylabel('ë¹ˆë„')\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'í’ˆì§ˆ ì ìˆ˜ ì—†ìŒ', ha='center', va='center')\n",
    "        axes[2].set_xlim(0, 1)\n",
    "        axes[2].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰\n",
    "analyze_generated_data(config['output_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì¶”ê°€ ìœ í‹¸ë¦¬í‹°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë³‘í•© ìœ í‹¸ë¦¬í‹°\n",
    "def merge_checkpoints(output_dir: Path) -> str:\n",
    "    \"\"\"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì„ ë³‘í•©\"\"\"\n",
    "    checkpoint_files = sorted(output_dir.glob('checkpoint_*.jsonl'))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(\"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nğŸ”€ {len(checkpoint_files)}ê°œ ì²´í¬í¬ì¸íŠ¸ ë³‘í•© ì¤‘...\")\n",
    "    \n",
    "    all_questions = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    for file in checkpoint_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                q = json.loads(line)\n",
    "                if q['id'] not in seen_ids:\n",
    "                    all_questions.append(q)\n",
    "                    seen_ids.add(q['id'])\n",
    "    \n",
    "    # ë³‘í•© íŒŒì¼ ì €ì¥\n",
    "    merged_file = output_dir / f'merged_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.jsonl'\n",
    "    with open(merged_file, 'w', encoding='utf-8') as f:\n",
    "        for q in all_questions:\n",
    "            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"âœ… ë³‘í•© ì™„ë£Œ: {merged_file.name}\")\n",
    "    print(f\"   ì´ {len(all_questions):,}ê°œ ë¬¸ì œ (ì¤‘ë³µ ì œê±°ë¨)\")\n",
    "    \n",
    "    return str(merged_file)\n",
    "\n",
    "\n",
    "# í’ˆì§ˆ í•„í„°ë§ ìœ í‹¸ë¦¬í‹°\n",
    "def filter_by_quality(input_file: Path, min_score: float = 85.0) -> str:\n",
    "    \"\"\"í’ˆì§ˆ ì ìˆ˜ë¡œ í•„í„°ë§\"\"\"\n",
    "    questions = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            q = json.loads(line)\n",
    "            if 'metadata' in q and q['metadata'].get('final_score', 0) >= min_score:\n",
    "                questions.append(q)\n",
    "    \n",
    "    # í•„í„°ë§ëœ íŒŒì¼ ì €ì¥\n",
    "    filtered_file = input_file.parent / f'filtered_{min_score}_{input_file.name}'\n",
    "    with open(filtered_file, 'w', encoding='utf-8') as f:\n",
    "        for q in questions:\n",
    "            f.write(json.dumps(q, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"âœ… í•„í„°ë§ ì™„ë£Œ: {filtered_file.name}\")\n",
    "    print(f\"   {len(questions):,}ê°œ ë¬¸ì œ (ìµœì†Œ {min_score}ì )\")\n",
    "    \n",
    "    return str(filtered_file)\n",
    "\n",
    "\n",
    "# HuggingFace í˜•ì‹ ë³€í™˜\n",
    "def convert_to_hf_format(input_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"HuggingFace datasets í˜•ì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            q = json.loads(line)\n",
    "            data.append({\n",
    "                'instruction': q['question'],\n",
    "                'output': q['answer'],\n",
    "                'id': q['id']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥ (HF ê¶Œì¥)\n",
    "    parquet_file = input_file.parent / f'{input_file.stem}.parquet'\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "    \n",
    "    print(f\"âœ… HF í˜•ì‹ ë³€í™˜ ì™„ë£Œ: {parquet_file.name}\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ìœ í‹¸ë¦¬í‹° ë©”ë‰´\n",
    "print(\"\\nğŸ› ï¸ ì¶”ê°€ ìœ í‹¸ë¦¬í‹°:\")\n",
    "print(\"1. merge_checkpoints(config['output_dir']) - ì²´í¬í¬ì¸íŠ¸ ë³‘í•©\")\n",
    "print(\"2. filter_by_quality(file_path, min_score) - í’ˆì§ˆ í•„í„°ë§\")\n",
    "print(\"3. convert_to_hf_format(file_path) - HuggingFace í˜•ì‹ ë³€í™˜\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}