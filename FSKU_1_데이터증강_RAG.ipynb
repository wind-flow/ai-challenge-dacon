{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSKU ë°ì´í„° ì¦ê°• ë° RAG ì‹œìŠ¤í…œ (í†µí•©ë³¸)\n",
    "## ë‹µë³€ ìƒì„± ë³´ì¥ + í†µí•©í˜•/ë¶„ë¦¬í˜• ì„ íƒ ê°€ëŠ¥\n",
    "\n",
    "### ğŸ¯ ì£¼ìš” íŠ¹ì§•\n",
    "1. **í†µí•©í˜• ìƒì„±**: ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•œ ë²ˆì— ìƒì„± (ë¹ ë¦„, ì¼ê´€ì„±)\n",
    "2. **ë¶„ë¦¬í˜• ìƒì„±**: ì§ˆë¬¸ ìƒì„± í›„ ë‹µë³€ ìƒì„± (ì •í™•í•¨, ìœ ì—°í•¨)\n",
    "3. **ë‹µë³€ ë³´ì¥**: 3ë‹¨ê³„ í´ë°± ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„± ë³´ì¥\n",
    "4. **RAG í†µí•©**: ë¬¸ì„œ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€ ìƒì„±\n",
    "5. **ì‹¤í—˜ ê°€ëŠ¥í•œ ì„¤ì •**: ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ—ºï¸ ì½”ë“œ ì‹¤í–‰ íë¦„ ê°€ì´ë“œ (ë°±ì—”ë“œ ê°œë°œìë¥¼ ìœ„í•œ)\n\n## ğŸ¯ ì´ ë…¸íŠ¸ë¶ì˜ ëª©ì \nê¸ˆìœµ AI ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ **ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±**í•˜ëŠ” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n\n## ğŸ“‹ ì‹¤í–‰ ìˆœì„œì™€ ê° ë‹¨ê³„ì˜ ì—­í• \n\n### 1ï¸âƒ£ **í™˜ê²½ ì„¤ì •** (Setup)\n```\në°±ì—”ë“œ ë¹„ìœ : ì„œë²„ ì´ˆê¸°í™” ë° ì˜ì¡´ì„± ì£¼ì…\n```\n- GPU ì„¤ì • = ì„œë²„ ë¦¬ì†ŒìŠ¤ í• ë‹¹\n- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ = npm install / pip install\n- ë””ë ‰í† ë¦¬ êµ¬ì¡° = í”„ë¡œì íŠ¸ ìŠ¤ìºí´ë”©\n\n### 2ï¸âƒ£ **ì„¤ì • í´ë˜ìŠ¤** (Configuration) \n```\në°±ì—”ë“œ ë¹„ìœ : application.yml / config.json\n```\n- ExperimentConfig = í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬\n- í•˜ì´í¼íŒŒë¼ë¯¸í„° = API rate limit, timeout ì„¤ì •\n- í”„ë¦¬ì…‹ = dev/staging/prod í™˜ê²½ ì„¤ì •\n\n### 3ï¸âƒ£ **RAG ì‹œìŠ¤í…œ** (Knowledge Base)\n```\në°±ì—”ë“œ ë¹„ìœ : ì™¸ë¶€ DB ì—°ê²° ë° ìºì‹± ë ˆì´ì–´\n```\n- PDF ë¡œë“œ = ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°\n- ë²¡í„° ì¸ë±ì‹± = Elasticsearch ì¸ë±ì‹±  \n- ìœ ì‚¬ë„ ê²€ìƒ‰ = Full-text search\n- ìºì‹± = Redis ìºì‹±\n\n### 4ï¸âƒ£ **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿** (Templates)\n```\në°±ì—”ë“œ ë¹„ìœ : API ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜\n```\n- í”„ë¡¬í”„íŠ¸ = Request DTO\n- ìƒì„±ëœ í…ìŠ¤íŠ¸ = Response DTO\n- íŒŒì‹± = Serialization/Deserialization\n\n### 5ï¸âƒ£ **ë°ì´í„° ìƒì„±ê¸°** (Generator)\n```\në°±ì—”ë“œ ë¹„ìœ : ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë ˆì´ì–´\n```\n- ëª¨ë¸ ë¡œë“œ = ì„œë¹„ìŠ¤ ì´ˆê¸°í™”\n- generate_text() = í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§\n- í†µí•©í˜•/ë¶„ë¦¬í˜•/CoT = ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì „ëµ íŒ¨í„´\n\n### 6ï¸âƒ£ **í’ˆì§ˆ ê²€ì¦** (Validation)\n```\në°±ì—”ë“œ ë¹„ìœ : ìœ íš¨ì„± ê²€ì‚¬ ë° ì—ëŸ¬ í•¸ë“¤ë§\n```\n- í’ˆì§ˆ ì ìˆ˜ = Validation rules\n- ì¬ì‹œë„ ë¡œì§ = Retry mechanism\n- í´ë°± = Circuit breaker pattern\n\n### 7ï¸âƒ£ **ë°°ì¹˜ ì²˜ë¦¬** (Batch Processing)\n```\në°±ì—”ë“œ ë¹„ìœ : ë°°ì¹˜ ì¡ ì‹¤í–‰\n```\n- ëŒ€ëŸ‰ ìƒì„± = Batch job\n- ì§„í–‰ìƒí™© ì¶”ì  = Job monitoring\n- ì¤‘ê°„ ì €ì¥ = Checkpointing\n\n## ğŸ’¡ í•µì‹¬ ì‹¤í–‰ ê²½ë¡œ\n\n```python\n# ë©”ì¸ ì‹¤í–‰ íë¦„\n1. config = ExperimentConfig()          # ì„¤ì • ë¡œë“œ\n2. rag = RAGSystem(config)              # RAG ì´ˆê¸°í™”\n3. rag.initialize()                     # ì¸ë±ìŠ¤ êµ¬ì¶•/ë¡œë“œ\n4. generator = AnswerGuaranteedGenerator(config, rag)  # ìƒì„±ê¸° ìƒì„±\n5. generator.load_model()               # ëª¨ë¸ ë¡œë“œ\n6. for context in contexts:\n      qa_pair = generator.generate_qa_pair(context)  # ë°ì´í„° ìƒì„±\n7. save_results(qa_pairs)              # ê²°ê³¼ ì €ì¥\n```\n\n## ğŸ”„ ê° ëª¨ë“œë³„ ë‚´ë¶€ íë¦„\n\n### í†µí•©í˜• (Integrated)\n```\nContext â†’ LLM â†’ Q&A ë™ì‹œ ìƒì„± â†’ íŒŒì‹± â†’ ê²€ì¦ â†’ ì €ì¥\n```\n\n### ë¶„ë¦¬í˜• (Separated) - ë” ì •í™•\\!\n```\nContext â†’ LLM â†’ ì§ˆë¬¸ ìƒì„±\n    â†“\nì§ˆë¬¸ìœ¼ë¡œ RAG ì¬ê²€ìƒ‰\n    â†“\nContext + RAG ê²°ê³¼ â†’ LLM â†’ ë‹µë³€ ìƒì„±\n    â†“\nê²€ì¦ â†’ ì €ì¥\n```\n\n### CoT (Chain-of-Thought) - ìµœê³  í’ˆì§ˆ\\!\n```\nì´ˆê¸° ìƒì„± â†’ ìê°€ ê²€ì¦ â†’ ê°œì„  â†’ ìµœì¢… ê²€ì¦\n   â†“           â†“           â†“         â†“\n  70ì ?      ë¬¸ì œ ë°œê²¬    ìˆ˜ì •      85ì ?\n```\n\n## ğŸ® ì‹¤ì „ ì‚¬ìš©ë²•\n\n```python\n# 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (5ë¶„)\nconfig.GENERATION_MODE = \"integrated\"\nconfig.BATCH_CONFIG['target_count'] = 10\n\n# 2. í’ˆì§ˆ ìš°ì„  (30ë¶„)\nconfig.GENERATION_MODE = \"cot\"\nconfig.COT_CONFIG['use_cot'] = True\nconfig.CURRENT_COT_PRESET = \"quality\"\n\n# 3. ëŒ€íšŒ ì œì¶œìš© (2ì‹œê°„)\nconfig.GENERATION_MODE = \"separated\"\nconfig.BATCH_CONFIG['target_count'] = 1000\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ“ ë°±ì—”ë“œ ê°œë°œìë¥¼ ìœ„í•œ AI í•µì‹¬ ê°œë…\n\n## ğŸ¤– LLM (Large Language Model)\n```\në°±ì—”ë“œ ë¹„ìœ : ì´ˆê±°ëŒ€ í•¨ìˆ˜\n- ì…ë ¥: í…ìŠ¤íŠ¸ (Request)\n- ì²˜ë¦¬: ìˆ˜ì‹­ì–µ ê°œ íŒŒë¼ë¯¸í„°ë¡œ ê³„ì‚°\n- ì¶œë ¥: í…ìŠ¤íŠ¸ (Response)\n```\n\n### ì£¼ìš” ê°œë… ë§¤í•‘\n| AI ìš©ì–´ | ë°±ì—”ë“œ ìš©ì–´ | ì„¤ëª… |\n|---------|------------|------|\n| Model | Service/Engine | ì‹¤ì œ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ |\n| Tokenizer | Parser/Serializer | í…ìŠ¤íŠ¸ â†” ìˆ«ì ë³€í™˜ |\n| Inference | API Call | ëª¨ë¸ì— ìš”ì²­ ë³´ë‚´ê³  ì‘ë‹µ ë°›ê¸° |\n| Fine-tuning | Customization | íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§• |\n| Prompt | Request Body | ëª¨ë¸ì— ë³´ë‚´ëŠ” ì…ë ¥ |\n| Temperature | Randomness Config | ì‘ë‹µì˜ ë‹¤ì–‘ì„± ì¡°ì ˆ (0=ê²°ì •ì , 1=ì°½ì˜ì ) |\n| Batch Size | Connection Pool Size | ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ |\n| Learning Rate | Update Speed | í•™ìŠµ ì†ë„ (ë„ˆë¬´ ë¹ ë¥´ë©´ ë¶ˆì•ˆì •) |\n\n## ğŸ§® ì–‘ìí™” (Quantization)\n```python\n# ë°±ì—”ë“œ ë¹„ìœ : ë°ì´í„° ì••ì¶•\n# ì›ë³¸: {\"price\": 123.456789} (float64)\n# ì••ì¶•: {\"price\": 123.46} (float16)\n# ë” ì••ì¶•: {\"price\": 123} (int8)\n\n# AIì—ì„œì˜ ì–‘ìí™”\nì›ë³¸ ëª¨ë¸: 28GB (FP32)\nâ†“ ì–‘ìí™”\nì••ì¶• ëª¨ë¸: 3.5GB (INT4)\n# ë©”ëª¨ë¦¬ 87.5% ì ˆì•½\\!\n```\n\n## ğŸ”— LoRA (Low-Rank Adaptation)\n```python\n# ë°±ì—”ë“œ ë¹„ìœ : ì–´ëŒ‘í„° íŒ¨í„´\n\nclass OriginalService:  # ê±°ëŒ€í•œ ì›ë³¸ ëª¨ë¸ (ìˆ˜ì • ë¶ˆê°€)\n    def process(self, input):\n        return expensive_computation(input)\n\nclass LoRAAdapter:  # ì‘ì€ ì–´ëŒ‘í„° (í•™ìŠµ ê°€ëŠ¥)\n    def __init__(self, rank=16):  # rank = ì–´ëŒ‘í„° í¬ê¸°\n        self.adapter_weights = small_matrix(rank)\n    \n    def process(self, input):\n        original_output = OriginalService.process(input)\n        adapter_output = self.adapter_weights @ input\n        return original_output + adapter_output  # ì›ë³¸ + ì–´ëŒ‘í„°\n\n# ì¥ì : ì›ë³¸ì€ ê·¸ëŒ€ë¡œ, ì–´ëŒ‘í„°ë§Œ í•™ìŠµ â†’ ë©”ëª¨ë¦¬ 99% ì ˆì•½\\!\n```\n\n## ğŸ“š RAG (Retrieval-Augmented Generation)\n```python\n# ë°±ì—”ë“œ ë¹„ìœ : ìºì‹œ + ì™¸ë¶€ API íŒ¨í„´\n\ndef generate_answer(question):\n    # 1. ìºì‹œ(DB) ê²€ìƒ‰\n    relevant_docs = search_database(question)  # SELECT * FROM docs WHERE ...\n    \n    # 2. ì»¨í…ìŠ¤íŠ¸ ë³´ê°•\n    context = f\"{question}\\nê´€ë ¨ ì •ë³´: {relevant_docs}\"\n    \n    # 3. LLM í˜¸ì¶œ\n    answer = llm.generate(context)  # ì™¸ë¶€ API í˜¸ì¶œì²˜ëŸ¼\n    \n    return answer\n\n# RAG ì—†ì´: \"ë°”ì ¤IIIê°€ ë­ì•¼?\" â†’ LLMì´ í•™ìŠµí•œ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€\n# RAG ìˆìœ¼ë©´: \"ë°”ì ¤IIIê°€ ë­ì•¼?\" â†’ DB ê²€ìƒ‰ â†’ ìµœì‹  ì •ë³´ í¬í•¨í•´ì„œ ë‹µë³€\n```\n\n## ğŸ”„ Fine-tuning í”„ë¡œì„¸ìŠ¤\n```python\n# ë°±ì—”ë“œ ë¹„ìœ : ì ì§„ì  ë°°í¬ (Canary Deployment)\n\n# 1. Pre-trained Model (ê¸°ë³¸ ëª¨ë¸)\nbase_model = load_model(\"gpt-base\")  # npm install express\n\n# 2. Add Custom Layer (ì»¤ìŠ¤í…€ ë ˆì´ì–´)\ncustom_layer = LoRAAdapter()  # ìš°ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§\n\n# 3. Training Loop (í•™ìŠµ ë£¨í”„)\nfor epoch in range(3):  # 3ë²ˆ ë°˜ë³µ\n    for batch in training_data:  # ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬\n        loss = compute_loss(batch)  # ì—ëŸ¬ ê³„ì‚°\n        update_weights(loss)  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n        \n        # ë°±ì—”ë“œì˜ ëª¨ë‹ˆí„°ë§ì²˜ëŸ¼\n        if step % 100 == 0:\n            log_metrics(loss, accuracy)\n            save_checkpoint()  # ì¤‘ê°„ ì €ì¥\n```\n\n## ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ\n```python\n# RTX 4090 (24GB) ê¸°ì¤€\n\n# âŒ ë‚˜ìœ ì˜ˆ: OOM (Out of Memory)\nmodel = load_model(\"70B-model\")  # 70B = 280GB í•„ìš”\\!\n\n# âœ… ì¢‹ì€ ì˜ˆ: ë©”ëª¨ë¦¬ ìµœì í™”\nmodel = load_model(\"7B-model\", quantization=\"4bit\")  # 3.5GBë§Œ ì‚¬ìš©\noptimizer = \"paged_adamw\"  # í˜ì´ì§•ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\ngradient_checkpointing = True  # ë©”ëª¨ë¦¬ â†” ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„\n\n# ë°±ì—”ë“œ ë¹„ìœ : \n# - Quantization = Response ì••ì¶• (gzip)\n# - Gradient Checkpointing = Lazy Loading\n# - Paged Optimizer = Swap ë©”ëª¨ë¦¬ í™œìš©\n```\n\n## ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ\n```python\n# ë°±ì—”ë“œì˜ ì„±ëŠ¥ íŠœë‹ê³¼ ìœ ì‚¬\n\n# 1. Learning Rate (ì²˜ë¦¬ ì†ë„)\n# - ë„ˆë¬´ ë†’ìŒ = 429 Too Many Requests (ë°œì‚°)\n# - ë„ˆë¬´ ë‚®ìŒ = 408 Request Timeout (í•™ìŠµ ì•ˆë¨)\n# - ì ì ˆí•¨ = 200 OK\n\n# 2. Batch Size (ë™ì‹œ ìš”ì²­ ìˆ˜)\n# - ë„ˆë¬´ í¼ = 503 Service Unavailable (OOM)\n# - ë„ˆë¬´ ì‘ìŒ = ë¹„íš¨ìœ¨ì  (ëŠë¦¼)\n# - ì ì ˆí•¨ = Thread Pool Sizeì²˜ëŸ¼ ì¡°ì ˆ\n\n# 3. Temperature (ì‘ë‹µ ë‹¤ì–‘ì„±)\n# - 0.1 = Deterministic (í•­ìƒ ê°™ì€ ì‘ë‹µ)\n# - 0.7 = Balanced (ì ë‹¹í•œ ë³€í™”)\n# - 1.5 = Creative (ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥)\n```"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# ğŸ“ AI ê°œë°œ ì‹¤ìŠµ: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n# ========================================\n# \n# ğŸ’¡ ì´ ì„¹ì…˜ì—ì„œ ë°°ìš°ê²Œ ë  ë‚´ìš©:\n#   1. AI ê°œë°œì— í•„ìš”í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì˜ ì—­í• \n#   2. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë°©ë²•\n#   3. í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì • ë°©ë²•\n\nimport os\nimport json\nimport time\nimport pickle\nimport warnings\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm  # ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ëŠ” í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë¼ì´ë¸ŒëŸ¬ë¦¬\n\n# ğŸ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ì„í¬íŠ¸\nimport torch  # PyTorch: ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ í”„ë ˆì„ì›Œí¬\nimport torch.nn as nn  # ì‹ ê²½ë§ ëª¨ë“ˆë“¤ (ì‹¤ì œë¡œëŠ” Transformer ëª¨ë¸ì´ ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆì–´ ì§ì ‘ ì‚¬ìš©í•  ì¼ì€ ì ìŒ)\n\n# ğŸ¤— Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ - LLM ì‚¬ìš©ì˜ í•µì‹¬\nfrom transformers import (\n    AutoModelForCausalLM,     # ìë™ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤ (GPT ìŠ¤íƒ€ì¼ ëª¨ë¸ìš©)\n    AutoTokenizer,            # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬\n    BitsAndBytesConfig,       # ì–‘ìí™”(Quantization) ì„¤ì • - ë©”ëª¨ë¦¬ ì ˆì•½ì˜ í•µì‹¬\\!\n)\n\n# ğŸ”§ PEFT (Parameter-Efficient Fine-Tuning) - LoRAì˜ í•µì‹¬\nfrom peft import (\n    LoraConfig,               # LoRA ì„¤ì • í´ë˜ìŠ¤\n    get_peft_model,          # ì¼ë°˜ ëª¨ë¸ì„ LoRA ëª¨ë¸ë¡œ ë³€í™˜\n    prepare_model_for_kbit_training,  # ì–‘ìí™”ëœ ëª¨ë¸ì„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦\n    TaskType                  # ì‘ì—… ìœ í˜• ì •ì˜ (ì˜ˆ: ì–¸ì–´ ìƒì„±)\n)\n\n# ğŸ“š RAG (Retrieval-Augmented Generation) ê´€ë ¨\nfrom sentence_transformers import SentenceTransformer  # ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜ (ì„ë² ë”©)\nimport faiss  # Facebookì˜ ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë§¤ìš° ë¹ ë¦„\\!)\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter  # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°\nimport PyPDF2  # PDF íŒŒì¼ ì½ê¸°\n\n# âš ï¸ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ê°œë°œ ì‹œì—ëŠ” ì£¼ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ)\nwarnings.filterwarnings('ignore')\n\n# ğŸ“‚ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •\n# ğŸ’¡ Path ê°ì²´ë¥¼ ì‚¬ìš©í•˜ë©´ OSì— ê´€ê³„ì—†ì´ ê²½ë¡œë¥¼ ë‹¤ë£° ìˆ˜ ìˆìŒ\nBASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"data\"\nEXTERNAL_DIR = DATA_DIR / \"external\"  # ì™¸ë¶€ ë¬¸ì„œ (PDF, Excel ë“±)\nAUGMENTED_DIR = DATA_DIR / \"augmented\"  # ìƒì„±ëœ ë°ì´í„°\nVECTORDB_DIR = DATA_DIR / \"vectordb\"  # RAG ì¸ë±ìŠ¤ ì €ì¥\nMODELS_DIR = BASE_DIR / \"models\"  # í•™ìŠµëœ ëª¨ë¸ ì €ì¥\nRESULTS_DIR = BASE_DIR / \"results\"  # ì‹¤í–‰ ê²°ê³¼\n\n# ë””ë ‰í† ë¦¬ ìƒì„± (exist_ok=True: ì´ë¯¸ ìˆì–´ë„ ì—ëŸ¬ ì•ˆ ë‚¨)\nfor dir_path in [DATA_DIR, EXTERNAL_DIR, AUGMENTED_DIR, VECTORDB_DIR, MODELS_DIR, RESULTS_DIR]:\n    dir_path.mkdir(parents=True, exist_ok=True)\n\n# ğŸ”§ ë¡œê¹… ì„¤ì •\nimport logging\nlogging.basicConfig(\n    level=logging.INFO,  # INFO ë ˆë²¨ ì´ìƒë§Œ ì¶œë ¥\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('training.log'),  # íŒŒì¼ì— ì €ì¥\n        logging.StreamHandler()  # ì½˜ì†”ì—ë„ ì¶œë ¥\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# ğŸ¯ GPU ì„¤ì • ë° ë©”ëª¨ë¦¬ ê´€ë¦¬\nif torch.cuda.is_available():\n    # GPU ì‚¬ìš© ê°€ëŠ¥\n    device = torch.device(\"cuda\")\n    print(f\"ğŸ® GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}\")\n    \n    # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥ (RTX 4090ì€ 24GB)\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    allocated = torch.cuda.memory_allocated() / 1024**3\n    print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB / {total_memory:.2f}GB\")\n    \n    # ğŸ’¡ ë©”ëª¨ë¦¬ ìµœì í™” íŒ: ìºì‹œ ë¹„ìš°ê¸°\n    torch.cuda.empty_cache()\nelse:\n    device = torch.device(\"cpu\")\n    print(\"âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n\n# ğŸŒ± ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n# ğŸ’¡ ì™œ ì¤‘ìš”í•œê°€? ë™ì¼í•œ ê²°ê³¼ë¥¼ ì¬í˜„í•˜ê¸° ìœ„í•´ í•„ìˆ˜\\!\ndef set_seed(seed: int = 42):\n    \"\"\"\n    ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n    ë”¥ëŸ¬ë‹ì—ì„œëŠ” ì—¬ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëœë¤ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ëª¨ë‘ ê³ ì •í•´ì•¼ í•¨.\n    \"\"\"\n    import random\n    \n    random.seed(seed)  # Python ê¸°ë³¸ random\n    np.random.seed(seed)  # NumPy random\n    torch.manual_seed(seed)  # PyTorch CPU\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)  # PyTorch GPU\n        torch.cuda.manual_seed_all(seed)  # ë©€í‹° GPU\n        # Deterministic ì—°ì‚° ê°•ì œ (ì•½ê°„ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\nprint(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\\!\")\nprint(f\"ğŸ“‚ ì‘ì—… ë””ë ‰í† ë¦¬: {BASE_DIR.absolute()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# PDF ì²˜ë¦¬\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# ë²¡í„° DB\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "PROJECT_ROOT = Path(\"/Users/gunwoo/Downloads/project/ai-dacon\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "EXTERNAL_DIR = DATA_DIR / \"external\"\n",
    "AUGMENTED_DIR = DATA_DIR / \"augmented\"\n",
    "VECTORDB_DIR = DATA_DIR / \"vectordb\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for dir_path in [EXTERNAL_DIR, AUGMENTED_DIR, VECTORDB_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ“ ì™¸ë¶€ ë°ì´í„°: {EXTERNAL_DIR}\")\n",
    "print(f\"ğŸ“ ì¦ê°• ë°ì´í„°: {AUGMENTED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ”§ ì‹¤í—˜ ì„¤ì • (ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# ğŸ“ ì‹¤í—˜ ì„¤ì • í´ë˜ìŠ¤ - AI ê°œë°œì˜ í•µì‹¬: í•˜ì´í¼íŒŒë¼ë¯¸í„° ê´€ë¦¬\n# ========================================\n#\n# ğŸ’¡ ì™œ ì„¤ì • í´ë˜ìŠ¤ê°€ ì¤‘ìš”í•œê°€?\n#   1. ì‹¤í—˜ ì¬í˜„ì„±: ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ\n#   2. A/B í…ŒìŠ¤íŠ¸: ì„¤ì •ë§Œ ë°”ê¿”ê°€ë©° ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥\n#   3. í˜‘ì—…: íŒ€ì›ë“¤ê³¼ ì„¤ì • ê³µìœ ê°€ ì‰¬ì›€\n\nclass ExperimentConfig:\n    \"\"\"\n    ì‹¤í—˜ ì„¤ì • í´ë˜ìŠ¤ - ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œ ê³³ì—ì„œ ê´€ë¦¬\n    \n    ğŸ’¡ í´ë˜ìŠ¤ ë³€ìˆ˜ vs ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜:\n    - ì—¬ê¸°ì„œëŠ” í´ë˜ìŠ¤ ë³€ìˆ˜ë¥¼ ì‚¬ìš© (ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ê°€ ê³µìœ )\n    - ì¥ì : ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ì „ì—­ ì„¤ì •ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ì¢‹ìŒ\n    \"\"\"\n    \n    # ===== ëª¨ë¸ ì„ íƒ =====\n    # ğŸ¤” ì–´ë–¤ ëª¨ë¸ì„ ì„ íƒí•´ì•¼ í• ê¹Œ?\n    # - í•œêµ­ì–´ ì„±ëŠ¥: korean_optimized, exaoneì´ ì¢‹ìŒ\n    # - ë²”ìš©ì„±: qwen, mistralì´ ì¢‹ìŒ\n    # - ì„±ëŠ¥: solarê°€ ê°€ì¥ í¬ê³  ì„±ëŠ¥ì´ ì¢‹ì§€ë§Œ ë©”ëª¨ë¦¬ë„ ë§ì´ í•„ìš”\n    MODEL_OPTIONS = {\n        \"korean_optimized\": \"beomi/llama-2-ko-7b\",      # 7B = 70ì–µ ê°œ íŒŒë¼ë¯¸í„°\n        \"solar\": \"upstage/SOLAR-10.7B-v1.0\",            # 10.7B = 107ì–µ ê°œ\n        \"exaone\": \"LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n        \"qwen\": \"Qwen/Qwen2.5-7B-Instruct\",\n        \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\"\n    }\n    \n    MODEL_NAME = MODEL_OPTIONS[\"korean_optimized\"]\n    \n    # ===== ìƒì„± ëª¨ë“œ ì„¤ì • =====\n    # ğŸ¯ ê° ëª¨ë“œì˜ íŠ¹ì§•ê³¼ ì‚¬ìš© ì‹œì :\n    GENERATION_MODE = \"integrated\"  # ê¸°ë³¸ê°’: í†µí•©í˜•\n    # - \"integrated\": ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•œ ë²ˆì— ìƒì„± (ë¹ ë¥´ì§€ë§Œ ë‹µë³€ í’ˆì§ˆì´ ë‚®ì„ ìˆ˜ ìˆìŒ)\n    # - \"separated\": ì§ˆë¬¸ ë¨¼ì €, ë‹µë³€ì€ RAG ê²€ìƒ‰ í›„ ìƒì„± (ëŠë¦¬ì§€ë§Œ ì •í™•)\n    # - \"cot\": Chain-of-Thought, ë‹¨ê³„ë³„ ê²€ì¦ (ë§¤ìš° ëŠë¦¬ì§€ë§Œ ìµœê³  í’ˆì§ˆ)\n    \n    # ===== CoT (Chain-of-Thought) ì„¤ì • =====\n    # ğŸ§  CoTë€? LLMì´ \"ìƒê°ì˜ ê³¼ì •\"ì„ ê±°ì³ ë‹µë³€í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•\n    COT_CONFIG = {\n        \"use_cot\": False,           # CoT ì‚¬ìš© ì—¬ë¶€\n        \"max_iterations\": 3,        # ê°œì„  ë°˜ë³µ íšŸìˆ˜ (ë§ì„ìˆ˜ë¡ í’ˆì§ˆâ†‘ ì†ë„â†“)\n        \"quality_threshold\": 80,    # í’ˆì§ˆ ê¸°ì¤€ (100ì  ë§Œì )\n        \n        # ğŸ” CoTì˜ 4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤:\n        # 1. ì´ˆê¸° ìƒì„± (Initial Generation)\n        # 2. ìê°€ ê²€ì¦ (Self-Verification) \n        # 3. ê°œì„  (Improvement)\n        # 4. ìµœì¢… í™•ì¸ (Final Check)\n        \"use_self_verification\": True,\n        \"use_improvement\": True,\n        \"use_final_check\": True,\n        \n        # ğŸ’¾ ìºì‹±: ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ê²°ê³¼ ì¬ì‚¬ìš©\n        \"cache_results\": True,\n        \n        # ì‹¤í—˜ìš© ì„¸ë¶€ ì„¤ì •ë“¤\n        \"verification_strictness\": \"medium\",  # ê²€ì¦ ì—„ê²©ë„\n        \"focus_areas\": [\"accuracy\", \"clarity\", \"completeness\"],  # ì§‘ì¤‘ ì˜ì—­\n        \"reasoning_depth\": 2,       # ì¶”ë¡  ê¹Šì´ (1~3)\n        \"multi_perspective\": True,  # ë‹¤ê°ë„ ê²€ì¦\n        \"self_critique_level\": 2,  # ìê¸° ë¹„íŒ ìˆ˜ì¤€ (0~3)\n    }\n    \n    # CoT í”„ë¦¬ì…‹ - ë¹ ë¥´ê²Œ ì„¤ì • ì „í™˜ ê°€ëŠ¥\n    # ğŸ’¡ í”„ë¦¬ì…‹ì„ ì‚¬ìš©í•˜ë©´ ì¼ì¼ì´ ì„¤ì •ì„ ë°”ê¾¸ì§€ ì•Šì•„ë„ ë¨\\!\n    COT_PRESETS = {\n        \"fast\": {  # ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ìš©\n            \"max_iterations\": 1,\n            \"quality_threshold\": 70,\n            \"use_improvement\": False,\n        },\n        \"balanced\": {  # ê· í˜•ì¡íŒ ì„¤ì • (ì¶”ì²œ\\!)\n            \"max_iterations\": 2,\n            \"quality_threshold\": 75,\n            \"use_improvement\": True,\n        },\n        \"quality\": {  # í’ˆì§ˆ ìš°ì„ \n            \"max_iterations\": 4,\n            \"quality_threshold\": 85,\n            \"use_improvement\": True,\n            \"multi_perspective\": True,\n        },\n        \"research\": {  # ì—°êµ¬/ë…¼ë¬¸ìš© (ë§¤ìš° ëŠë¦¼)\n            \"max_iterations\": 5,\n            \"quality_threshold\": 90,\n            \"use_improvement\": True,\n            \"example_generation\": True,\n        }\n    }\n    \n    CURRENT_COT_PRESET = \"balanced\"\n    \n    # í”„ë¦¬ì…‹ ì ìš© ë¡œì§\n    # ğŸ’¡ Pythonì˜ ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸ íŒ¨í„´\n    if CURRENT_COT_PRESET in COT_PRESETS:\n        for key, value in COT_PRESETS[CURRENT_COT_PRESET].items():\n            if key in COT_CONFIG:\n                COT_CONFIG[key] = value\n    \n    # ===== ì–‘ìí™”(Quantization) ì„¤ì • =====\n    # ğŸ¯ ì–‘ìí™”ë€? ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì••ì¶•í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê¸°ë²•\n    # - FP32 (32ë¹„íŠ¸) â†’ FP16 (16ë¹„íŠ¸) â†’ INT8 (8ë¹„íŠ¸) â†’ INT4 (4ë¹„íŠ¸)\n    # - 4ë¹„íŠ¸ ì–‘ìí™” ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 1/8ë¡œ ì¤„ì–´ë“¦\\!\n    USE_QUANTIZATION = True  # RTX 4090 24GBì—ì„œëŠ” í•„ìˆ˜\\!\n    \n    QUANTIZATION_CONFIG = {\n        \"load_in_4bit\": True,  # 4ë¹„íŠ¸ë¡œ ë¡œë“œ\n        \"bnb_4bit_quant_type\": \"nf4\",  # NormalFloat4 - ì •ê·œë¶„í¬ ê¸°ë°˜ ì–‘ìí™”\n        \"bnb_4bit_compute_dtype\": torch.float16,  # ê³„ì‚°ì€ FP16ìœ¼ë¡œ\n        \"bnb_4bit_use_double_quant\": True  # ì´ì¤‘ ì–‘ìí™” (ë” ì••ì¶•\\!)\n    }\n    \n    # ===== ìƒì„± íŒŒë¼ë¯¸í„° (ë§¤ìš° ì¤‘ìš”\\!) =====\n    # ğŸ¨ í…ìŠ¤íŠ¸ ìƒì„±ì˜ í’ˆì§ˆì„ ê²°ì •í•˜ëŠ” í•µì‹¬ íŒŒë¼ë¯¸í„°ë“¤\n    GENERATION_PARAMS = {\n        \"max_new_tokens\": 400,      # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (1í† í° â‰ˆ 0.75ë‹¨ì–´)\n        \n        # ğŸŒ¡ï¸ Temperature: ì°½ì˜ì„± ì¡°ì ˆ (0.1~2.0)\n        # - ë‚®ì„ìˆ˜ë¡ (0.1): ì•ˆì „í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹µë³€\n        # - ë†’ì„ìˆ˜ë¡ (1.5): ì°½ì˜ì ì´ì§€ë§Œ ë•Œë¡œëŠ” ì´ìƒí•œ ë‹µë³€\n        \"temperature\": 0.8,\n        \n        # ğŸ¯ Sampling ì „ëµë“¤:\n        \"top_p\": 0.9,              # Nucleus sampling - ìƒìœ„ 90% í™•ë¥ ì˜ í† í°ë§Œ ê³ ë ¤\n        \"top_k\": 50,               # ìƒìœ„ 50ê°œ í† í°ë§Œ ê³ ë ¤\n        \"do_sample\": True,         # ìƒ˜í”Œë§ ì‚¬ìš© (Falseë©´ í•­ìƒ ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ì„ íƒ)\n        \n        # ğŸ” ë°˜ë³µ ë°©ì§€\n        \"repetition_penalty\": 1.2,  # ì´ë¯¸ ë‚˜ì˜¨ í† í°ì˜ í™•ë¥ ì„ ë‚®ì¶¤ (1.0 = íŒ¨ë„í‹° ì—†ìŒ)\n        \n        # ğŸ” Beam Search (ë¹„í™œì„±í™”ë¨)\n        # - num_beams > 1ì´ë©´ ì—¬ëŸ¬ ê²½ë¡œë¥¼ ë™ì‹œì— íƒìƒ‰\n        # - í’ˆì§ˆì€ ì¢‹ì•„ì§€ì§€ë§Œ ì†ë„ê°€ ëŠë ¤ì§\n        \"num_beams\": 1,\n    }\n    \n    # CoT ëª¨ë“œì—ì„œëŠ” ë‹¨ê³„ë³„ë¡œ ë‹¤ë¥¸ Temperature ì‚¬ìš©\n    # ğŸ’¡ ì™œ? ê²€ì¦ ë‹¨ê³„ì—ì„œëŠ” ì •í™•ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ì€ ì˜¨ë„ ì‚¬ìš©\n    COT_GENERATION_PARAMS = {\n        \"temperature_initial\": 0.7,     # ì´ˆê¸° ìƒì„± (ì•½ê°„ ì°½ì˜ì )\n        \"temperature_verification\": 0.3, # ê²€ì¦ (ë§¤ìš° ë³´ìˆ˜ì )\n        \"temperature_improvement\": 0.5,  # ê°œì„  (ì¤‘ê°„)\n        \"temperature_final\": 0.3,        # ìµœì¢… í™•ì¸ (ë³´ìˆ˜ì )\n    }\n    \n    # ===== RAG (Retrieval-Augmented Generation) ì„¤ì • =====\n    # ğŸ“š RAGë€? ì™¸ë¶€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì„œ LLMì˜ ë‹µë³€ í’ˆì§ˆì„ ë†’ì´ëŠ” ê¸°ë²•\n    RAG_CONFIG = {\n        \"use_rag\": True,           # RAG ì‚¬ìš© ì—¬ë¶€\n        \"top_k_retrieval\": 3,      # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ë§ì„ìˆ˜ë¡ ì •ë³´ëŠ” ë§ì§€ë§Œ ë…¸ì´ì¦ˆë„ ì¦ê°€)\n        \n        # ì²­í‚¹(Chunking) ì„¤ì • - ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n        # ğŸ’¡ ì™œ ë‚˜ëˆ„ë‚˜? ì „ì²´ ë¬¸ì„œëŠ” ë„ˆë¬´ ì»¤ì„œ í•œ ë²ˆì— ì²˜ë¦¬ ë¶ˆê°€\n        \"chunk_size\": 500,         # ê° ì²­í¬ì˜ í¬ê¸° (ë¬¸ì ìˆ˜)\n        \"chunk_overlap\": 50,       # ì²­í¬ ê°„ ê²¹ì¹¨ (ë¬¸ë§¥ ìœ ì§€ìš©)\n        \n        # ì„ë² ë”© ëª¨ë¸ - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n        # ğŸ’¡ ì‘ì€ ëª¨ë¸ì´ì§€ë§Œ ì„±ëŠ¥ì´ ì¢‹ìŒ (384ì°¨ì› ë²¡í„°)\n        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n        \n        \"use_cache\": True,         # ì¸ë±ìŠ¤ ìºì‹± (ì¬ì‹¤í–‰ ì‹œ ë¹ ë¦„\\!)\n    }\n    \n    # ===== í’ˆì§ˆ ê´€ë¦¬ =====\n    QUALITY_CONFIG = {\n        \"min_answer_length\": 10,   # ìµœì†Œ ë‹µë³€ ê¸¸ì´ (ë„ˆë¬´ ì§§ì€ ë‹µë³€ ë°©ì§€)\n        \"max_retry_attempts\": 3,   # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜\n        \"quality_threshold\": 70,   # í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ (100ì  ë§Œì )\n        \"use_validation\": True,    # í’ˆì§ˆ ê²€ì¦ ì‚¬ìš©\n        \"use_fallback\": True,      # ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ë²• ì‚¬ìš©\n    }\n    \n    # ===== ë¬¸ì œ ìœ í˜• ë¶„í¬ =====\n    # ğŸ’¡ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë¸ì˜ ë²”ìš©ì„± í–¥ìƒ\n    QUESTION_TYPE_DISTRIBUTION = {\n        \"ê°ê´€ì‹\": 0.30,    # 30% - ì„ íƒì§€ì—ì„œ ê³ ë¥´ê¸°\n        \"ì£¼ê´€ì‹\": 0.30,    # 30% - ììœ ë¡­ê²Œ ì„œìˆ \n        \"ë‹¨ë‹µí˜•\": 0.15,    # 15% - ì§§ì€ ë‹µë³€\n        \"ì„œìˆ í˜•\": 0.15,    # 15% - ê¸´ ì„¤ëª…\n        \"ê³„ì‚°í˜•\": 0.05,    # 5% - ìˆ˜ì¹˜ ê³„ì‚°\n        \"ì‚¬ë¡€ë¶„ì„\": 0.05,  # 5% - ì‹¤ì œ ì‚¬ë¡€ ë¶„ì„\n    }\n    \n    # ===== ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì • =====\n    # ğŸš€ ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± ì‹œ ì¤‘ìš”\\!\n    BATCH_CONFIG = {\n        \"batch_size\": 4,          # ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ (ë©”ëª¨ë¦¬ì™€ íŠ¸ë ˆì´ë“œì˜¤í”„)\n        \"target_count\": 100,      # ëª©í‘œ ìƒì„± ê°œìˆ˜\n        \"max_attempts_ratio\": 3,  # ìµœëŒ€ ì‹œë„ = target * ratio\n        \"save_interval\": 20,      # Nê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ (ì•ˆì „ì¥ì¹˜\\!)\n    }\n    \n    # ===== ì‹¤í—˜ ëª¨ë“œ =====\n    EXPERIMENT_MODE = {\n        \"verbose\": True,          # ìƒì„¸ ë¡œê·¸ ì¶œë ¥\n        \"debug\": False,           # ë””ë²„ê·¸ ëª¨ë“œ (ë” ë§ì€ ì •ë³´ ì¶œë ¥)\n        \"dry_run\": False,         # ì‹¤ì œ ì‹¤í–‰ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ\n        \"compare_modes\": False,   # ì—¬ëŸ¬ ëª¨ë“œ ë¹„êµ ì‹¤í–‰\n        \"save_stats\": True,       # í†µê³„ ì €ì¥\n    }\n\n# ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\nconfig = ExperimentConfig()\n\n# ì„¤ì • ìš”ì•½ ì¶œë ¥\nprint(\"ğŸ”¬ ì‹¤í—˜ ì„¤ì • ì™„ë£Œ\\!\")\nprint(f\"  ğŸ“Œ ëª¨ë¸: {config.MODEL_NAME}\")\nprint(f\"  ğŸ“Œ ìƒì„± ëª¨ë“œ: {config.GENERATION_MODE}\")\nprint(f\"  ğŸ“Œ CoT ì‚¬ìš©: {config.COT_CONFIG['use_cot'] or config.GENERATION_MODE == 'cot'}\")\nprint(f\"  ğŸ“Œ CoT í”„ë¦¬ì…‹: {config.CURRENT_COT_PRESET}\")\nprint(f\"  ğŸ“Œ ì–‘ìí™”: {config.USE_QUANTIZATION}\")\nprint(f\"  ğŸ“Œ Temperature: {config.GENERATION_PARAMS['temperature']}\")\nprint(f\"  ğŸ“Œ RAG ì‚¬ìš©: {config.RAG_CONFIG['use_rag']}\")\nprint(f\"  ğŸ“Œ í’ˆì§ˆ ì„ê³„ê°’: {config.QUALITY_CONFIG['quality_threshold']}\")\nprint(\"\\nğŸ’¡ ì´ ì„¤ì •ë“¤ì„ ììœ ë¡­ê²Œ ë³€ê²½í•˜ë©° ì‹¤í—˜í•´ë³´ì„¸ìš”\\!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG ì‹œìŠ¤í…œ (ë¬¸ì„œ ê²€ìƒ‰ ë° ìºì‹±)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# ğŸ“ RAG ì‹œìŠ¤í…œ êµ¬í˜„ - ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•œ AIì˜ í•µì‹¬\n# ========================================\n#\n# ğŸ’¡ RAG(Retrieval-Augmented Generation)ë€?\n#   - LLMì˜ í•œê³„: í•™ìŠµ ë°ì´í„°ì—ë§Œ ì˜ì¡´, ìµœì‹  ì •ë³´ ë¶€ì¡±\n#   - RAGì˜ í•´ê²°ì±…: ì™¸ë¶€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì„œ ë‹µë³€ì— í™œìš©\n#   - ë¹„ìœ : ì‹œí—˜ ë³¼ ë•Œ ì˜¤í”ˆë¶ìœ¼ë¡œ ë³´ëŠ” ê²ƒê³¼ ê°™ìŒ\\!\n\nclass RAGSystem:\n    \"\"\"\n    RAG ì‹œìŠ¤í…œ - ë¬¸ì„œ ê²€ìƒ‰ ë° ìºì‹± ì§€ì›\n    \n    ğŸ” RAGì˜ 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤:\n    1. ë¬¸ì„œ ì¤€ë¹„ (Indexing): PDF/í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n    2. ê²€ìƒ‰ (Retrieval): ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°\n    3. ìƒì„± (Generation): ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ë‹µë³€ ìƒì„±\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig):\n        self.config = config.RAG_CONFIG\n        \n        # ğŸ§  ì„ë² ë”© ëª¨ë¸: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜\n        # ì˜ˆ: \"ê¸ˆìœµ\" â†’ [0.1, -0.3, 0.5, ...] (384ì°¨ì›)\n        self.embedding_model = None\n        \n        # ğŸ—‚ï¸ ë²¡í„° ì¸ë±ìŠ¤: ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ìë£Œêµ¬ì¡°\n        # FAISSëŠ” Facebookì´ ë§Œë“  ì´ˆê³ ì† ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬\n        self.index = None\n        \n        # ğŸ“š ì›ë³¸ ë¬¸ì„œë“¤ ì €ì¥ (ì¸ë±ìŠ¤ëŠ” ë²¡í„°ë§Œ, ì‹¤ì œ í…ìŠ¤íŠ¸ëŠ” ì—¬ê¸°ì—)\n        self.documents = []\n        \n        # ğŸ’¾ ìºì‹œ íŒŒì¼ ê²½ë¡œ (í•œ ë²ˆ ë§Œë“  ì¸ë±ìŠ¤ ì¬ì‚¬ìš©)\n        self.index_path = VECTORDB_DIR / \"index.pkl\"\n        \n    def initialize(self):\n        \"\"\"ì´ˆê¸°í™” - ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ì¸ë±ìŠ¤ ì¤€ë¹„\"\"\"\n        print(\"ğŸ” RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n        \n        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n        # ğŸ’¡ SentenceTransformer: ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ í‘œí˜„\n        # all-MiniLM-L6-v2ëŠ” ì‘ì§€ë§Œ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ (50MB)\n        self.embedding_model = SentenceTransformer(self.config['embedding_model'])\n        \n        # 2. ìºì‹œ í™•ì¸ - ì´ë¯¸ ë§Œë“  ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©\n        if self.config['use_cache'] and self.index_path.exists():\n            # ğŸš€ ìºì‹±ì˜ íš¨ê³¼: 46ì´ˆ â†’ 0.02ì´ˆ (2,300ë°° ë¹¨ë¼ì§\\!)\n            self.load_index()\n        else:\n            # ì²˜ìŒ ì‹¤í–‰ì´ë©´ ì¸ë±ìŠ¤ êµ¬ì¶•\n            self.build_index()\n    \n    def build_index(self):\n        \"\"\"ì¸ë±ìŠ¤ êµ¬ì¶• - PDF ë¬¸ì„œë¥¼ ë²¡í„° DBë¡œ ë³€í™˜\"\"\"\n        print(\"ğŸ“š ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...\")\n        \n        # 1. PDF ë¬¸ì„œ ë¡œë“œ\n        documents = self.load_documents()\n        \n        if not documents:\n            print(\"âš ï¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data/external/ í´ë”ì— PDFë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\")\n            return\n        \n        # 2. ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \n        # ğŸ’¡ ì™œ ë¶„í• ? LLMì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë•Œë¬¸\n        # ì²­í¬: ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆˆ ê²ƒ\n        chunks = self.split_documents(documents)\n        print(f\"  ğŸ“„ {len(chunks)}ê°œ ì²­í¬ ìƒì„±\")\n        \n        # 3. ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì„ë² ë”©)\n        # ğŸ¯ ì´ ê³¼ì •ì´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ (GPU ìˆìœ¼ë©´ ë¹ ë¦„)\n        print(\"  ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘... (ì²« ì‹¤í–‰ ì‹œ 1-2ë¶„ ì†Œìš”)\")\n        embeddings = self.embedding_model.encode(\n            chunks,\n            show_progress_bar=True,  # ì§„í–‰ ìƒí™© í‘œì‹œ\n            batch_size=32  # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ\n        )\n        \n        # 4. FAISS ì¸ë±ìŠ¤ ìƒì„±\n        # ğŸ’¡ FAISS: ìˆ˜ë°±ë§Œ ê°œ ë²¡í„°ë„ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ê²€ìƒ‰ ê°€ëŠ¥\\!\n        dimension = embeddings.shape[1]  # ë²¡í„° ì°¨ì› (ë³´í†µ 384)\n        \n        # IndexFlatL2: L2 ê±°ë¦¬(ìœ í´ë¦¬ë“œ ê±°ë¦¬) ê¸°ë°˜ ê²€ìƒ‰\n        # ê°€ì¥ ì •í™•í•˜ì§€ë§Œ ëŒ€ìš©ëŸ‰ì—ì„œëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ\n        self.index = faiss.IndexFlatL2(dimension)\n        \n        # ë²¡í„° ì¶”ê°€\n        self.index.add(embeddings.astype('float32'))\n        \n        # ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥ (ì¸ë±ìŠ¤ëŠ” ë²¡í„°ë§Œ ì €ì¥í•˜ë¯€ë¡œ)\n        self.documents = chunks\n        \n        # 5. ìºì‹œ ì €ì¥\n        if self.config['use_cache']:\n            self.save_index()\n            \n        print(f\"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ\\! ({len(chunks)}ê°œ ë¬¸ì„œ)\")\n        \n    def load_documents(self) -> List[str]:\n        \"\"\"PDF ë¬¸ì„œ ë¡œë“œ\"\"\"\n        documents = []\n        pdf_files = list(EXTERNAL_DIR.glob(\"*.pdf\"))\n        \n        if not pdf_files:\n            # PDFê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¼ë„ ì‚¬ìš©\n            return self._get_sample_documents()\n        \n        for pdf_path in pdf_files:\n            try:\n                # PyPDF2ë¡œ PDF ì½ê¸°\n                with open(pdf_path, 'rb') as file:\n                    pdf_reader = PyPDF2.PdfReader(file)\n                    text = \"\"\n                    \n                    # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n                    for page in pdf_reader.pages:\n                        text += page.extract_text() + \"\\n\"\n                    \n                    documents.append(text)\n                    print(f\"  âœ… {pdf_path.name} ë¡œë“œ ì™„ë£Œ\")\n                    \n            except Exception as e:\n                print(f\"  âŒ {pdf_path.name} ë¡œë“œ ì‹¤íŒ¨: {e}\")\n                \n        return documents\n    \n    def split_documents(self, documents: List[str]) -> List[str]:\n        \"\"\"\n        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \n        \n        ğŸ’¡ ì²­í‚¹ ì „ëµì´ RAG ì„±ëŠ¥ì— í° ì˜í–¥\\!\n        - ë„ˆë¬´ ì‘ìœ¼ë©´: ë¬¸ë§¥ ì •ë³´ ë¶€ì¡±\n        - ë„ˆë¬´ í¬ë©´: ë…¸ì´ì¦ˆ ì¦ê°€, ì •í™•ë„ í•˜ë½\n        \"\"\"\n        # RecursiveCharacterTextSplitter: ë¬¸ì¥ â†’ ë‹¨ë½ â†’ í˜ì´ì§€ ìˆœìœ¼ë¡œ ë¶„í• \n        # ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í•  ë°©ë²•\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.config['chunk_size'],      # ê° ì²­í¬ í¬ê¸°\n            chunk_overlap=self.config['chunk_overlap'], # ì²­í¬ ê°„ ê²¹ì¹¨\n            length_function=len,  # ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜\n            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # ë¶„í•  ìš°ì„ ìˆœìœ„\n        )\n        \n        chunks = []\n        for doc in documents:\n            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \n            doc_chunks = text_splitter.split_text(doc)\n            chunks.extend(doc_chunks)\n            \n        return chunks\n    \n    def search(self, query: str, top_k: int = None) -> List[Dict]:\n        \"\"\"\n        ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n        \n        ğŸ” ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ê³¼ì •:\n        1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜\n        2. ëª¨ë“  ë¬¸ì„œ ë²¡í„°ì™€ ê±°ë¦¬ ê³„ì‚°\n        3. ê°€ì¥ ê°€ê¹Œìš´ kê°œ ì„ íƒ\n        \"\"\"\n        if self.index is None or not self.documents:\n            print(\"âš ï¸ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € initialize()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n            return []\n        \n        top_k = top_k or self.config['top_k_retrieval']\n        \n        # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜\n        # ğŸ’¡ ì§ˆë¬¸ê³¼ ë¬¸ì„œë¥¼ ê°™ì€ ë²¡í„° ê³µê°„ì— ë§¤í•‘\n        query_embedding = self.embedding_model.encode([query])\n        \n        # 2. ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n        # D: ê±°ë¦¬(ì‘ì„ìˆ˜ë¡ ìœ ì‚¬), I: ì¸ë±ìŠ¤\n        distances, indices = self.index.search(\n            query_embedding.astype('float32'), \n            top_k\n        )\n        \n        # 3. ê²°ê³¼ ì •ë¦¬\n        results = []\n        for idx, distance in zip(indices[0], distances[0]):\n            if idx < len(self.documents):  # ë²”ìœ„ ì²´í¬\n                results.append({\n                    'text': self.documents[idx],\n                    'distance': float(distance),  # L2 ê±°ë¦¬\n                    'similarity': 1 / (1 + float(distance))  # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜\n                })\n        \n        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n        results.sort(key=lambda x: x['similarity'], reverse=True)\n        \n        return results\n    \n    def save_index(self):\n        \"\"\"ì¸ë±ìŠ¤ ìºì‹œ ì €ì¥\"\"\"\n        print(\"ğŸ’¾ ì¸ë±ìŠ¤ ìºì‹œ ì €ì¥ ì¤‘...\")\n        \n        # pickleë¡œ ì €ì¥ (Python ê°ì²´ ì§ë ¬í™”)\n        cache_data = {\n            'index': faiss.serialize_index(self.index),  # FAISS ì¸ë±ìŠ¤\n            'documents': self.documents,  # ì›ë³¸ í…ìŠ¤íŠ¸\n            'timestamp': datetime.now().isoformat()  # ìƒì„± ì‹œê°„\n        }\n        \n        with open(self.index_path, 'wb') as f:\n            pickle.dump(cache_data, f)\n            \n        print(f\"âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ: {self.index_path}\")\n        \n    def load_index(self):\n        \"\"\"ì¸ë±ìŠ¤ ìºì‹œ ë¡œë“œ\"\"\"\n        print(\"ğŸ“‚ ìºì‹œëœ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\")\n        \n        with open(self.index_path, 'rb') as f:\n            cache_data = pickle.load(f)\n            \n        # FAISS ì¸ë±ìŠ¤ ë³µì›\n        self.index = faiss.deserialize_index(cache_data['index'])\n        self.documents = cache_data['documents']\n        \n        print(f\"âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ\\! ({len(self.documents)}ê°œ ë¬¸ì„œ)\")\n        print(f\"  ìƒì„± ì‹œê°„: {cache_data['timestamp']}\")\n        \n    def _get_sample_documents(self) -> List[str]:\n        \"\"\"PDFê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•  ìƒ˜í”Œ ë¬¸ì„œ\"\"\"\n        # ğŸ’¡ ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë°˜ë“œì‹œ ì‹¤ì œ ë¬¸ì„œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\\!\n        return [\n            \"\"\"ë°”ì ¤III ê·œì œëŠ” 2008ë…„ ê¸ˆìœµìœ„ê¸° ì´í›„ ë„ì…ëœ êµ­ì œ ì€í–‰ ê·œì œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n            ì£¼ìš” ë‚´ìš©ìœ¼ë¡œëŠ” ìë³¸ ì ì •ì„± ê°•í™”, ë ˆë²„ë¦¬ì§€ ë¹„ìœ¨ ë„ì…, ìœ ë™ì„± ê·œì œ ì‹ ì„¤ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n            ë³´í†µì£¼ìë³¸ë¹„ìœ¨ì€ 4.5%, Tier1 ìë³¸ë¹„ìœ¨ì€ 6%, ì´ìë³¸ë¹„ìœ¨ì€ 8% ì´ìƒì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\",\n            \n            \"\"\"ê¸ˆìœµë³´ì•ˆì›(FSI)ì€ êµ­ë‚´ ê¸ˆìœµ IT ë³´ì•ˆì„ ì´ê´„í•˜ëŠ” ì „ë¬¸ê¸°ê´€ì…ë‹ˆë‹¤.\n            ì£¼ìš” ì—…ë¬´ë¡œëŠ” ê¸ˆìœµê¶Œ ì‚¬ì´ë²„ ë³´ì•ˆ ê°•í™”, ì „ìê¸ˆìœµê±°ë˜ ì•ˆì „ì„± í™•ë³´,\n            ê¸ˆìœµíšŒì‚¬ ë³´ì•ˆ ìˆ˜ì¤€ í‰ê°€ ë° ì ê²€ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\"\"\",\n            \n            \"\"\"íŒŒìƒìƒí’ˆì€ ê¸°ì´ˆìì‚°ì˜ ê°€ê²© ë³€ë™ì— ë”°ë¼ ê°€ì¹˜ê°€ ê²°ì •ë˜ëŠ” ê¸ˆìœµìƒí’ˆì…ë‹ˆë‹¤.\n            ì„ ë¬¼(Futures), ì˜µì…˜(Options), ìŠ¤ì™‘(Swaps) ë“±ì´ ëŒ€í‘œì ì´ë©°,\n            ìœ„í—˜ í—¤ì§€(Hedging)ì™€ íˆ¬ê¸°(Speculation) ëª©ì ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤.\"\"\"\n        ]\n\nprint(\"âœ… RAG ì‹œìŠ¤í…œ ì½”ë“œ ì •ì˜ ì™„ë£Œ\\!\")\nprint(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\nprint(\"  rag = RAGSystem(config)\")\nprint(\"  rag.initialize()  # ì¸ë±ìŠ¤ êµ¬ì¶• ë˜ëŠ” ë¡œë“œ\")\nprint(\"  results = rag.search('ë°”ì ¤III ìë³¸ë¹„ìœ¨')  # ê²€ìƒ‰\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í†µí•©í˜•/ë¶„ë¦¬í˜•)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPromptTemplates:\n",
    "    \"\"\"\n",
    "    ë‹µë³€ ìƒì„±ì´ ë³´ì¥ëœ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "    í†µí•©í˜•ê³¼ ë¶„ë¦¬í˜• ëª¨ë‘ ì§€ì›\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, style: str = \"detailed\"):\n",
    "        self.style = style  # simple, detailed, expert\n",
    "        self.templates = self._init_templates()\n",
    "    \n",
    "    def _init_templates(self) -> Dict:\n",
    "        \"\"\"ìŠ¤íƒ€ì¼ë³„ í…œí”Œë¦¿ ì´ˆê¸°í™”\"\"\"\n",
    "        \n",
    "        if self.style == \"simple\":\n",
    "            # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆ ë‚®ìŒ)\n",
    "            return self._get_simple_templates()\n",
    "        elif self.style == \"expert\":\n",
    "            # ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ (ëŠë¦¬ì§€ë§Œ í’ˆì§ˆ ë†’ìŒ)\n",
    "            return self._get_expert_templates()\n",
    "        else:\n",
    "            # ê¸°ë³¸: ìƒì„¸ í”„ë¡¬í”„íŠ¸ (ê· í˜•)\n",
    "            return self._get_detailed_templates()\n",
    "    \n",
    "    def _get_detailed_templates(self) -> Dict:\n",
    "        \"\"\"ìƒì„¸ í…œí”Œë¦¿ (ê¸°ë³¸)\"\"\"\n",
    "        return {\n",
    "            # ===== í†µí•©í˜• í…œí”Œë¦¿ (ì§ˆë¬¸+ë‹µë³€ ë™ì‹œ) =====\n",
    "            \"integrated\": {\n",
    "                \"ê°ê´€ì‹\": \"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆì› FSKU ì¶œì œìœ„ì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ FSKU ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "1. ë¬¸ì œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•¨\n",
    "2. 4ê°œì˜ ì„ íƒì§€ë¥¼ ì œì‹œ\n",
    "3. ì •ë‹µì€ ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì¡´ì¬\n",
    "4. ì˜¤ë‹µë„ ê·¸ëŸ´ë“¯í•´ì•¼ í•¨\n",
    "\n",
    "í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”):\n",
    "ë¬¸ì œ: [êµ¬ì²´ì ì¸ ì§ˆë¬¸]\n",
    "â‘  [ì„ íƒì§€1]\n",
    "â‘¡ [ì„ íƒì§€2]\n",
    "â‘¢ [ì„ íƒì§€3]\n",
    "â‘£ [ì„ íƒì§€4]\n",
    "ì •ë‹µ: [ì •ë‹µ ë²ˆí˜¸ì™€ ë‚´ìš©]\n",
    "í•´ì„¤: [ì •ë‹µ ì„ íƒ ì´ìœ ]\"\"\",\n",
    "\n",
    "                \"ì£¼ê´€ì‹\": \"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆì› FSKU ì¶œì œìœ„ì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ FSKU ì£¼ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "1. 2-3ë¬¸ì¥ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ\n",
    "2. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì´ ê°€ëŠ¥í•´ì•¼ í•¨\n",
    "3. í•µì‹¬ ê°œë…ì„ ë¬»ëŠ” ë¬¸ì œì—¬ì•¼ í•¨\n",
    "\n",
    "í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”):\n",
    "ë¬¸ì œ: [êµ¬ì²´ì ì¸ ì§ˆë¬¸]\n",
    "ì •ë‹µ: [2-3ë¬¸ì¥ì˜ ì™„ì „í•œ ë‹µë³€]\n",
    "í•µì‹¬ í‚¤ì›Œë“œ: [ì¤‘ìš” í‚¤ì›Œë“œ 3-5ê°œ]\n",
    "ì±„ì  ê¸°ì¤€: [í‰ê°€ ê¸°ì¤€]\"\"\",\n",
    "\n",
    "                \"ë‹¨ë‹µí˜•\": \"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆì› FSKU ì¶œì œìœ„ì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ FSKU ë‹¨ë‹µí˜• ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "1. í•œ ë‹¨ì–´ë‚˜ ì§§ì€ êµ¬ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ\n",
    "2. ëª…í™•í•œ ì •ë‹µì´ ì¡´ì¬í•´ì•¼ í•¨\n",
    "3. ìš©ì–´, ìˆ˜ì¹˜, ê°œë…ëª…ì„ ë¬»ëŠ” ë¬¸ì œ\n",
    "\n",
    "í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”):\n",
    "ë¬¸ì œ: [êµ¬ì²´ì ì¸ ì§ˆë¬¸]\n",
    "ì •ë‹µ: [ë‹¨ë‹µí˜• ì •ë‹µ]\n",
    "í—ˆìš© ë‹µì•ˆ: [ìœ ì‚¬ ì •ë‹µë“¤]\"\"\",\n",
    "\n",
    "                \"ì„œìˆ í˜•\": \"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆì› FSKU ì¶œì œìœ„ì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ FSKU ì„œìˆ í˜• ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "1. ìƒì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•œ ë¬¸ì œ\n",
    "2. ë…¼ë¦¬ì  ì „ê°œê°€ ì¤‘ìš”í•œ ë‹µë³€\n",
    "3. 5ë¬¸ì¥ ì´ìƒì˜ ë‹µë³€ í•„ìš”\n",
    "\n",
    "í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”):\n",
    "ë¬¸ì œ: [êµ¬ì²´ì ì¸ ì§ˆë¬¸]\n",
    "ëª¨ë²” ë‹µì•ˆ: [5ë¬¸ì¥ ì´ìƒì˜ ìƒì„¸í•œ ë‹µë³€]\n",
    "í•µì‹¬ í‰ê°€ ìš”ì†Œ: [í‰ê°€í•  ìš”ì†Œë“¤]\"\"\",\n",
    "            },\n",
    "            \n",
    "            # ===== ë¶„ë¦¬í˜• í…œí”Œë¦¿ - ì§ˆë¬¸ ìƒì„±ìš© =====\n",
    "            \"question\": {\n",
    "                \"ê°ê´€ì‹\": \"\"\"ê¸ˆìœµë³´ì•ˆì› FSKU ì‹œí—˜ì„ ìœ„í•œ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì—ì„œ ì¤‘ìš”í•œ ê°œë…ì„ ì„ íƒí•˜ì—¬ 4ì§€ì„ ë‹¤ ë¬¸ì œë¥¼ ë§Œë“œì„¸ìš”.\n",
    "ë¬¸ì œì™€ ì„ íƒì§€ë§Œ ìƒì„±í•˜ê³ , ì •ë‹µì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "í˜•ì‹:\n",
    "ë¬¸ì œ: [ì§ˆë¬¸]\n",
    "â‘  [ì„ íƒì§€1]\n",
    "â‘¡ [ì„ íƒì§€2]\n",
    "â‘¢ [ì„ íƒì§€3]\n",
    "â‘£ [ì„ íƒì§€4]\"\"\",\n",
    "\n",
    "                \"ì£¼ê´€ì‹\": \"\"\"ê¸ˆìœµë³´ì•ˆì› FSKU ì‹œí—˜ì„ ìœ„í•œ ì£¼ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì—ì„œ ì¤‘ìš”í•œ ê°œë…ì„ ì„¤ëª…í•˜ë„ë¡ ìš”êµ¬í•˜ëŠ” ë¬¸ì œë¥¼ ë§Œë“œì„¸ìš”.\n",
    "ë¬¸ì œë§Œ ìƒì„±í•˜ê³ , ë‹µë³€ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "í˜•ì‹:\n",
    "ë¬¸ì œ: [ì„œìˆ í˜• ì§ˆë¬¸]\"\"\",\n",
    "            },\n",
    "            \n",
    "            # ===== ë¶„ë¦¬í˜• í…œí”Œë¦¿ - ë‹µë³€ ìƒì„±ìš© =====\n",
    "            \"answer\": {\n",
    "                \"ê°ê´€ì‹\": \"\"\"ë‹¤ìŒ ë¬¸ì œì˜ ì •ë‹µì„ ì„ íƒí•˜ê³  ì„¤ëª…í•˜ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ë¬¸ì œ:\n",
    "{question}\n",
    "\n",
    "ìœ„ ë¬¸ì œì˜ ì •ë‹µì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "\n",
    "í˜•ì‹:\n",
    "ì •ë‹µ: [ì •ë‹µ ë²ˆí˜¸ì™€ ë‚´ìš©]\n",
    "í•´ì„¤: [ì„ íƒ ì´ìœ ì™€ ë‹¤ë¥¸ ì„ íƒì§€ê°€ í‹€ë¦° ì´ìœ ]\"\"\",\n",
    "\n",
    "                \"ì£¼ê´€ì‹\": \"\"\"ë‹¤ìŒ ë¬¸ì œì— ëŒ€í•œ ì™„ì „í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ë¬¸ì œ:\n",
    "{question}\n",
    "\n",
    "ìœ„ ë¬¸ì œì— ëŒ€í•´ 2-3ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "í˜•ì‹:\n",
    "ì •ë‹µ: [ì™„ì „í•œ ë‹µë³€]\n",
    "í•µì‹¬ í‚¤ì›Œë“œ: [ì¤‘ìš” ìš©ì–´ë“¤]\"\"\",\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_simple_templates(self) -> Dict:\n",
    "        \"\"\"ê°„ë‹¨í•œ í…œí”Œë¦¿ (ë¹ ë¦„)\"\"\"\n",
    "        return {\n",
    "            \"integrated\": {\n",
    "                \"ê°ê´€ì‹\": \"\"\"ì°¸ê³ : {context}\n",
    "\n",
    "ê°ê´€ì‹ ë¬¸ì œ ìƒì„±:\n",
    "ë¬¸ì œ:\n",
    "â‘ \n",
    "â‘¡\n",
    "â‘¢\n",
    "â‘£\n",
    "ì •ë‹µ:\"\"\",\n",
    "                \"ì£¼ê´€ì‹\": \"\"\"ì°¸ê³ : {context}\n",
    "\n",
    "ì£¼ê´€ì‹ ë¬¸ì œ ìƒì„±:\n",
    "ë¬¸ì œ:\n",
    "ì •ë‹µ:\"\"\",\n",
    "            },\n",
    "            \"question\": {\n",
    "                \"ê°ê´€ì‹\": \"\"\"ì°¸ê³ : {context}\n",
    "ê°ê´€ì‹ ë¬¸ì œë§Œ:\"\"\",\n",
    "                \"ì£¼ê´€ì‹\": \"\"\"ì°¸ê³ : {context}\n",
    "ì£¼ê´€ì‹ ë¬¸ì œë§Œ:\"\"\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"ê°ê´€ì‹\": \"\"\"ë¬¸ì œ: {question}\n",
    "ì •ë‹µ:\"\"\",\n",
    "                \"ì£¼ê´€ì‹\": \"\"\"ë¬¸ì œ: {question}\n",
    "ì •ë‹µ:\"\"\",\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_expert_templates(self) -> Dict:\n",
    "        \"\"\"ì „ë¬¸ê°€ í…œí”Œë¦¿ (ê³ í’ˆì§ˆ)\"\"\"\n",
    "        return {\n",
    "            \"integrated\": {\n",
    "                \"ê°ê´€ì‹\": \"\"\"[ì „ë¬¸ê°€ ëª¨ë“œ]\n",
    "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ FSKU ì¶œì œìœ„ì›ì¥ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ìµœê³  í’ˆì§ˆì˜ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”:\n",
    "1. Bloom's Taxonomy ìƒìœ„ ìˆ˜ì¤€ (ë¶„ì„, í‰ê°€, ì°½ì¡°)\n",
    "2. ì‹¤ë¬´ ìƒí™© ê¸°ë°˜\n",
    "3. í•¨ì • ì„ íƒì§€ í¬í•¨\n",
    "4. ëª…í™•í•œ ë³€ë³„ë ¥\n",
    "\n",
    "í•„ìˆ˜ í¬í•¨ ìš”ì†Œ:\n",
    "- ë¬¸ì œ ìƒí™© ì„¤ì •\n",
    "- êµ¬ì²´ì  ì¡°ê±´ ì œì‹œ\n",
    "- 4ê°œ ì„ íƒì§€ (ëª¨ë‘ ê·¸ëŸ´ë“¯í•¨)\n",
    "- ì •ë‹µê³¼ ìƒì„¸ í•´ì„¤\n",
    "\n",
    "í˜•ì‹:\n",
    "ë¬¸ì œ: [ìƒí™© ì„¤ì • + êµ¬ì²´ì  ì§ˆë¬¸]\n",
    "â‘  [ì„ íƒì§€1 - ë§¤ë ¥ì ì¸ ì˜¤ë‹µ]\n",
    "â‘¡ [ì„ íƒì§€2 - ë¶€ë¶„ì ìœ¼ë¡œ ë§ëŠ” ì˜¤ë‹µ]\n",
    "â‘¢ [ì„ íƒì§€3 - ì •ë‹µ ë˜ëŠ” ì˜¤ë‹µ]\n",
    "â‘£ [ì„ íƒì§€4 - í˜¼ë™í•˜ê¸° ì‰¬ìš´ ì˜¤ë‹µ]\n",
    "ì •ë‹µ: [ë²ˆí˜¸ì™€ ë‚´ìš©]\n",
    "í•´ì„¤: [ê° ì„ íƒì§€ë³„ ìƒì„¸ ì„¤ëª…]\n",
    "ë‚œì´ë„: [ìƒ/ì¤‘/í•˜]\n",
    "ì¶œì œ ì˜ë„: [í‰ê°€í•˜ê³ ì í•˜ëŠ” ì—­ëŸ‰]\"\"\",\n",
    "                \n",
    "                # ë‹¤ë¥¸ ìœ í˜•ë“¤ë„ ìœ ì‚¬í•˜ê²Œ ì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ...\n",
    "            },\n",
    "            # question, answerë„ ì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ í™•ì¥...\n",
    "        }\n",
    "    \n",
    "    def get_template(self, mode: str, question_type: str, template_type: str = \"integrated\") -> str:\n",
    "        \"\"\"\n",
    "        í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°\n",
    "        \n",
    "        Args:\n",
    "            mode: generation mode (integrated/separated)\n",
    "            question_type: ë¬¸ì œ ìœ í˜•\n",
    "            template_type: integrated/question/answer\n",
    "        \"\"\"\n",
    "        if mode == \"integrated\":\n",
    "            return self.templates[\"integrated\"].get(\n",
    "                question_type, \n",
    "                self.templates[\"integrated\"][\"ì£¼ê´€ì‹\"]\n",
    "            )\n",
    "        else:  # separated\n",
    "            return self.templates[template_type].get(\n",
    "                question_type,\n",
    "                self.templates[template_type][\"ì£¼ê´€ì‹\"]\n",
    "            )\n",
    "\n",
    "print(\"âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"ğŸ“ ìŠ¤íƒ€ì¼ ì˜µì…˜: simple, detailed, expert\")\n",
    "print(\"ğŸ“ ëª¨ë“œ: integrated (í†µí•©í˜•), separated (ë¶„ë¦¬í˜•)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# ğŸ“ ë°ì´í„° ìƒì„±ê¸° - LLMì„ í™œìš©í•œ í•™ìŠµ ë°ì´í„° ìë™ ìƒì„±\n# ========================================\n#\n# ğŸ’¡ ì™œ ë°ì´í„° ìƒì„±ì´ í•„ìš”í•œê°€?\n#   - ë¬¸ì œ: ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (íŠ¹íˆ í•œêµ­ì–´ ê¸ˆìœµ ë¶„ì•¼)\n#   - í•´ê²°: LLMì„ ì‚¬ìš©í•´ ìë™ìœ¼ë¡œ Q&A ìŒ ìƒì„±\n#   - ì£¼ì˜: ìƒì„±ëœ ë°ì´í„°ì˜ í’ˆì§ˆ ê²€ì¦ í•„ìˆ˜\\!\n\nclass AnswerGuaranteedGenerator:\n    \"\"\"\n    ë‹µë³€ ìƒì„±ì´ ë³´ì¥ëœ ë°ì´í„° ìƒì„±ê¸°\n    \n    ğŸ¯ í•µì‹¬ ê¸°ëŠ¥:\n    1. í†µí•©í˜•/ë¶„ë¦¬í˜•/CoT ëª¨ë“œ ì§€ì›\n    2. ë‹µë³€ ì—†ì„ ì‹œ 3ë‹¨ê³„ í´ë°± ì‹œìŠ¤í…œ\n    3. í’ˆì§ˆ ê²€ì¦ í¬í•¨\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n        self.config = config\n        self.generation_mode = config.GENERATION_MODE\n        self.rag = rag_system\n        \n        # ğŸ¤– ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € (ë‚˜ì¤‘ì— ë¡œë“œ)\n        self.model = None\n        self.tokenizer = None\n        self.model_loaded = False\n        \n        # ğŸ“Š í†µê³„ ì¶”ì  (ì„±ëŠ¥ ë¶„ì„ìš©)\n        self.stats = {\n            'total_attempts': 0,      # ì´ ì‹œë„ íšŸìˆ˜\n            'successful': 0,           # ì„±ê³µ íšŸìˆ˜\n            'failed': 0,              # ì‹¤íŒ¨ íšŸìˆ˜\n            'retry_count': 0,         # ì¬ì‹œë„ íšŸìˆ˜\n            'with_answer': 0,         # ë‹µë³€ ìˆëŠ” ê²½ìš°\n            'without_answer': 0,      # ë‹µë³€ ì—†ëŠ” ê²½ìš°\n            'fallback_used': 0,       # í´ë°± ì‚¬ìš© íšŸìˆ˜\n        }\n    \n    def load_model(self):\n        \"\"\"\n        ëª¨ë¸ ë¡œë“œ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ LLM ë¡œë“œ\n        \n        ğŸ’¡ ëª¨ë¸ ë¡œë“œ ì‹œ ê³ ë ¤ì‚¬í•­:\n        1. ë©”ëª¨ë¦¬ ì œí•œ: RTX 4090ì€ 24GB\n        2. ì–‘ìí™” ì‚¬ìš©: 4bitë¡œ ì••ì¶•í•˜ë©´ 7B ëª¨ë¸ë„ ë¡œë“œ ê°€ëŠ¥\n        3. device_map=\"auto\": ìë™ìœ¼ë¡œ GPU/CPU ë¶„ë°°\n        \"\"\"\n        if self.model_loaded:\n            return  # ì´ë¯¸ ë¡œë“œë¨\n        \n        print(f\"ğŸš€ ëª¨ë¸ ë¡œë”©: {self.config.MODEL_NAME}\")\n        print(f\"  ì–‘ìí™”: {self.config.USE_QUANTIZATION}\")\n        \n        try:\n            # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ\n            # ğŸ’¡ í† í¬ë‚˜ì´ì €: í…ìŠ¤íŠ¸ â†” í† í° ë³€í™˜\n            # ì˜ˆ: \"ì•ˆë…•í•˜ì„¸ìš”\" â†’ [1234, 5678, 9012]\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.config.MODEL_NAME,\n                trust_remote_code=True  # ì»¤ìŠ¤í…€ ì½”ë“œ í—ˆìš© (ì¼ë¶€ ëª¨ë¸ í•„ìš”)\n            )\n            \n            # íŒ¨ë”© í† í° ì„¤ì • (ì—†ìœ¼ë©´ EOS í† í° ì‚¬ìš©)\n            # ğŸ’¡ íŒ¨ë”©: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•œ íŠ¹ìˆ˜ í† í°\n            if not self.tokenizer.pad_token:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # 2. ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ)\n            if self.config.USE_QUANTIZATION:\n                # ğŸ”¥ QLoRA ë°©ì‹: 4bit ì–‘ìí™”\n                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 7B ëª¨ë¸ â†’ ì•½ 4GB\n                bnb_config = BitsAndBytesConfig(**self.config.QUANTIZATION_CONFIG)\n                \n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.config.MODEL_NAME,\n                    quantization_config=bnb_config,  # ì–‘ìí™” ì„¤ì •\n                    device_map=\"auto\",  # GPU/CPU ìë™ ë¶„ë°°\n                    trust_remote_code=True\n                )\n            else:\n                # ì¼ë°˜ ë°©ì‹: FP16 (ë°˜ì •ë°€ë„)\n                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 7B ëª¨ë¸ â†’ ì•½ 14GB\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.config.MODEL_NAME,\n                    torch_dtype=torch.float16,  # FP32 â†’ FP16 (ë©”ëª¨ë¦¬ ì ˆë°˜)\n                    device_map=\"auto\",\n                    trust_remote_code=True\n                )\n            \n            self.model_loaded = True\n            print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\\!\")\n            \n            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\n            if torch.cuda.is_available():\n                memory = torch.cuda.memory_allocated() / 1024**3\n                print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory:.2f}GB\")\n                \n        except Exception as e:\n            logger.error(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n            # ğŸ’¡ ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ì›ì¸:\n            # 1. ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ ì–‘ìí™” ì‚¬ìš© ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸\n            # 2. ëª¨ë¸ëª… ì˜¤íƒ€ â†’ MODEL_NAME í™•ì¸\n            # 3. ì¸í„°ë„· ì—°ê²° â†’ ì²« ë‹¤ìš´ë¡œë“œ ì‹œ í•„ìš”\n            raise\n    \n    def generate_text(self, prompt: str, max_tokens: int = None) -> str:\n        \"\"\"\n        í…ìŠ¤íŠ¸ ìƒì„± - LLMì˜ í•µì‹¬ ê¸°ëŠ¥\n        \n        ğŸ¨ ìƒì„± ê³¼ì •:\n        1. í”„ë¡¬í”„íŠ¸ â†’ í† í°í™”\n        2. ëª¨ë¸ ì¶”ë¡  (Forward pass)\n        3. í† í° ìƒ˜í”Œë§ (Temperature, Top-p ë“± ì ìš©)\n        4. í† í° â†’ í…ìŠ¤íŠ¸ ë³€í™˜\n        \"\"\"\n        if not self.model_loaded:\n            self.load_model()\n        \n        max_tokens = max_tokens or self.config.GENERATION_PARAMS['max_new_tokens']\n        \n        # 1. í† í°í™”\n        # ğŸ’¡ return_tensors=\"pt\": PyTorch í…ì„œë¡œ ë°˜í™˜\n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°\n            max_length=2000   # ìµœëŒ€ ì…ë ¥ ê¸¸ì´\n        )\n        \n        # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n        \n        # 2. ìƒì„± (ì¶”ë¡ )\n        # ğŸ’¡ torch.no_grad(): ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,  # input_ids, attention_mask ë“±\n                max_new_tokens=max_tokens,\n                \n                # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë“¤ (í’ˆì§ˆ ê²°ì •\\!)\n                temperature=self.config.GENERATION_PARAMS['temperature'],\n                top_p=self.config.GENERATION_PARAMS['top_p'],\n                top_k=self.config.GENERATION_PARAMS['top_k'],\n                do_sample=self.config.GENERATION_PARAMS['do_sample'],\n                repetition_penalty=self.config.GENERATION_PARAMS['repetition_penalty'],\n                \n                # íŠ¹ìˆ˜ í† í° ID\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n            )\n        \n        # 3. ë””ì½”ë”© (í† í° â†’ í…ìŠ¤íŠ¸)\n        # ì…ë ¥ ë¶€ë¶„ ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n        generated = self.tokenizer.decode(\n            outputs[0][inputs['input_ids'].shape[1]:],  # ì…ë ¥ ê¸¸ì´ ì´í›„ë¶€í„°\n            skip_special_tokens=True  # <pad>, <eos> ë“± ì œê±°\n        )\n        \n        return generated.strip()\n    \n    def generate_qa_pair(self, context: str, question_type: str = \"ì£¼ê´€ì‹\") -> Optional[Dict]:\n        \"\"\"\n        QA ìŒ ìƒì„± - ë©”ì¸ í•¨ìˆ˜\n        \n        ğŸ”„ ìƒì„± ëª¨ë“œë³„ ì°¨ì´:\n        1. integrated: í•œ ë²ˆì— Q&A ìƒì„± (ë¹ ë¦„)\n        2. separated: Q ìƒì„± â†’ RAG ê²€ìƒ‰ â†’ A ìƒì„± (ì •í™•)\n        3. cot: 4ë‹¨ê³„ ê²€ì¦ ê³¼ì • (ìµœê³  í’ˆì§ˆ)\n        \"\"\"\n        self.stats['total_attempts'] += 1\n        \n        try:\n            if self.generation_mode == \"integrated\":\n                # í†µí•©í˜•: í”„ë¡¬í”„íŠ¸ í•˜ë‚˜ë¡œ Q&A ë™ì‹œ ìƒì„±\n                return self._generate_integrated(context, question_type)\n                \n            elif self.generation_mode == \"separated\":\n                # ë¶„ë¦¬í˜•: ì§ˆë¬¸ ë¨¼ì €, ë‹µë³€ì€ ë”°ë¡œ\n                return self._generate_separated(context, question_type)\n                \n            elif self.generation_mode == \"cot\":\n                # CoT: Chain-of-Thoughtë¡œ ë‹¨ê³„ë³„ ê²€ì¦\n                # ë³„ë„ í´ë˜ìŠ¤ì—ì„œ ì²˜ë¦¬ (ChainOfThoughtGenerator)\n                pass\n                \n        except Exception as e:\n            logger.error(f\"QA ìƒì„± ì˜¤ë¥˜: {e}\")\n            self.stats['failed'] += 1\n            return None\n    \n    def _generate_integrated(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        í†µí•©í˜• ìƒì„± - í•œ ë²ˆì— Q&A ìƒì„±\n        \n        ì¥ì : ë¹ ë¦„, ì¼ê´€ì„± ìˆìŒ\n        ë‹¨ì : ë‹µë³€ í’ˆì§ˆì´ ë‚®ì„ ìˆ˜ ìˆìŒ\n        \"\"\"\n        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n        # ğŸ’¡ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ í’ˆì§ˆì˜ 80%ë¥¼ ê²°ì •\\!\n        prompt = f\"\"\"ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµê°ë…ì›ì˜ FSKU ì‹œí—˜ ì¶œì œìœ„ì›ì…ë‹ˆë‹¤.\në‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ {question_type} ë¬¸ì œì™€ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n\n### ì°¸ê³  ë¬¸ì„œ:\n{context[:1000]}  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ì‚¬ìš©\n\n### ìƒì„± ì§€ì¹¨:\n1. ë¬¸ì œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ\n2. ë‹µë³€ì€ ì™„ì „í•˜ê³  ì •í™•í•˜ê²Œ\n3. ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ë¥¼ ì ì ˆíˆ ì‚¬ìš©\n4. ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ ë‚´ìš© ìœ„ì£¼ë¡œ\n\n### í˜•ì‹:\në¬¸ì œ: [ì—¬ê¸°ì— ì§ˆë¬¸ ì‘ì„±]\nì •ë‹µ: [ì—¬ê¸°ì— ë‹µë³€ ì‘ì„±]\n\n### ìƒì„±:\"\"\"\n        \n        # í…ìŠ¤íŠ¸ ìƒì„±\n        generated = self.generate_text(prompt)\n        \n        # íŒŒì‹± (ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ Q&A ì¶”ì¶œ)\n        result = self._parse_qa(generated)\n        \n        if result:\n            self.stats['successful'] += 1\n            if result.get('answer'):\n                self.stats['with_answer'] += 1\n            else:\n                self.stats['without_answer'] += 1\n                \n        return result\n    \n    def _generate_separated(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        ë¶„ë¦¬í˜• ìƒì„± - ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë”°ë¡œ ìƒì„±\n        \n        ğŸ” ê°œì„ ëœ í”„ë¡œì„¸ìŠ¤:\n        1. ì»¨í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸ ìƒì„±\n        2. ìƒì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ RAG ì¬ê²€ìƒ‰ â† í•µì‹¬\\!\n        3. ì›ë³¸ + ê²€ìƒ‰ ê²°ê³¼ë¡œ ë‹µë³€ ìƒì„±\n        \"\"\"\n        # 1ë‹¨ê³„: ì§ˆë¬¸ ìƒì„±\n        question_prompt = f\"\"\"ë¬¸ì„œë¥¼ ì½ê³  {question_type} ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ìƒì„±í•˜ì„¸ìš”.\n\në¬¸ì„œ: {context[:500]}\n\nì§ˆë¬¸:\"\"\"\n        \n        question = self.generate_text(question_prompt, max_tokens=100)\n        \n        if not question:\n            return None\n        \n        # 2ë‹¨ê³„: RAG ê²€ìƒ‰ (ìƒì„±ëœ ì§ˆë¬¸ ê¸°ë°˜)\n        # ğŸ’¡ ì´ê²ƒì´ ë¶„ë¦¬í˜•ì˜ í•µì‹¬ ê°œì„ \\!\n        enhanced_context = context\n        if self.rag:\n            # ì§ˆë¬¸ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n            retrieved = self.rag.search(question, top_k=3)\n            if retrieved:\n                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€\n                rag_context = \"\\n\".join([r['text'][:200] for r in retrieved])\n                enhanced_context = f\"{context}\\n\\nê´€ë ¨ ì •ë³´:\\n{rag_context}\"\n        \n        # 3ë‹¨ê³„: ë‹µë³€ ìƒì„±\n        answer_prompt = f\"\"\"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n\nì°¸ê³  ìë£Œ:\n{enhanced_context[:800]}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:\"\"\"\n        \n        answer = self.generate_text(answer_prompt, max_tokens=300)\n        \n        return {\n            'question': question.strip(),\n            'answer': answer.strip(),\n            'context': context[:500],\n            'question_type': question_type,\n            'generation_mode': 'separated',\n            'rag_used': self.rag is not None\n        }\n    \n    def _parse_qa(self, text: str) -> Optional[Dict]:\n        \"\"\"\n        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ Q&A ì¶”ì¶œ\n        \n        ğŸ’¡ íŒŒì‹±ì€ ì˜ì™¸ë¡œ ê¹Œë‹¤ë¡œì›€\\!\n        LLMì´ í•­ìƒ ì •í™•í•œ í˜•ì‹ìœ¼ë¡œ ìƒì„±í•˜ì§€ ì•Šê¸° ë•Œë¬¸\n        \"\"\"\n        result = {}\n        \n        # ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬\n        # LLMë§ˆë‹¤ ì„ í˜¸í•˜ëŠ” í˜•ì‹ì´ ë‹¤ë¦„\n        question_markers = ['ë¬¸ì œ:', 'ì§ˆë¬¸:', 'Q:', 'Question:']\n        answer_markers = ['ì •ë‹µ:', 'ë‹µë³€:', 'ë‹µ:', 'A:', 'Answer:']\n        \n        # ì§ˆë¬¸ ì¶”ì¶œ\n        for marker in question_markers:\n            if marker in text:\n                parts = text.split(marker, 1)[1]\n                # ë‹µë³€ ë§ˆì»¤ê¹Œì§€ë§Œ ì¶”ì¶œ\n                for ans_marker in answer_markers:\n                    if ans_marker in parts:\n                        result['question'] = parts.split(ans_marker)[0].strip()\n                        break\n                break\n        \n        # ë‹µë³€ ì¶”ì¶œ\n        for marker in answer_markers:\n            if marker in text:\n                answer_part = text.split(marker, 1)[1]\n                # ë‹¤ìŒ ì„¹ì…˜ì´ë‚˜ ì¤„ë°”ê¿ˆê¹Œì§€\n                result['answer'] = answer_part.split('\\n\\n')[0].strip()\n                break\n        \n        # ë‘˜ ë‹¤ ìˆì–´ì•¼ ìœ íš¨\n        if 'question' in result and 'answer' in result:\n            return result\n        \n        return None\n    \n    def get_stats(self) -> Dict:\n        \"\"\"í†µê³„ ë°˜í™˜ - ì„±ëŠ¥ ë¶„ì„ìš©\"\"\"\n        total = self.stats['total_attempts']\n        if total == 0:\n            return self.stats\n        \n        # ì„±ê³µë¥  ê³„ì‚°\n        success_rate = self.stats['successful'] / total * 100\n        answer_rate = self.stats['with_answer'] / max(self.stats['successful'], 1) * 100\n        \n        return {\n            **self.stats,\n            'success_rate': f\"{success_rate:.1f}%\",\n            'answer_rate': f\"{answer_rate:.1f}%\",\n            'fallback_rate': f\"{self.stats['fallback_used'] / total * 100:.1f}%\"\n        }\n\nprint(\"âœ… ë°ì´í„° ìƒì„±ê¸° ì •ì˜ ì™„ë£Œ\\!\")\nprint(\"ğŸ’¡ ê° ëª¨ë“œì˜ ì‚¬ìš© ì‹œì :\")\nprint(\"  - integrated: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘\")\nprint(\"  - separated: ì •í™•í•œ ë‹µë³€ì´ í•„ìš”í•  ë•Œ\")\nprint(\"  - cot: ìµœê³  í’ˆì§ˆì´ í•„ìš”í•  ë•Œ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGuaranteedGenerator:\n",
    "    \"\"\"\n",
    "    ë‹µë³€ ìƒì„±ì´ ë³´ì¥ëœ ë°ì´í„° ìƒì„±ê¸°\n",
    "    - í†µí•©í˜•ê³¼ ë¶„ë¦¬í˜• ëª¨ë‘ ì§€ì›\n",
    "    - ë‹µë³€ ì—†ì„ ì‹œ 3ë‹¨ê³„ í´ë°± ì‹œìŠ¤í…œ\n",
    "    - í’ˆì§ˆ ê²€ì¦ í¬í•¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            config: ì‹¤í—˜ ì„¤ì •\n",
    "            rag_system: RAG ì‹œìŠ¤í…œ (ì„ íƒì )\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.generation_mode = config.GENERATION_MODE\n",
    "        self.rag = rag_system\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "        self.prompts = EnhancedPromptTemplates(config.PROMPT_STYLE)\n",
    "        \n",
    "        # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.model_loaded = False\n",
    "        \n",
    "        # í†µê³„\n",
    "        self.stats = {\n",
    "            'total_attempts': 0,\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'retry_count': 0,\n",
    "            'with_answer': 0,\n",
    "            'without_answer': 0,\n",
    "            'fallback_used': 0,\n",
    "            'mode_stats': {'integrated': 0, 'separated': 0}\n",
    "        }\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        if self.model_loaded:\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸš€ ëª¨ë¸ ë¡œë”©: {self.config.MODEL_NAME}\")\n",
    "        print(f\"  ì–‘ìí™”: {self.config.USE_QUANTIZATION}\")\n",
    "        \n",
    "        try:\n",
    "            # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config.MODEL_NAME,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if not self.tokenizer.pad_token:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # ëª¨ë¸ ë¡œë“œ\n",
    "            if self.config.USE_QUANTIZATION:\n",
    "                bnb_config = BitsAndBytesConfig(**self.config.QUANTIZATION_CONFIG)\n",
    "                \n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.config.MODEL_NAME,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            else:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.config.MODEL_NAME,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            \n",
    "            self.model_loaded = True\n",
    "            print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "            \n",
    "            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n",
    "            if torch.cuda.is_available():\n",
    "                memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {memory:.2f}GB\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_text(self, prompt: str, max_tokens: int = None) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "        if not self.model_loaded:\n",
    "            self.load_model()\n",
    "        \n",
    "        max_tokens = max_tokens or self.config.GENERATION_PARAMS['max_new_tokens']\n",
    "        \n",
    "        # í† í°í™”\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2000\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=self.config.GENERATION_PARAMS['temperature'],\n",
    "                top_p=self.config.GENERATION_PARAMS['top_p'],\n",
    "                top_k=self.config.GENERATION_PARAMS['top_k'],\n",
    "                do_sample=self.config.GENERATION_PARAMS['do_sample'],\n",
    "                repetition_penalty=self.config.GENERATION_PARAMS['repetition_penalty'],\n",
    "                num_beams=self.config.GENERATION_PARAMS['num_beams'],\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        generated = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return generated.strip()\n",
    "    \n",
    "    def generate_integrated(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        í†µí•©í˜• ìƒì„± (ì§ˆë¬¸+ë‹µë³€ ë™ì‹œ)\n",
    "        ë¹ ë¥´ê³  ì¼ê´€ì„± ìˆìŒ\n",
    "        \"\"\"\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(f\"  [í†µí•©í˜•] {question_type} ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        prompt_template = self.prompts.get_template(\n",
    "            \"integrated\", question_type, \"integrated\"\n",
    "        )\n",
    "        prompt = prompt_template.format(context=context[:1000])\n",
    "        \n",
    "        # ì¬ì‹œë„ ë¡œì§\n",
    "        max_retry = self.config.QUALITY_CONFIG['max_retry_attempts']\n",
    "        \n",
    "        for attempt in range(max_retry):\n",
    "            generated = self.generate_text(prompt)\n",
    "            result = self._parse_integrated_result(generated, question_type)\n",
    "            \n",
    "            if result and result.get('answer'):\n",
    "                # ë‹µë³€ ê¸¸ì´ í™•ì¸\n",
    "                if len(result['answer']) >= self.config.QUALITY_CONFIG['min_answer_length']:\n",
    "                    return result\n",
    "            \n",
    "            self.stats['retry_count'] += 1\n",
    "            \n",
    "            if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                print(f\"    âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨, ì¬ì‹œë„ {attempt + 1}/{max_retry}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def generate_separated(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        ë¶„ë¦¬í˜• ìƒì„± (ì§ˆë¬¸ ë¨¼ì €, ë‹µë³€ ë‚˜ì¤‘ì—)\n",
    "        ëŠë¦¬ì§€ë§Œ ë” ì •í™•í•œ ë‹µë³€ ê°€ëŠ¥\n",
    "        \"\"\"\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(f\"  [ë¶„ë¦¬í˜•] {question_type} ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # 1ë‹¨ê³„: ì§ˆë¬¸ ìƒì„±\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(\"    1/2. ì§ˆë¬¸ ìƒì„±...\")\n",
    "        \n",
    "        q_template = self.prompts.get_template(\n",
    "            \"separated\", question_type, \"question\"\n",
    "        )\n",
    "        q_prompt = q_template.format(context=context[:1000])\n",
    "        \n",
    "        question_text = self.generate_text(q_prompt, max_tokens=200)\n",
    "        parsed_question = self._parse_question(question_text, question_type)\n",
    "        \n",
    "        if not parsed_question:\n",
    "            if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                print(\"    âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨\")\n",
    "            return None\n",
    "        \n",
    "        # 2ë‹¨ê³„: ë‹µë³€ ìƒì„±\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(\"    2/2. ë‹µë³€ ìƒì„±...\")\n",
    "        \n",
    "        a_template = self.prompts.get_template(\n",
    "            \"separated\", question_type, \"answer\"\n",
    "        )\n",
    "        a_prompt = a_template.format(\n",
    "            context=context[:1000],\n",
    "            question=parsed_question['question']\n",
    "        )\n",
    "        \n",
    "        answer_text = self.generate_text(a_prompt, max_tokens=300)\n",
    "        parsed_answer = self._parse_answer(answer_text, question_type)\n",
    "        \n",
    "        if not parsed_answer and self.config.QUALITY_CONFIG['use_fallback']:\n",
    "            # ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°±\n",
    "            if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                print(\"    âš ï¸ í´ë°± ë‹µë³€ ìƒì„±...\")\n",
    "            \n",
    "            parsed_answer = self._generate_fallback_answer(\n",
    "                context, parsed_question['question'], question_type\n",
    "            )\n",
    "            self.stats['fallback_used'] += 1\n",
    "        \n",
    "        # ê²°ê³¼ ë³‘í•©\n",
    "        result = {**parsed_question, **parsed_answer}\n",
    "        return result\n",
    "    \n",
    "    def generate_qa_pair(self, context: str, question_type: str = \"ì£¼ê´€ì‹\") -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        QA ìŒ ìƒì„± (ë©”ì¸ ë©”ì„œë“œ)\n",
    "        \n",
    "        Args:\n",
    "            context: ì°¸ê³  ë¬¸ì„œ ë‚´ìš©\n",
    "            question_type: ë¬¸ì œ ìœ í˜•\n",
    "        \n",
    "        Returns:\n",
    "            ìƒì„±ëœ QA ìŒ\n",
    "        \"\"\"\n",
    "        self.stats['total_attempts'] += 1\n",
    "        \n",
    "        try:\n",
    "            # RAG ì‚¬ìš© ì‹œ ì»¨í…ìŠ¤íŠ¸ ë³´ê°•\n",
    "            if self.rag and self.config.RAG_CONFIG['use_rag']:\n",
    "                # ì§ˆë¬¸ ìœ í˜•ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©\n",
    "                query = f\"{question_type} ë¬¸ì œ ìƒì„±ì„ ìœ„í•œ {context[:100]}\"\n",
    "                retrieved = self.rag.search(query)\n",
    "                \n",
    "                if retrieved:\n",
    "                    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶”ê°€\n",
    "                    additional_context = \"\\n\".join([r['text'][:200] for r in retrieved[:2]])\n",
    "                    context = f\"{context}\\n\\nê´€ë ¨ ë¬¸ì„œ:\\n{additional_context}\"\n",
    "            \n",
    "            # ìƒì„± ëª¨ë“œì— ë”°ë¼ ë¶„ê¸°\n",
    "            if self.generation_mode == \"integrated\":\n",
    "                result = self.generate_integrated(context, question_type)\n",
    "                self.stats['mode_stats']['integrated'] += 1\n",
    "            else:\n",
    "                result = self.generate_separated(context, question_type)\n",
    "                self.stats['mode_stats']['separated'] += 1\n",
    "            \n",
    "            if result:\n",
    "                # ë‹µë³€ ì¡´ì¬ í™•ì¸\n",
    "                if result.get('answer'):\n",
    "                    self.stats['with_answer'] += 1\n",
    "                else:\n",
    "                    self.stats['without_answer'] += 1\n",
    "                    \n",
    "                    # ë‹µë³€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë‹µë³€ ìƒì„±\n",
    "                    if self.config.QUALITY_CONFIG['use_fallback']:\n",
    "                        result['answer'] = self._generate_basic_answer(\n",
    "                            context, result.get('question', ''), question_type\n",
    "                        )\n",
    "                \n",
    "                # í’ˆì§ˆ ê²€ì¦\n",
    "                if self.config.QUALITY_CONFIG['use_validation']:\n",
    "                    quality = self._validate_quality(result)\n",
    "                    result['quality_score'] = quality\n",
    "                    \n",
    "                    # í’ˆì§ˆ ì„ê³„ê°’ í™•ì¸\n",
    "                    if quality < self.config.QUALITY_CONFIG['quality_threshold']:\n",
    "                        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                            print(f\"    âš ï¸ í’ˆì§ˆ ë¯¸ë‹¬: {quality}/100\")\n",
    "                        # í’ˆì§ˆ ë¯¸ë‹¬ì´ì–´ë„ ì¼ë‹¨ ë°˜í™˜ (ë‚˜ì¤‘ì— í•„í„°ë§)\n",
    "                \n",
    "                # ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "                result['context'] = context[:500]  # ì»¨í…ìŠ¤íŠ¸ ì¼ë¶€ë§Œ ì €ì¥\n",
    "                result['question_type'] = question_type\n",
    "                result['generation_mode'] = self.generation_mode\n",
    "                result['model'] = self.config.MODEL_NAME\n",
    "                result['timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                self.stats['successful'] += 1\n",
    "                return result\n",
    "            else:\n",
    "                self.stats['failed'] += 1\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "    \n",
    "    def _parse_integrated_result(self, text: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"í†µí•© ê²°ê³¼ íŒŒì‹±\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        # ë¬¸ì œ ì¶”ì¶œ\n",
    "        if 'ë¬¸ì œ:' in text:\n",
    "            parts = text.split('ë¬¸ì œ:', 1)[1]\n",
    "            \n",
    "            # ì •ë‹µ êµ¬ë¶„ì ì°¾ê¸°\n",
    "            if 'ì •ë‹µ:' in parts:\n",
    "                result['question'] = parts.split('ì •ë‹µ:')[0].strip()\n",
    "            elif 'ëª¨ë²” ë‹µì•ˆ:' in parts:\n",
    "                result['question'] = parts.split('ëª¨ë²” ë‹µì•ˆ:')[0].strip()\n",
    "            elif 'â‘ ' in parts:  # ê°ê´€ì‹\n",
    "                result['question'] = parts.split('â‘ ')[0].strip()\n",
    "            else:\n",
    "                result['question'] = parts.split('\\n')[0].strip()\n",
    "        \n",
    "        # ì •ë‹µ ì¶”ì¶œ (ì¤‘ìš”!)\n",
    "        if 'ì •ë‹µ:' in text:\n",
    "            answer_part = text.split('ì •ë‹µ:', 1)[1]\n",
    "            \n",
    "            # ë‹¤ìŒ ì„¹ì…˜ê¹Œì§€ ì¶”ì¶œ\n",
    "            for delimiter in ['í•´ì„¤:', 'í•µì‹¬', 'ì±„ì ', '\\n\\n', '\\në¬¸ì œ:']:\n",
    "                if delimiter in answer_part:\n",
    "                    result['answer'] = answer_part.split(delimiter)[0].strip()\n",
    "                    break\n",
    "            else:\n",
    "                result['answer'] = answer_part.strip()\n",
    "        \n",
    "        elif 'ëª¨ë²” ë‹µì•ˆ:' in text:\n",
    "            answer_part = text.split('ëª¨ë²” ë‹µì•ˆ:', 1)[1]\n",
    "            result['answer'] = answer_part.split('\\n\\n')[0].strip()\n",
    "        \n",
    "        # ì¶”ê°€ ì •ë³´\n",
    "        if 'í•´ì„¤:' in text:\n",
    "            result['explanation'] = text.split('í•´ì„¤:', 1)[1].strip()\n",
    "        \n",
    "        if 'í•µì‹¬ í‚¤ì›Œë“œ:' in text:\n",
    "            result['keywords'] = text.split('í•µì‹¬ í‚¤ì›Œë“œ:', 1)[1].split('\\n')[0].strip()\n",
    "        \n",
    "        # ê°ê´€ì‹ ì„ íƒì§€\n",
    "        if question_type == \"ê°ê´€ì‹\":\n",
    "            choices = []\n",
    "            for marker in ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£']:\n",
    "                if marker in text:\n",
    "                    idx = text.index(marker)\n",
    "                    choice_text = text[idx:]\n",
    "                    choice_line = choice_text.split('\\n')[0]\n",
    "                    choices.append(choice_line)\n",
    "            if choices:\n",
    "                result['choices'] = choices\n",
    "        \n",
    "        # ê²€ì¦: ì§ˆë¬¸ê³¼ ë‹µë³€ ëª¨ë‘ ìˆì–´ì•¼ í•¨\n",
    "        if 'question' in result and 'answer' in result:\n",
    "            return result\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _parse_question(self, text: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"ì§ˆë¬¸ íŒŒì‹± (ë¶„ë¦¬í˜•ìš©)\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        if 'ë¬¸ì œ:' in text:\n",
    "            question_part = text.split('ë¬¸ì œ:', 1)[1]\n",
    "            \n",
    "            if question_type == \"ê°ê´€ì‹\":\n",
    "                # ì„ íƒì§€ ë¶„ë¦¬\n",
    "                if 'â‘ ' in question_part:\n",
    "                    result['question'] = question_part.split('â‘ ')[0].strip()\n",
    "                    \n",
    "                    # ì„ íƒì§€ ì¶”ì¶œ\n",
    "                    choices = []\n",
    "                    for marker in ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£']:\n",
    "                        if marker in text:\n",
    "                            idx = text.index(marker)\n",
    "                            choice_text = text[idx:]\n",
    "                            choice_line = choice_text.split('\\n')[0]\n",
    "                            choices.append(choice_line)\n",
    "                    result['choices'] = choices\n",
    "                else:\n",
    "                    result['question'] = question_part.strip()\n",
    "            else:\n",
    "                result['question'] = question_part.strip()\n",
    "        else:\n",
    "            # ë¬¸ì œ: ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš°\n",
    "            result['question'] = text.strip()\n",
    "        \n",
    "        return result if 'question' in result else None\n",
    "    \n",
    "    def _parse_answer(self, text: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"ë‹µë³€ íŒŒì‹± (ë¶„ë¦¬í˜•ìš©)\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        if 'ì •ë‹µ:' in text:\n",
    "            answer_part = text.split('ì •ë‹µ:', 1)[1]\n",
    "            \n",
    "            # ë‹¤ìŒ ì„¹ì…˜ê¹Œì§€ ì¶”ì¶œ\n",
    "            for delimiter in ['í•´ì„¤:', 'í•µì‹¬ í‚¤ì›Œë“œ:', 'ì±„ì  ê¸°ì¤€:', '\\n\\n']:\n",
    "                if delimiter in answer_part:\n",
    "                    result['answer'] = answer_part.split(delimiter)[0].strip()\n",
    "                    break\n",
    "            else:\n",
    "                result['answer'] = answer_part.strip()\n",
    "        else:\n",
    "            # ì •ë‹µ: ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš°\n",
    "            result['answer'] = text.strip()\n",
    "        \n",
    "        # ì¶”ê°€ ì •ë³´\n",
    "        if 'í•´ì„¤:' in text:\n",
    "            result['explanation'] = text.split('í•´ì„¤:', 1)[1].strip()\n",
    "        \n",
    "        if 'í•µì‹¬ í‚¤ì›Œë“œ:' in text:\n",
    "            result['keywords'] = text.split('í•µì‹¬ í‚¤ì›Œë“œ:', 1)[1].split('\\n')[0].strip()\n",
    "        \n",
    "        return result if 'answer' in result else None\n",
    "    \n",
    "    def _generate_fallback_answer(self, context: str, question: str, question_type: str) -> Dict:\n",
    "        \"\"\"í´ë°± ë‹µë³€ ìƒì„± (ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ)\"\"\"\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(\"      ğŸ”„ í´ë°± ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë‹µë³€ë§Œ ìƒì„±\n",
    "        simple_prompt = f\"\"\"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³ : {context[:400]}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "        \n",
    "        answer = self.generate_text(simple_prompt, max_tokens=200)\n",
    "        \n",
    "        return {\n",
    "            'answer': answer if answer else \"[ë‹µë³€ ìƒì„± ì‹¤íŒ¨ - ìˆ˜ë™ ì‘ì„± í•„ìš”]\",\n",
    "            'fallback': True\n",
    "        }\n",
    "    \n",
    "    def _generate_basic_answer(self, context: str, question: str, question_type: str) -> str:\n",
    "        \"\"\"ê¸°ë³¸ ë‹µë³€ ìƒì„± (ë§¤ìš° ê°„ë‹¨)\"\"\"\n",
    "        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ë¶€ë¶„ ì¶”ì¶œ\n",
    "        sentences = context.split('.')\n",
    "        \n",
    "        # ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë¬¸ì¥ ì°¾ê¸°\n",
    "        question_words = set(question.lower().split())\n",
    "        best_sentence = \"\"\n",
    "        best_score = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_words = set(sentence.lower().split())\n",
    "            score = len(question_words & sentence_words)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sentence = sentence.strip()\n",
    "        \n",
    "        if best_sentence:\n",
    "            return best_sentence + \".\"\n",
    "        else:\n",
    "            return \"[ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„± í•„ìš”]\"\n",
    "    \n",
    "    def _validate_quality(self, qa_pair: Dict) -> float:\n",
    "        \"\"\"í’ˆì§ˆ ê²€ì¦ (0-100 ì ìˆ˜)\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # ê¸°ë³¸ ì ìˆ˜\n",
    "        if qa_pair.get('question'):\n",
    "            score += 20\n",
    "            if len(qa_pair['question']) > 20:\n",
    "                score += 10\n",
    "        \n",
    "        if qa_pair.get('answer'):\n",
    "            score += 30\n",
    "            if len(qa_pair['answer']) > self.config.QUALITY_CONFIG['min_answer_length']:\n",
    "                score += 20\n",
    "        \n",
    "        # ì¶”ê°€ ì •ë³´\n",
    "        if qa_pair.get('explanation'):\n",
    "            score += 10\n",
    "        \n",
    "        if qa_pair.get('keywords'):\n",
    "            score += 10\n",
    "        \n",
    "        # í´ë°± ì‚¬ìš© ì‹œ ê°ì \n",
    "        if qa_pair.get('fallback'):\n",
    "            score -= 20\n",
    "        \n",
    "        return min(max(score, 0), 100)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"í†µê³„ ë°˜í™˜\"\"\"\n",
    "        total = self.stats['total_attempts']\n",
    "        if total == 0:\n",
    "            return self.stats\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            'success_rate': round(self.stats['successful'] / total * 100, 1),\n",
    "            'answer_rate': round(\n",
    "                self.stats['with_answer'] / max(self.stats['successful'], 1) * 100, 1\n",
    "            ),\n",
    "            'retry_rate': round(self.stats['retry_count'] / total * 100, 1),\n",
    "            'fallback_rate': round(self.stats['fallback_used'] / total * 100, 1),\n",
    "        }\n",
    "\n",
    "print(\"âœ… ë‹µë³€ ë³´ì¥ ìƒì„±ê¸° í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"ğŸ”§ ì£¼ìš” ê¸°ëŠ¥:\")\n",
    "print(\"  - í†µí•©í˜•/ë¶„ë¦¬í˜• ì„ íƒ ê°€ëŠ¥\")\n",
    "print(\"  - 3ë‹¨ê³„ í´ë°± ì‹œìŠ¤í…œ\")\n",
    "print(\"  - í’ˆì§ˆ ê²€ì¦\")\n",
    "print(\"  - RAG í†µí•©\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# ğŸ“ Chain-of-Thought (CoT) ìƒì„±ê¸° - ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •\n# ========================================\n#\n# ğŸ’¡ CoTë¥¼ ë°±ì—”ë“œ ê´€ì ì—ì„œ ì´í•´í•˜ê¸°:\n#   ì¼ë°˜ API: Request â†’ Response (ë°”ë¡œ ì‘ë‹µ)\n#   CoT API: Request â†’ Think â†’ Verify â†’ Improve â†’ Response (ê²€ì¦ í›„ ì‘ë‹µ)\n#   \n#   ë¹„ìœ : ì½”ë“œ ë¦¬ë·° í”„ë¡œì„¸ìŠ¤\n#   1. ì´ˆì•ˆ ì‘ì„± (Initial) \n#   2. ì…€í”„ ë¦¬ë·° (Self-Verification)\n#   3. ìˆ˜ì • (Improvement)\n#   4. ìµœì¢… ë¦¬ë·° (Final Check)\n\nclass ChainOfThoughtGenerator:\n    \"\"\"\n    Chain-of-Thought (CoT) ë°ì´í„° ìƒì„±ê¸°\n    \n    ğŸ”„ 4ë‹¨ê³„ ê²€ì¦ í”„ë¡œì„¸ìŠ¤:\n    1. ì´ˆê¸° ìƒì„±: ì²« ë²ˆì§¸ ì‹œë„\n    2. ìê°€ ê²€ì¦: \"ì´ê²Œ ë§ë‚˜?\" ìŠ¤ìŠ¤ë¡œ ì²´í¬\n    3. ê°œì„ : \"ì´ë ‡ê²Œ í•˜ë©´ ë” ë‚«ê² ë‹¤\" ìˆ˜ì •\n    4. ìµœì¢… ê²€ì¦: \"ì´ì œ ê´œì°®ì€ê°€?\" ë§ˆì§€ë§‰ ì²´í¬\n    \n    ë°±ì—”ë“œ ë¹„ìœ : \n    - ì´ˆê¸° ìƒì„± = MVP ê°œë°œ\n    - ìê°€ ê²€ì¦ = Unit Test\n    - ê°œì„  = Refactoring  \n    - ìµœì¢… ê²€ì¦ = Integration Test\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n        \"\"\"\n        ì´ˆê¸°í™”\n        \n        ğŸ’¡ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì‚¬ìš©\n        - config: ì„¤ì • ê°ì²´ (application.yml ê°™ì€)\n        - rag_system: ì„ íƒì  ì˜ì¡´ì„± (Optional dependency)\n        \"\"\"\n        self.config = config\n        self.cot_config = config.COT_CONFIG\n        self.rag = rag_system\n        \n        # ëª¨ë¸ ê´€ë ¨ (lazy loading)\n        self.model = None\n        self.tokenizer = None\n        self.model_loaded = False\n        \n        # CoT í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ\n        # ğŸ’¡ ê° ë‹¨ê³„ë³„ë¡œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (Strategy Pattern)\n        self.cot_prompts = self._load_cot_prompts()\n        \n        # ìºì‹œ ì„¤ì • (ë™ì¼ ì…ë ¥ ì¬ì‚¬ìš©)\n        # ğŸ’¡ ë°±ì—”ë“œì˜ Redis ìºì‹±ê³¼ ìœ ì‚¬\n        self.cache = {} if config.COT_CONFIG['cache_results'] else None\n        \n        # í†µê³„ ì¶”ì  (ëª¨ë‹ˆí„°ë§ìš©)\n        self.stats = {\n            'total_attempts': 0,      # ì´ ì‹œë„\n            'successful': 0,           # ì„±ê³µ\n            'failed': 0,              # ì‹¤íŒ¨\n            'avg_iterations': 0,      # í‰ê·  ë°˜ë³µ íšŸìˆ˜\n            'improvement_count': 0,   # ê°œì„  íšŸìˆ˜\n            'cache_hits': 0,          # ìºì‹œ íˆíŠ¸\n            'quality_scores': []      # í’ˆì§ˆ ì ìˆ˜ë“¤\n        }\n    \n    def generate_qa_pair(self, context: str, question_type: str = \"ì£¼ê´€ì‹\") -> Optional[Dict]:\n        \"\"\"\n        CoT ë°©ì‹ìœ¼ë¡œ QA ìŒ ìƒì„± - ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸\n        \n        ğŸ”„ ì‹¤í–‰ íë¦„:\n        1. ìºì‹œ í™•ì¸ (ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜)\n        2. ì´ˆê¸° ìƒì„±\n        3. í’ˆì§ˆ ì²´í¬ ë£¨í”„ (ìµœëŒ€ Në²ˆ)\n           - ìê°€ ê²€ì¦\n           - ì ìˆ˜ í™•ì¸\n           - í•„ìš”ì‹œ ê°œì„ \n        4. ìµœì¢… ê²€ì¦\n        5. ê²°ê³¼ ë°˜í™˜\n        \n        Args:\n            context: ì°¸ê³  ë¬¸ì„œ (ì»¨í…ìŠ¤íŠ¸)\n            question_type: ë¬¸ì œ ìœ í˜•\n            \n        Returns:\n            ìƒì„±ëœ QA ìŒ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)\n        \"\"\"\n        self.stats['total_attempts'] += 1\n        \n        # 1. ìºì‹œ í™•ì¸ (ë°±ì—”ë“œì˜ ìºì‹± ë ˆì´ì–´)\n        if self.cache is not None:\n            # í•´ì‹œ í‚¤ ìƒì„± (contextì˜ ì• 200ìë¡œ)\n            cache_key = hash(f\"{context[:200]}_{question_type}\")\n            if cache_key in self.cache:\n                self.stats['cache_hits'] += 1\n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(\"  ğŸ’¾ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜ (Cache Hit\\!)\")\n                return self.cache[cache_key]\n        \n        try:\n            # 2. RAG í™œìš© (ì„ íƒì )\n            # ğŸ’¡ ë°±ì—”ë“œì˜ ì™¸ë¶€ API í˜¸ì¶œê³¼ ìœ ì‚¬\n            if self.rag and self.config.RAG_CONFIG['use_rag']:\n                query = f\"{question_type} ë¬¸ì œ ìƒì„±ì„ ìœ„í•œ {context[:100]}\"\n                retrieved = self.rag.search(query)  # DB ê²€ìƒ‰\n                \n                if retrieved:\n                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€\n                    additional = \"\\n\".join([r['text'][:200] for r in retrieved[:2]])\n                    context = f\"{context}\\n\\nê´€ë ¨ ë¬¸ì„œ:\\n{additional}\"\n            \n            # 3. ë‹¨ê³„ë³„ ì‹¤í–‰\n            if self.config.EXPERIMENT_MODE['verbose']:\n                print(f\"  [CoT] {question_type} ìƒì„± ì‹œì‘...\")\n                print(\"    ğŸ”„ [1/4] ì´ˆê¸° ë¬¸ì œ ìƒì„±...\")\n            \n            # 3-1. ì´ˆê¸° ìƒì„± (ì²« ì‹œë„)\n            initial_qa = self._initial_generation(context, question_type)\n            if not initial_qa:\n                raise ValueError(\"ì´ˆê¸° ìƒì„± ì‹¤íŒ¨\")\n            \n            current_question = initial_qa['question']\n            current_answer = initial_qa['answer']\n            \n            # 3-2. ë°˜ë³µì  ê°œì„  ë£¨í”„\n            iteration_count = 0\n            for i in range(self.cot_config['max_iterations']):\n                if not self.cot_config['use_self_verification']:\n                    break  # ìê°€ ê²€ì¦ ë¹„í™œì„±í™”ë©´ ìŠ¤í‚µ\n                    \n                iteration_count += 1\n                \n                # ìê°€ ê²€ì¦ ë‹¨ê³„\n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(f\"    ğŸ”„ [{2+i*2}/4] ìê°€ ê²€ì¦...\")\n                \n                # í˜„ì¬ QAë¥¼ ê²€ì¦\n                verification = self._self_verification(current_question, current_answer)\n                \n                # ì ìˆ˜ ì¶”ì¶œ (1-10 â†’ 1-100ìœ¼ë¡œ ë³€í™˜)\n                score = self._extract_score(verification)\n                \n                # í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ ì²´í¬\n                if score >= self.cot_config['quality_threshold']:\n                    if self.config.EXPERIMENT_MODE['verbose']:\n                        print(f\"      âœ… í’ˆì§ˆ ê¸°ì¤€ í†µê³¼\\! (ì ìˆ˜: {score}/100)\")\n                    break  # ì¶©ë¶„íˆ ì¢‹ìœ¼ë©´ ì¢…ë£Œ\n                \n                # ê°œì„  í•„ìš”\n                if not self.cot_config['use_improvement']:\n                    break  # ê°œì„  ë¹„í™œì„±í™”ë©´ ìŠ¤í‚µ\n                    \n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(f\"    ğŸ”„ [{3+i*2}/4] ê°œì„  ìƒì„±...\")\n                    print(f\"      (í˜„ì¬ ì ìˆ˜: {score}/100)\")\n                \n                # í”¼ë“œë°± ê¸°ë°˜ ê°œì„ \n                improved_qa = self._improvement_generation(\n                    current_question, \n                    current_answer, \n                    verification  # ê²€ì¦ í”¼ë“œë°± ì „ë‹¬\n                )\n                \n                if improved_qa:\n                    # ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ êµì²´\n                    current_question = improved_qa['question']\n                    current_answer = improved_qa['answer']\n                    self.stats['improvement_count'] += 1\n            \n            # 3-3. ìµœì¢… ê²€ì¦\n            final_score = 70  # ê¸°ë³¸ê°’\n            if self.cot_config['use_final_check']:\n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(\"    ğŸ”„ [4/4] ìµœì¢… ê²€ì¦...\")\n                \n                final_check = self._final_check(current_question, current_answer)\n                final_score = self._extract_final_score(final_check)\n                \n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(f\"      ìµœì¢… í’ˆì§ˆ ì ìˆ˜: {final_score}/100\")\n            \n            # 4. ê²°ê³¼ ì¤€ë¹„\n            result = {\n                'question': current_question,\n                'answer': current_answer,\n                'context': context[:500],\n                'question_type': question_type,\n                'generation_mode': 'cot',\n                'quality_score': final_score,\n                'iterations': iteration_count,  # ëª‡ ë²ˆ ê°œì„ í–ˆëŠ”ì§€\n                'model': self.config.MODEL_NAME,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            # 5. í†µê³„ ì—…ë°ì´íŠ¸\n            self.stats['successful'] += 1\n            self.stats['quality_scores'].append(final_score)\n            \n            # 6. ìºì‹œ ì €ì¥ (í’ˆì§ˆ ì¢‹ì€ ê²ƒë§Œ)\n            if self.cache is not None and final_score >= self.cot_config['quality_threshold']:\n                cache_key = hash(f\"{context[:200]}_{question_type}\")\n                self.cache[cache_key] = result\n                \n            return result\n            \n        except Exception as e:\n            logger.error(f\"[CoT] ìƒì„± ì˜¤ë¥˜: {e}\")\n            self.stats['failed'] += 1\n            return None\n    \n    def _initial_generation(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        1ë‹¨ê³„: ì´ˆê¸° ìƒì„±\n        \n        ğŸ’¡ ì²« ë²ˆì§¸ ì‹œë„ - MVPì²˜ëŸ¼ ë¹ ë¥´ê²Œ ë§Œë“¤ê¸°\n        Temperatureë¥¼ ì•½ê°„ ë†’ê²Œ(0.7) ì„¤ì •í•´ì„œ ì°½ì˜ì ì¸ ë¬¸ì œ ìƒì„±\n        \"\"\"\n        prompt = self.cot_prompts['initial_generation'].format(\n            context=context[:800],  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ\n            question_type=question_type\n        )\n        \n        # ì´ˆê¸° ìƒì„±ì€ ì•½ê°„ ì°½ì˜ì ìœ¼ë¡œ (temperature ë†’ê²Œ)\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_initial', 0.7)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ Q&A ì¶”ì¶œ\n        return self._parse_qa(generated)\n    \n    def _self_verification(self, question: str, answer: str) -> str:\n        \"\"\"\n        2ë‹¨ê³„: ìê°€ ê²€ì¦\n        \n        ğŸ’¡ ìŠ¤ìŠ¤ë¡œ ì²´í¬ - Code Reviewì²˜ëŸ¼\n        Temperatureë¥¼ ë‚®ê²Œ(0.3) ì„¤ì •í•´ì„œ ë¹„íŒì ìœ¼ë¡œ ê²€í† \n        \n        ì²´í¬ í¬ì¸íŠ¸:\n        - ë¬¸ì œê°€ ëª…í™•í•œê°€?\n        - ë‹µë³€ì´ ì •í™•í•œê°€?\n        - ê¸ˆìœµ ìš©ì–´ê°€ ì˜¬ë°”ë¥¸ê°€?\n        \"\"\"\n        prompt = self.cot_prompts['self_verification'].format(\n            question=question,\n            answer=answer\n        )\n        \n        # ê²€ì¦ì€ ë³´ìˆ˜ì ìœ¼ë¡œ (temperature ë‚®ê²Œ)\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_verification', 0.3)\n        verification = self.generate_text(prompt, temperature=temp, max_tokens=300)\n        \n        return verification\n    \n    def _improvement_generation(self, question: str, answer: str, feedback: str) -> Optional[Dict]:\n        \"\"\"\n        3ë‹¨ê³„: ê°œì„  ìƒì„±\n        \n        ğŸ’¡ í”¼ë“œë°± ë°˜ì˜ - Refactoringì²˜ëŸ¼\n        ê²€ì¦ì—ì„œ ë‚˜ì˜¨ ë¬¸ì œì ì„ ìˆ˜ì •\n        \n        ê°œì„  ì „ëµ:\n        - ì§€ì ëœ ë¬¸ì œì  ìˆ˜ì •\n        - ë” ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©\n        - êµ¬ì²´ì  ì˜ˆì‹œ ì¶”ê°€\n        \"\"\"\n        # ì§ˆë¬¸ ê¸°ë°˜ RAG ì¬ê²€ìƒ‰ (ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•´)\n        enhanced_context = \"\"\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            # ì§ˆë¬¸ìœ¼ë¡œ ë” ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰\n            retrieved = self.rag.search(question, top_k=5)\n            \n            if retrieved:\n                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¡œ\n                enhanced_context = \"\\n\\n### ì§ˆë¬¸ ê´€ë ¨ ì°¸ê³  ìë£Œ:\\n\" + \"\\n\".join([\n                    f\"- {doc['text'][:200]}\"\n                    for doc in retrieved[:3]\n                ])\n        \n        # ê°œì„  í”„ë¡¬í”„íŠ¸ ìƒì„±\n        prompt = f\"\"\"{self.cot_prompts['improvement']}\n\n{enhanced_context}\"\"\".format(\n            question=question,\n            answer=answer,\n            feedback=feedback\n        )\n        \n        # ê°œì„ ì€ ì¤‘ê°„ ì˜¨ë„ë¡œ (ê· í˜•)\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_improvement', 0.5)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        return self._parse_qa(generated)\n    \n    def _final_check(self, question: str, answer: str) -> str:\n        \"\"\"\n        4ë‹¨ê³„: ìµœì¢… ê²€ì¦\n        \n        ğŸ’¡ ë§ˆì§€ë§‰ ì²´í¬ - Production ë°°í¬ ì „ ì²´í¬ì²˜ëŸ¼\n        FSKU ì‹œí—˜ ì¶œì œ ê°€ëŠ¥í•œ ìˆ˜ì¤€ì¸ì§€ ìµœì¢… í™•ì¸\n        \"\"\"\n        prompt = self.cot_prompts['final_check'].format(\n            question=question,\n            answer=answer\n        )\n        \n        # ìµœì¢… ê²€ì¦ë„ ë³´ìˆ˜ì ìœ¼ë¡œ\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_final', 0.3)\n        check_result = self.generate_text(prompt, temperature=temp, max_tokens=200)\n        \n        return check_result\n    \n    def get_stats(self) -> Dict:\n        \"\"\"\n        í†µê³„ ë°˜í™˜ - ëª¨ë‹ˆí„°ë§ìš©\n        \n        ğŸ’¡ ë°±ì—”ë“œì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê³¼ ìœ ì‚¬\n        - ì„±ê³µë¥  = API Success Rate\n        - ê°œì„ ë¥  = Cache Hit Rate  \n        - í’ˆì§ˆ ì ìˆ˜ = Response Time\n        \"\"\"\n        total = self.stats['total_attempts']\n        if total == 0:\n            return self.stats\n        \n        avg_quality = np.mean(self.stats['quality_scores']) if self.stats['quality_scores'] else 0\n        \n        return {\n            **self.stats,\n            'success_rate': round(self.stats['successful'] / total * 100, 1),\n            'improvement_rate': round(self.stats['improvement_count'] / max(self.stats['successful'], 1) * 100, 1),\n            'cache_hit_rate': round(self.stats['cache_hits'] / total * 100, 1) if self.cache else 0,\n            'avg_quality_score': round(avg_quality, 1)\n        }\n\nprint(\"âœ… Chain-of-Thought (CoT) ìƒì„±ê¸° ì •ì˜ ì™„ë£Œ\\!\")\nprint(\"\\nğŸ¯ CoT ì‚¬ìš© ì‹œì :\")\nprint(\"  - ì¼ë°˜ ìƒì„±: ëŒ€ëŸ‰ ë°ì´í„°ê°€ í•„ìš”í•  ë•Œ (ì†ë„ ìš°ì„ )\")\nprint(\"  - CoT ìƒì„±: ê³ í’ˆì§ˆ ë°ì´í„°ê°€ í•„ìš”í•  ë•Œ (í’ˆì§ˆ ìš°ì„ )\")\nprint(\"\\nğŸ’¡ ë°±ì—”ë“œ ë¹„ìœ :\")\nprint(\"  - ì¼ë°˜ = Sync API (ë°”ë¡œ ì‘ë‹µ)\")\nprint(\"  - CoT = Async + Queue + Retry (ì‹ ì¤‘í•œ ì²˜ë¦¬)\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 4-1. Chain-of-Thought (CoT) ë°ì´í„° ìƒì„±ê¸° â­",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë¹„êµ í…ŒìŠ¤íŠ¸: í†µí•©í˜• vs ë¶„ë¦¬í˜•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_modes():\n",
    "    \"\"\"\n",
    "    í†µí•©í˜•ê³¼ ë¶„ë¦¬í˜• ë¹„êµ í…ŒìŠ¤íŠ¸\n",
    "    ì‹¤ì œë¡œ ë‘ ëª¨ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ ë¹„êµ\n",
    "    \"\"\"\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸\n",
    "    test_contexts = [\n",
    "        \"\"\"ê¸ˆìœµë³´ì•ˆì›(FSI)ì€ êµ­ë‚´ ê¸ˆìœµ IT ë³´ì•ˆì„ ì´ê´„í•˜ëŠ” ì „ë¬¸ê¸°ê´€ìœ¼ë¡œ, \n",
    "        ê¸ˆìœµê¶Œ ì‚¬ì´ë²„ ë³´ì•ˆ ê°•í™”ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ì •ì±…ê³¼ ê¸°ìˆ ì„ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤. \n",
    "        íŠ¹íˆ ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„± í™•ë³´ë¥¼ ìœ„í•´ ë‹¤ë‹¨ê³„ ì¸ì¦, ì•”í˜¸í™” ê¸°ìˆ , \n",
    "        ì´ìƒê±°ë˜ íƒì§€ ì‹œìŠ¤í…œ ë“±ì„ ìš´ì˜í•˜ê³  ìˆìœ¼ë©°, ê¸ˆìœµíšŒì‚¬ë“¤ì˜ ë³´ì•ˆ ìˆ˜ì¤€ì„ \n",
    "        ì •ê¸°ì ìœ¼ë¡œ ì ê²€í•˜ê³  í‰ê°€í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\",\n",
    "        \n",
    "        \"\"\"ë°”ì ¤III ê·œì œëŠ” ì€í–‰ì˜ ìë³¸ ì ì •ì„±ì„ ê°•í™”í•˜ê¸° ìœ„í•œ êµ­ì œ ê¸°ì¤€ìœ¼ë¡œ,\n",
    "        ë³´í†µì£¼ìë³¸ë¹„ìœ¨ 4.5%, Tier1 ìë³¸ë¹„ìœ¨ 6%, ì´ìë³¸ë¹„ìœ¨ 8% ì´ìƒì„\n",
    "        ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ìœ ë™ì„± ì»¤ë²„ë¦¬ì§€ ë¹„ìœ¨(LCR)ê³¼ ìˆœì•ˆì •ìê¸ˆì¡°ë‹¬ë¹„ìœ¨(NSFR)ì„\n",
    "        ë„ì…í•˜ì—¬ ì€í–‰ì˜ ìœ ë™ì„± ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ê°•í™”í–ˆìŠµë‹ˆë‹¤.\"\"\",\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ“Š í†µí•©í˜• vs ë¶„ë¦¬í˜• ë¹„êµ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. í†µí•©í˜• í…ŒìŠ¤íŠ¸\n",
    "    print(\"\\n1ï¸âƒ£ í†µí•©í˜• ëª¨ë“œ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # í†µí•©í˜• ì„¤ì •\n",
    "    integrated_config = ExperimentConfig()\n",
    "    integrated_config.GENERATION_MODE = \"integrated\"\n",
    "    integrated_config.EXPERIMENT_MODE['verbose'] = True\n",
    "    \n",
    "    integrated_gen = AnswerGuaranteedGenerator(integrated_config)\n",
    "    \n",
    "    integrated_results = []\n",
    "    integrated_times = []\n",
    "    \n",
    "    for i, context in enumerate(test_contexts):\n",
    "        for question_type in [\"ê°ê´€ì‹\", \"ì£¼ê´€ì‹\", \"ë‹¨ë‹µí˜•\"]:\n",
    "            print(f\"\\ní…ŒìŠ¤íŠ¸ {i+1} - {question_type}:\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = integrated_gen.generate_qa_pair(context, question_type)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if result:\n",
    "                integrated_results.append(result)\n",
    "                integrated_times.append(elapsed)\n",
    "                \n",
    "                print(f\"  âœ… ì„±ê³µ ({elapsed:.2f}ì´ˆ)\")\n",
    "                print(f\"  ë‹µë³€ ê¸¸ì´: {len(result.get('answer', ''))} ê¸€ì\")\n",
    "                print(f\"  í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.0f}/100\")\n",
    "            else:\n",
    "                print(f\"  âŒ ì‹¤íŒ¨\")\n",
    "    \n",
    "    results['integrated'] = {\n",
    "        'data': integrated_results,\n",
    "        'times': integrated_times,\n",
    "        'stats': integrated_gen.get_stats()\n",
    "    }\n",
    "    \n",
    "    # 2. ë¶„ë¦¬í˜• í…ŒìŠ¤íŠ¸\n",
    "    print(\"\\n\\n2ï¸âƒ£ ë¶„ë¦¬í˜• ëª¨ë“œ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # ë¶„ë¦¬í˜• ì„¤ì •\n",
    "    separated_config = ExperimentConfig()\n",
    "    separated_config.GENERATION_MODE = \"separated\"\n",
    "    separated_config.EXPERIMENT_MODE['verbose'] = True\n",
    "    \n",
    "    separated_gen = AnswerGuaranteedGenerator(separated_config)\n",
    "    \n",
    "    separated_results = []\n",
    "    separated_times = []\n",
    "    \n",
    "    for i, context in enumerate(test_contexts):\n",
    "        for question_type in [\"ê°ê´€ì‹\", \"ì£¼ê´€ì‹\", \"ë‹¨ë‹µí˜•\"]:\n",
    "            print(f\"\\ní…ŒìŠ¤íŠ¸ {i+1} - {question_type}:\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = separated_gen.generate_qa_pair(context, question_type)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if result:\n",
    "                separated_results.append(result)\n",
    "                separated_times.append(elapsed)\n",
    "                \n",
    "                print(f\"  âœ… ì„±ê³µ ({elapsed:.2f}ì´ˆ)\")\n",
    "                print(f\"  ë‹µë³€ ê¸¸ì´: {len(result.get('answer', ''))} ê¸€ì\")\n",
    "                print(f\"  í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.0f}/100\")\n",
    "                print(f\"  í´ë°± ì‚¬ìš©: {result.get('fallback', False)}\")\n",
    "            else:\n",
    "                print(f\"  âŒ ì‹¤íŒ¨\")\n",
    "    \n",
    "    results['separated'] = {\n",
    "        'data': separated_results,\n",
    "        'times': separated_times,\n",
    "        'stats': separated_gen.get_stats()\n",
    "    }\n",
    "    \n",
    "    # 3. ê²°ê³¼ ë¹„êµ\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“ˆ ë¹„êµ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # í†µê³„ ë¹„êµ í…Œì´ë¸”\n",
    "    comparison_data = []\n",
    "    \n",
    "    for mode in ['integrated', 'separated']:\n",
    "        mode_stats = results[mode]['stats']\n",
    "        mode_times = results[mode]['times']\n",
    "        mode_data = results[mode]['data']\n",
    "        \n",
    "        avg_time = np.mean(mode_times) if mode_times else 0\n",
    "        avg_answer_len = np.mean([len(d.get('answer', '')) for d in mode_data]) if mode_data else 0\n",
    "        avg_quality = np.mean([d.get('quality_score', 0) for d in mode_data]) if mode_data else 0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'ëª¨ë“œ': mode.upper(),\n",
    "            'ì„±ê³µë¥ ': f\"{mode_stats.get('success_rate', 0):.1f}%\",\n",
    "            'ë‹µë³€ í¬í•¨ë¥ ': f\"{mode_stats.get('answer_rate', 0):.1f}%\",\n",
    "            'í‰ê·  ì‹œê°„': f\"{avg_time:.2f}ì´ˆ\",\n",
    "            'í‰ê·  ë‹µë³€ ê¸¸ì´': f\"{avg_answer_len:.0f}ì\",\n",
    "            'í‰ê·  í’ˆì§ˆ': f\"{avg_quality:.0f}/100\",\n",
    "            'ì¬ì‹œë„ìœ¨': f\"{mode_stats.get('retry_rate', 0):.1f}%\",\n",
    "            'í´ë°± ì‚¬ìš©ë¥ ': f\"{mode_stats.get('fallback_rate', 0):.1f}%\"\n",
    "        })\n",
    "    \n",
    "    # í…Œì´ë¸” ì¶œë ¥\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"\\në¹„êµ í…Œì´ë¸”:\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # 4. ìƒ˜í”Œ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“ ìƒì„±ëœ ìƒ˜í”Œ ë¹„êµ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for mode in ['integrated', 'separated']:\n",
    "        print(f\"\\n[{mode.upper()} ëª¨ë“œ ìƒ˜í”Œ]\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        if results[mode]['data']:\n",
    "            sample = results[mode]['data'][0]\n",
    "            print(f\"ë¬¸ì œ ìœ í˜•: {sample.get('question_type')}\")\n",
    "            print(f\"ë¬¸ì œ: {sample.get('question', 'N/A')[:150]}...\")\n",
    "            print(f\"ë‹µë³€: {sample.get('answer', 'N/A')[:150]}...\")\n",
    "            print(f\"í’ˆì§ˆ: {sample.get('quality_score', 0):.0f}/100\")\n",
    "    \n",
    "    # 5. ê¶Œì¥ì‚¬í•­\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ’¡ ê¶Œì¥ì‚¬í•­\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ì†ë„ ë¹„êµ\n",
    "    integrated_avg_time = np.mean(results['integrated']['times']) if results['integrated']['times'] else 999\n",
    "    separated_avg_time = np.mean(results['separated']['times']) if results['separated']['times'] else 999\n",
    "    \n",
    "    if integrated_avg_time < separated_avg_time * 0.7:\n",
    "        print(\"ğŸš€ ì†ë„ ìš°ì„ : í†µí•©í˜• ëª¨ë“œ ì¶”ì²œ (ì•½ {:.0f}% ë¹ ë¦„)\".format(\n",
    "            (1 - integrated_avg_time/separated_avg_time) * 100\n",
    "        ))\n",
    "    elif separated_avg_time < integrated_avg_time * 0.7:\n",
    "        print(\"ğŸš€ ì†ë„ ìš°ì„ : ë¶„ë¦¬í˜• ëª¨ë“œ ì¶”ì²œ (ì•½ {:.0f}% ë¹ ë¦„)\".format(\n",
    "            (1 - separated_avg_time/integrated_avg_time) * 100\n",
    "        ))\n",
    "    else:\n",
    "        print(\"âš–ï¸ ì†ë„: ë‘ ëª¨ë“œ ë¹„ìŠ·í•¨\")\n",
    "    \n",
    "    # í’ˆì§ˆ ë¹„êµ\n",
    "    integrated_avg_quality = np.mean([d.get('quality_score', 0) for d in results['integrated']['data']])\n",
    "    separated_avg_quality = np.mean([d.get('quality_score', 0) for d in results['separated']['data']])\n",
    "    \n",
    "    if integrated_avg_quality > separated_avg_quality + 5:\n",
    "        print(\"â­ í’ˆì§ˆ ìš°ì„ : í†µí•©í˜• ëª¨ë“œ ì¶”ì²œ (í‰ê·  {:.0f}ì  ë†’ìŒ)\".format(\n",
    "            integrated_avg_quality - separated_avg_quality\n",
    "        ))\n",
    "    elif separated_avg_quality > integrated_avg_quality + 5:\n",
    "        print(\"â­ í’ˆì§ˆ ìš°ì„ : ë¶„ë¦¬í˜• ëª¨ë“œ ì¶”ì²œ (í‰ê·  {:.0f}ì  ë†’ìŒ)\".format(\n",
    "            separated_avg_quality - integrated_avg_quality\n",
    "        ))\n",
    "    else:\n",
    "        print(\"âš–ï¸ í’ˆì§ˆ: ë‘ ëª¨ë“œ ë¹„ìŠ·í•¨\")\n",
    "    \n",
    "    print(\"\\nğŸ“Œ ì¼ë°˜ì  ê¶Œì¥ì‚¬í•­:\")\n",
    "    print(\"  - ëŒ€ëŸ‰ ìƒì„± ì‹œ: í†µí•©í˜• (ë¹ ë¦„)\")\n",
    "    print(\"  - ê³ í’ˆì§ˆ í•„ìš” ì‹œ: ë¶„ë¦¬í˜• (ì •í™•í•¨)\")\n",
    "    print(\"  - ê· í˜•: í†µí•©í˜• + í’ˆì§ˆ ê²€ì¦\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰)\n",
    "# comparison_results = compare_generation_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bulk_generation():\n",
    "    \"\"\"\n",
    "    ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± ì‹¤í–‰\n",
    "    ì„¤ì •ì— ë”°ë¼ í†µí•©í˜• ë˜ëŠ” ë¶„ë¦¬í˜•ìœ¼ë¡œ ì‹¤í–‰\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ FSKU ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± ì‹œì‘\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ì„¤ì • ì¶œë ¥\n",
    "    print(\"\\nğŸ“‹ í˜„ì¬ ì„¤ì •:\")\n",
    "    print(f\"  - ëª¨ë¸: {config.MODEL_NAME}\")\n",
    "    print(f\"  - ìƒì„± ëª¨ë“œ: {config.GENERATION_MODE}\")\n",
    "    print(f\"  - ëª©í‘œ ê°œìˆ˜: {config.BATCH_CONFIG['target_count']}ê°œ\")\n",
    "    print(f\"  - Temperature: {config.GENERATION_PARAMS['temperature']}\")\n",
    "    print(f\"  - í’ˆì§ˆ ì„ê³„ê°’: {config.QUALITY_CONFIG['quality_threshold']}\")\n",
    "    print(f\"  - RAG ì‚¬ìš©: {config.RAG_CONFIG['use_rag']}\")\n",
    "    \n",
    "    # 1. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì„ íƒì )\n",
    "    rag_system = None\n",
    "    if config.RAG_CONFIG['use_rag']:\n",
    "        print(\"\\nğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...\")\n",
    "        rag_system = RAGSystem(config)\n",
    "        rag_system.initialize()\n",
    "    \n",
    "    # 2. ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "    print(\"\\nğŸ¤– ìƒì„±ê¸° ì´ˆê¸°í™”...\")\n",
    "    generator = AnswerGuaranteedGenerator(config, rag_system)\n",
    "    generator.load_model()\n",
    "    \n",
    "    # 3. ìƒ˜í”Œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "    sample_contexts = [\n",
    "        \"\"\"ì „ìê¸ˆìœµê±°ë˜ë²•ì€ ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„±ê³¼ ì‹ ë¢°ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ë²•ë¥ ë¡œ,\n",
    "        ê¸ˆìœµíšŒì‚¬ëŠ” ì „ìê¸ˆìœµê±°ë˜ ì‹œ ë‹¤ë‹¨ê³„ ì¸ì¦ì„ ì ìš©í•´ì•¼ í•˜ë©°,\n",
    "        ê±°ë˜ ë‚´ì—­ì€ ìµœì†Œ 5ë…„ê°„ ë³´ê´€í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\",\n",
    "        \n",
    "        \"\"\"ê°œì¸ì •ë³´ë³´í˜¸ë²•ì— ë”°ë¼ ê¸ˆìœµíšŒì‚¬ëŠ” ê³ ê°ì˜ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•  ë•Œ\n",
    "        ëª…í™•í•œ ë™ì˜ë¥¼ ë°›ì•„ì•¼ í•˜ë©°, ìˆ˜ì§‘ ëª©ì  ì™¸ ì‚¬ìš©ì€ ê¸ˆì§€ë©ë‹ˆë‹¤.\n",
    "        ê°œì¸ì •ë³´ ìœ ì¶œ ì‹œ 24ì‹œê°„ ë‚´ ì‹ ê³  ì˜ë¬´ê°€ ìˆìŠµë‹ˆë‹¤.\"\"\",\n",
    "        \n",
    "        \"\"\"ë°”ì ¤III ê·œì œëŠ” ì€í–‰ì˜ ìë³¸ ì ì •ì„±ì„ ê°•í™”í•˜ê¸° ìœ„í•œ êµ­ì œ ê¸°ì¤€ìœ¼ë¡œ,\n",
    "        ë³´í†µì£¼ìë³¸ë¹„ìœ¨ 4.5%, Tier1 ìë³¸ë¹„ìœ¨ 6%, ì´ìë³¸ë¹„ìœ¨ 8% ì´ìƒì„\n",
    "        ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\",\n",
    "        \n",
    "        \"\"\"ê¸ˆìœµì‹¤ëª…ê±°ë˜ë²•ì€ ê¸ˆìœµê±°ë˜ì˜ íˆ¬ëª…ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ë²•ë¥ ë¡œ,\n",
    "        ëª¨ë“  ê¸ˆìœµê±°ë˜ëŠ” ì‹¤ëª…ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ì•¼ í•˜ë©°,\n",
    "        ì°¨ëª…ê±°ë˜ëŠ” ì—„ê²©íˆ ê¸ˆì§€ë©ë‹ˆë‹¤.\"\"\",\n",
    "        \n",
    "        \"\"\"ìê¸ˆì„¸íƒë°©ì§€ì œë„(AML)ëŠ” ë¶ˆë²•ìê¸ˆì˜ ì„¸íƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì œë„ë¡œ,\n",
    "        ê¸ˆìœµíšŒì‚¬ëŠ” ê³ ê°í™•ì¸ì˜ë¬´(CDD)ë¥¼ ìˆ˜í–‰í•´ì•¼ í•˜ë©°,\n",
    "        ì˜ì‹¬ê±°ë˜ëŠ” FIUì— ë³´ê³ í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\",\n",
    "    ]\n",
    "    \n",
    "    # 4. ëŒ€ëŸ‰ ìƒì„±\n",
    "    print(f\"\\nğŸ“Š ë°ì´í„° ìƒì„± ì‹œì‘ (ëª©í‘œ: {config.BATCH_CONFIG['target_count']}ê°œ)\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    generated_data = []\n",
    "    attempts = 0\n",
    "    max_attempts = config.BATCH_CONFIG['target_count'] * config.BATCH_CONFIG['max_attempts_ratio']\n",
    "    \n",
    "    # ë¬¸ì œ ìœ í˜•ë³„ ëª©í‘œ\n",
    "    type_targets = {\n",
    "        qtype: int(config.BATCH_CONFIG['target_count'] * ratio)\n",
    "        for qtype, ratio in config.QUESTION_TYPE_DISTRIBUTION.items()\n",
    "    }\n",
    "    type_counts = {qtype: 0 for qtype in type_targets}\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™© í‘œì‹œ ê°„ê²©\n",
    "    progress_interval = max(config.BATCH_CONFIG['target_count'] // 10, 1)\n",
    "    save_interval = config.BATCH_CONFIG['save_interval']\n",
    "    \n",
    "    # ì‹œì‘ ì‹œê°„\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while len(generated_data) < config.BATCH_CONFIG['target_count'] and attempts < max_attempts:\n",
    "        # ì»¨í…ìŠ¤íŠ¸ ì„ íƒ\n",
    "        context = np.random.choice(sample_contexts)\n",
    "        \n",
    "        # ë¬¸ì œ ìœ í˜• ì„ íƒ (ë¶€ì¡±í•œ ìœ í˜• ìš°ì„ )\n",
    "        remaining_types = [\n",
    "            qtype for qtype, target in type_targets.items()\n",
    "            if type_counts[qtype] < target\n",
    "        ]\n",
    "        \n",
    "        if remaining_types:\n",
    "            question_type = np.random.choice(remaining_types)\n",
    "        else:\n",
    "            question_type = np.random.choice(list(config.QUESTION_TYPE_DISTRIBUTION.keys()))\n",
    "        \n",
    "        # ìƒì„±\n",
    "        qa_pair = generator.generate_qa_pair(context, question_type)\n",
    "        \n",
    "        # ê²€ì¦\n",
    "        if qa_pair and qa_pair.get('answer'):\n",
    "            # í’ˆì§ˆ í™•ì¸\n",
    "            if qa_pair.get('quality_score', 0) >= config.QUALITY_CONFIG['quality_threshold']:\n",
    "                generated_data.append(qa_pair)\n",
    "                type_counts[question_type] += 1\n",
    "                \n",
    "                # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "                if len(generated_data) % progress_interval == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = len(generated_data) / elapsed\n",
    "                    eta = (config.BATCH_CONFIG['target_count'] - len(generated_data)) / rate\n",
    "                    \n",
    "                    print(f\"  ì§„í–‰: {len(generated_data)}/{config.BATCH_CONFIG['target_count']} \"\n",
    "                          f\"({len(generated_data)/config.BATCH_CONFIG['target_count']*100:.1f}%) \"\n",
    "                          f\"| ì†ë„: {rate:.1f}ê°œ/ì´ˆ | ì˜ˆìƒ ì‹œê°„: {eta:.0f}ì´ˆ\")\n",
    "                \n",
    "                # ì¤‘ê°„ ì €ì¥\n",
    "                if len(generated_data) % save_interval == 0:\n",
    "                    temp_path = AUGMENTED_DIR / f\"temp_batch_{len(generated_data)}.json\"\n",
    "                    with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(generated_data, f, ensure_ascii=False, indent=2)\n",
    "                    print(f\"  ğŸ’¾ ì¤‘ê°„ ì €ì¥: {temp_path}\")\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    # ìƒì„± ì™„ë£Œ\n",
    "    elapsed_total = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… ìƒì„± ì™„ë£Œ!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    print(f\"\\nğŸ“Š ìƒì„± í†µê³„:\")\n",
    "    print(f\"  - ì´ ìƒì„±: {len(generated_data)}ê°œ\")\n",
    "    print(f\"  - ì´ ì‹œë„: {attempts}íšŒ\")\n",
    "    print(f\"  - ì„±ê³µë¥ : {len(generated_data)/attempts*100:.1f}%\")\n",
    "    print(f\"  - ì†Œìš” ì‹œê°„: {elapsed_total:.1f}ì´ˆ\")\n",
    "    print(f\"  - í‰ê·  ì†ë„: {len(generated_data)/elapsed_total:.2f}ê°œ/ì´ˆ\")\n",
    "    \n",
    "    print(\"\\në¬¸ì œ ìœ í˜•ë³„ ë¶„í¬:\")\n",
    "    for qtype, count in type_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {qtype}: {count}ê°œ ({count/len(generated_data)*100:.1f}%)\")\n",
    "    \n",
    "    # ë‹µë³€ í†µê³„\n",
    "    answer_lengths = [len(d['answer']) for d in generated_data]\n",
    "    quality_scores = [d.get('quality_score', 0) for d in generated_data]\n",
    "    \n",
    "    if answer_lengths:\n",
    "        print(f\"\\në‹µë³€ ê¸¸ì´ í†µê³„:\")\n",
    "        print(f\"  - í‰ê· : {np.mean(answer_lengths):.1f} ê¸€ì\")\n",
    "        print(f\"  - ìµœì†Œ: {np.min(answer_lengths)} ê¸€ì\")\n",
    "        print(f\"  - ìµœëŒ€: {np.max(answer_lengths)} ê¸€ì\")\n",
    "    \n",
    "    if quality_scores:\n",
    "        print(f\"\\ní’ˆì§ˆ ì ìˆ˜ í†µê³„:\")\n",
    "        print(f\"  - í‰ê· : {np.mean(quality_scores):.1f}/100\")\n",
    "        print(f\"  - ìµœì†Œ: {np.min(quality_scores):.0f}/100\")\n",
    "        print(f\"  - ìµœëŒ€: {np.max(quality_scores):.0f}/100\")\n",
    "    \n",
    "    # ìƒì„±ê¸° í†µê³„\n",
    "    gen_stats = generator.get_stats()\n",
    "    print(f\"\\nìƒì„±ê¸° í†µê³„:\")\n",
    "    print(f\"  - ë‹µë³€ í¬í•¨ë¥ : {gen_stats['answer_rate']:.1f}%\")\n",
    "    print(f\"  - ì¬ì‹œë„ìœ¨: {gen_stats['retry_rate']:.1f}%\")\n",
    "    print(f\"  - í´ë°± ì‚¬ìš©ë¥ : {gen_stats['fallback_rate']:.1f}%\")\n",
    "    \n",
    "    # 5. ë°ì´í„° ì €ì¥\n",
    "    if generated_data:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # JSON ì €ì¥\n",
    "        json_path = AUGMENTED_DIR / f\"fsku_data_{config.GENERATION_MODE}_{timestamp}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(generated_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # JSONL ì €ì¥\n",
    "        jsonl_path = AUGMENTED_DIR / f\"fsku_data_{config.GENERATION_MODE}_{timestamp}.jsonl\"\n",
    "        with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "            for item in generated_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ:\")\n",
    "        print(f\"  - JSON: {json_path}\")\n",
    "        print(f\"  - JSONL: {jsonl_path}\")\n",
    "        print(f\"  - ì´ {len(generated_data)}ê°œ í•­ëª©\")\n",
    "        \n",
    "        # í†µê³„ ì €ì¥\n",
    "        if config.EXPERIMENT_MODE['save_stats']:\n",
    "            stats_path = AUGMENTED_DIR / f\"stats_{config.GENERATION_MODE}_{timestamp}.json\"\n",
    "            stats_data = {\n",
    "                'config': {\n",
    "                    'model': config.MODEL_NAME,\n",
    "                    'mode': config.GENERATION_MODE,\n",
    "                    'temperature': config.GENERATION_PARAMS['temperature'],\n",
    "                    'quality_threshold': config.QUALITY_CONFIG['quality_threshold'],\n",
    "                },\n",
    "                'results': {\n",
    "                    'total_generated': len(generated_data),\n",
    "                    'total_attempts': attempts,\n",
    "                    'success_rate': len(generated_data)/attempts*100,\n",
    "                    'elapsed_time': elapsed_total,\n",
    "                    'type_distribution': type_counts,\n",
    "                    'answer_length_mean': np.mean(answer_lengths),\n",
    "                    'quality_score_mean': np.mean(quality_scores),\n",
    "                },\n",
    "                'generator_stats': gen_stats,\n",
    "                'timestamp': timestamp\n",
    "            }\n",
    "            \n",
    "            with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(stats_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"  - í†µê³„: {stats_path}\")\n",
    "    \n",
    "    print(\"\\nâœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
    "    return generated_data\n",
    "\n",
    "# ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰)\n",
    "# data = run_bulk_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ğŸ§ª ì‹¤í—˜ ì‹¤í–‰ ì„¹ì…˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ§ª ì‹¤í—˜ ì‹¤í–‰\n",
    "# ì•„ë˜ ì½”ë“œì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”\n",
    "# ========================================\n",
    "\n",
    "print(\"ğŸ§ª ì‹¤í—˜ ì˜µì…˜:\")\n",
    "print(\"1. ë‹¨ì¼ í…ŒìŠ¤íŠ¸: test_single_generation()\")\n",
    "print(\"2. ëª¨ë“œ ë¹„êµ: compare_generation_modes()\")\n",
    "print(\"3. ëŒ€ëŸ‰ ìƒì„±: run_bulk_generation()\")\n",
    "print(\"\\nì›í•˜ëŠ” ì‹¤í—˜ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”!\")\n",
    "\n",
    "# 1. ë‹¨ì¼ ìƒì„± í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)\n",
    "def test_single_generation():\n",
    "    \"\"\"ë‹¨ì¼ QA ìƒì„± í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"\\nğŸ”¬ ë‹¨ì¼ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸\n",
    "    test_context = \"\"\"ê¸ˆìœµë³´ì•ˆì›ì€ ê¸ˆìœµ IT ë³´ì•ˆì„ ë‹´ë‹¹í•˜ëŠ” ê¸°ê´€ìœ¼ë¡œ,\n",
    "    ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„± í™•ë³´ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ ë³´ì•ˆ ê¸°ìˆ ì„ ìš´ì˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "    generator = AnswerGuaranteedGenerator(config)\n",
    "    generator.load_model()\n",
    "    \n",
    "    # ìƒì„±\n",
    "    result = generator.generate_qa_pair(test_context, \"ì£¼ê´€ì‹\")\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nâœ… ìƒì„± ì„±ê³µ!\")\n",
    "        print(f\"ë¬¸ì œ: {result['question']}\")\n",
    "        print(f\"ë‹µë³€: {result['answer']}\")\n",
    "        print(f\"í’ˆì§ˆ: {result.get('quality_score', 0):.0f}/100\")\n",
    "        print(f\"ëª¨ë“œ: {result['generation_mode']}\")\n",
    "    else:\n",
    "        print(\"âŒ ìƒì„± ì‹¤íŒ¨\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# test_single_generation()\n",
    "\n",
    "# 2. í†µí•©í˜• vs ë¶„ë¦¬í˜• ë¹„êµ\n",
    "# compare_generation_modes()\n",
    "\n",
    "# 3. ëŒ€ëŸ‰ ìƒì„± ì‹¤í–‰\n",
    "# data = run_bulk_generation()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# ğŸš€ ê°œì„ ëœ RAG í™œìš© ë‹µë³€ ìƒì„±ê¸°\n# ========================================\n\nclass ImprovedAnswerGenerator:\n    \"\"\"\n    ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸°\n    - ìƒì„±ëœ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ RAG ì¬ê²€ìƒ‰\n    - ë” ì •í™•í•œ ë‹µë³€ ìƒì„±\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n        self.config = config\n        self.rag = rag_system\n        \n    def generate_answer_with_rag(self, question: str, original_context: str = None) -> str:\n        \"\"\"\n        ì§ˆë¬¸ ê¸°ë°˜ RAG ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±\n        \n        Args:\n            question: ìƒì„±ëœ ì§ˆë¬¸\n            original_context: ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )\n            \n        Returns:\n            ìƒì„±ëœ ë‹µë³€\n        \"\"\"\n        # 1. ì§ˆë¬¸ìœ¼ë¡œ RAG ê²€ìƒ‰\n        if self.rag:\n            print(f\"  ğŸ” ì§ˆë¬¸ ê¸°ë°˜ RAG ì¬ê²€ìƒ‰...\")\n            retrieved_docs = self.rag.search(question, top_k=5)\n            \n            # ê²€ìƒ‰ëœ ë¬¸ì„œ ê²°í•©\n            rag_context = \"\\n\\n\".join([\n                f\"[ì°¸ê³  {i+1}] {doc['text'][:300]}\"\n                for i, doc in enumerate(retrieved_docs[:3])\n            ])\n            \n            # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì™€ ê²°í•©\n            if original_context:\n                full_context = f\"\"\"ì›ë³¸ ë¬¸ì„œ:\n{original_context[:500]}\n\nê´€ë ¨ ì°¸ê³  ìë£Œ:\n{rag_context}\"\"\"\n            else:\n                full_context = rag_context\n                \n        else:\n            full_context = original_context or \"\"\n            \n        # 2. ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸\n        answer_prompt = f\"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ì°¸ê³  ìë£Œë¥¼ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n### ì°¸ê³  ìë£Œ:\n{full_context}\n\n### ì§ˆë¬¸:\n{question}\n\n### ë‹µë³€ ì‘ì„± ì§€ì¹¨:\n1. ì°¸ê³  ìë£Œì˜ ì •ë³´ë¥¼ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”\n2. ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”\n3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê·œì •ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”\n4. ë‹µë³€ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”\n5. í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì œì‹œí•˜ê³  ë¶€ê°€ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”\n\n### ë‹µë³€:\"\"\"\n        \n        return answer_prompt\n    \n    def separated_generation_improved(self, context: str, question_type: str) -> Dict:\n        \"\"\"\n        ê°œì„ ëœ ë¶„ë¦¬í˜• ìƒì„± (RAG í™œìš© ê°•í™”)\n        \"\"\"\n        # 1ë‹¨ê³„: ì§ˆë¬¸ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)\n        question_prompt = self.get_question_prompt(context, question_type)\n        question = self.generate_text(question_prompt)\n        \n        # 2ë‹¨ê³„: ì§ˆë¬¸ ê¸°ë°˜ RAG ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±\n        answer_prompt = self.generate_answer_with_rag(question, context)\n        answer = self.generate_text(answer_prompt)\n        \n        return {\n            'question': question,\n            'answer': answer,\n            'mode': 'separated_improved',\n            'rag_used': self.rag is not None\n        }\n\n# AnswerGuaranteedGenerator í´ë˜ìŠ¤ ê°œì„ \nclass AnswerGuaranteedGeneratorV2(AnswerGuaranteedGenerator):\n    \"\"\"\n    ê°œì„ ëœ ë‹µë³€ ë³´ì¥ ìƒì„±ê¸° (RAG í™œìš© ê°•í™”)\n    \"\"\"\n    \n    def generate_qa_pair(self, context: str, question_type: str = \"ì£¼ê´€ì‹\") -> Optional[Dict]:\n        \"\"\"\n        QA ìŒ ìƒì„± (ê°œì„ ëœ ë²„ì „)\n        \"\"\"\n        self.stats['total_attempts'] += 1\n        \n        try:\n            if self.generation_mode == \"integrated\":\n                # í†µí•©í˜•: ê¸°ì¡´ê³¼ ë™ì¼\n                return self._generate_integrated(context, question_type)\n                \n            elif self.generation_mode == \"separated\":\n                # ë¶„ë¦¬í˜•: RAG ê°œì„  ì ìš©\n                return self._generate_separated_improved(context, question_type)\n                \n            else:\n                raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {self.generation_mode}\")\n                \n        except Exception as e:\n            logger.error(f\"QA ìƒì„± ì˜¤ë¥˜: {e}\")\n            self.stats['failed'] += 1\n            return None\n    \n    def _generate_separated_improved(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        ê°œì„ ëœ ë¶„ë¦¬í˜• ìƒì„± (ì§ˆë¬¸ ê¸°ë°˜ RAG ì¬ê²€ìƒ‰)\n        \"\"\"\n        if self.config.EXPERIMENT_MODE['verbose']:\n            print(f\"  [ë¶„ë¦¬í˜•-ê°œì„ ] {question_type} ìƒì„±...\")\n            \n        # 1ë‹¨ê³„: ì§ˆë¬¸ ìƒì„±\n        if self.config.EXPERIMENT_MODE['verbose']:\n            print(\"    ğŸ“ ì§ˆë¬¸ ìƒì„± ì¤‘...\")\n            \n        question_prompt = self.prompts.get_separated_question_prompt(context, question_type)\n        question = self.generate_text(question_prompt, max_tokens=150)\n        \n        if not question or len(question.strip()) < 10:\n            return None\n            \n        # 2ë‹¨ê³„: ì§ˆë¬¸ ê¸°ë°˜ RAG ì¬ê²€ìƒ‰\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            if self.config.EXPERIMENT_MODE['verbose']:\n                print(f\"    ğŸ” ì§ˆë¬¸ ê¸°ë°˜ RAG ì¬ê²€ìƒ‰...\")\n                \n            # ì§ˆë¬¸ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n            retrieved_docs = self.rag.search(question, top_k=5)\n            \n            # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€\n            rag_context = \"\\n\\n\".join([\n                f\"[ì°¸ê³  {i+1}] {doc['text'][:400]}\"\n                for i, doc in enumerate(retrieved_docs[:3])\n            ])\n            \n            # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì™€ ê²°í•©\n            enhanced_context = f\"\"\"### ì›ë³¸ ë¬¸ì„œ:\n{context[:600]}\n\n### ì§ˆë¬¸ ê´€ë ¨ ì¶”ê°€ ì°¸ê³  ìë£Œ:\n{rag_context}\"\"\"\n        else:\n            enhanced_context = context\n            \n        # 3ë‹¨ê³„: ê°•í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„±\n        if self.config.EXPERIMENT_MODE['verbose']:\n            print(\"    ğŸ’¡ ë‹µë³€ ìƒì„± ì¤‘ (RAG ê°•í™”)...\")\n            \n        answer_prompt = f\"\"\"ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµê°ë…ì›ì˜ FSKU ì‹œí—˜ ì¶œì œìœ„ì›ì…ë‹ˆë‹¤.\në‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ì°¸ê³  ìë£Œë¥¼ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n{enhanced_context}\n\n### ì§ˆë¬¸:\n{question}\n\n### ë‹µë³€ ì‘ì„± ì§€ì¹¨:\n1. ì œê³µëœ ì°¸ê³  ìë£Œì˜ ì •ë³´ë¥¼ ì •í™•íˆ í™œìš©í•˜ì„¸ìš”\n2. ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ì™€ ìˆ˜ì¹˜ë¥¼ ì •í™•íˆ í¬í•¨í•˜ì„¸ìš”\n3. FSKU ì‹œí—˜ ë‹µì•ˆ ìˆ˜ì¤€ì˜ ì™„ì„±ë„ë¥¼ ê°–ì¶”ì„¸ìš”\n4. í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì €, ë¶€ê°€ ì„¤ëª…ì„ ë‚˜ì¤‘ì— ì œì‹œí•˜ì„¸ìš”\n5. ë‹µë³€ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”\n\n### ëª¨ë²” ë‹µì•ˆ:\"\"\"\n        \n        answer = self.generate_text(answer_prompt, max_tokens=300)\n        \n        # ë‹µë³€ ê²€ì¦\n        if not answer or len(answer.strip()) < self.config.QUALITY_CONFIG['min_answer_length']:\n            # í´ë°±: ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„\n            if self.config.EXPERIMENT_MODE['verbose']:\n                print(\"    âš ï¸ ë‹µë³€ ë¶€ì¡±, í´ë°± ì‹œë„...\")\n                \n            fallback_prompt = f\"ì§ˆë¬¸: {question}\\nì •ë‹µ:\"\n            answer = self.generate_text(fallback_prompt, max_tokens=200)\n            self.stats['fallback_used'] += 1\n            \n        # ê²°ê³¼ ìƒì„±\n        result = {\n            'question': question.strip(),\n            'answer': answer.strip(),\n            'context': context[:500],\n            'question_type': question_type,\n            'generation_mode': 'separated_improved',\n            'rag_enhanced': self.rag is not None,\n            'quality_score': self._calculate_quality_score(question, answer),\n            'model': self.config.MODEL_NAME,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        self.stats['successful'] += 1\n        self.stats['mode_stats']['separated'] += 1\n        if answer and len(answer.strip()) > 10:\n            self.stats['with_answer'] += 1\n        else:\n            self.stats['without_answer'] += 1\n            \n        return result\n\n# ChainOfThoughtGenerator ê°œì„ \nclass ChainOfThoughtGeneratorV2(ChainOfThoughtGenerator):\n    \"\"\"\n    ê°œì„ ëœ CoT ìƒì„±ê¸° (RAG í™œìš© ê°•í™”)\n    \"\"\"\n    \n    def _initial_generation(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        1ë‹¨ê³„: ì´ˆê¸° ìƒì„± (RAG í™œìš©)\n        \"\"\"\n        # RAGë¡œ ì»¨í…ìŠ¤íŠ¸ ë³´ê°•\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            # ì£¼ì œ ì¶”ì¶œì„ ìœ„í•œ ê°„ë‹¨í•œ ê²€ìƒ‰\n            query = f\"{question_type} ë¬¸ì œ ìƒì„±ì„ ìœ„í•œ {context[:100]}\"\n            retrieved = self.rag.search(query, top_k=3)\n            \n            if retrieved:\n                additional = \"\\n\\n\".join([\n                    f\"[ì°¸ê³ ] {doc['text'][:300]}\"\n                    for doc in retrieved[:2]\n                ])\n                enhanced_context = f\"{context}\\n\\n### ì¶”ê°€ ì°¸ê³  ìë£Œ:\\n{additional}\"\n            else:\n                enhanced_context = context\n        else:\n            enhanced_context = context\n            \n        prompt = self.cot_prompts['initial_generation'].format(\n            context=enhanced_context[:1000],\n            question_type=question_type\n        )\n        \n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_initial', 0.7)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        return self._parse_qa(generated)\n    \n    def _improvement_generation(self, question: str, answer: str, feedback: str) -> Optional[Dict]:\n        \"\"\"\n        3ë‹¨ê³„: ê°œì„  ìƒì„± (ì§ˆë¬¸ ê¸°ë°˜ RAG ì¬ê²€ìƒ‰)\n        \"\"\"\n        # ìƒì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ RAG ì¬ê²€ìƒ‰\n        enhanced_context = \"\"\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            # ì§ˆë¬¸ìœ¼ë¡œ ë” ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰\n            retrieved = self.rag.search(question, top_k=5)\n            \n            if retrieved:\n                enhanced_context = \"\\n\\n### ì§ˆë¬¸ ê´€ë ¨ ì°¸ê³  ìë£Œ:\\n\" + \"\\n\".join([\n                    f\"- {doc['text'][:200]}\"\n                    for doc in retrieved[:3]\n                ])\n        \n        prompt = f\"\"\"{self.cot_prompts['improvement']}\n\n{enhanced_context}\"\"\".format(\n            question=question,\n            answer=answer,\n            feedback=feedback\n        )\n        \n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_improvement', 0.5)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        return self._parse_qa(generated)\n\nprint(\"âœ… RAG í™œìš© ê°œì„  ì™„ë£Œ\\!\")\nprint(\"\\nğŸš€ ê°œì„ ëœ ê¸°ëŠ¥:\")\nprint(\"  1. ë¶„ë¦¬í˜•: ì§ˆë¬¸ ìƒì„± â†’ ì§ˆë¬¸ìœ¼ë¡œ RAG ì¬ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„±\")\nprint(\"  2. CoT: ê° ë‹¨ê³„ì—ì„œ RAG í™œìš© ê°•í™”\")\nprint(\"  3. ë‹µë³€ ìƒì„± ì‹œ ì§ˆë¬¸ ê´€ë ¨ ë¬¸ì„œ ìš°ì„  ì°¸ì¡°\")\nprint(\"\\nğŸ’¡ ì‚¬ìš©ë²•:\")\nprint(\"  - config.GENERATION_MODE = 'separated' ì„¤ì • í›„ ì‹¤í–‰\")\nprint(\"  - ìë™ìœ¼ë¡œ ê°œì„ ëœ RAG í™œìš© ì ìš©ë¨\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# ğŸ§ª CoT í†µí•© í…ŒìŠ¤íŠ¸ ë° ëª¨ë“œ ë¹„êµ\n# ========================================\n\ndef test_cot_generation():\n    \"\"\"\n    CoT ëª¨ë“œ ë‹¨ì¼ í…ŒìŠ¤íŠ¸\n    \"\"\"\n    print(\"=\"*80)\n    print(\"ğŸ§  Chain-of-Thought (CoT) í…ŒìŠ¤íŠ¸\")\n    print(\"=\"*80)\n    \n    # CoT ì„¤ì •\n    test_config = ExperimentConfig()\n    test_config.GENERATION_MODE = \"cot\"\n    test_config.COT_CONFIG['use_cot'] = True\n    test_config.EXPERIMENT_MODE['verbose'] = True\n    \n    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì„ íƒì )\n    rag = None\n    if test_config.RAG_CONFIG['use_rag']:\n        print(\"\\nğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...\")\n        rag = RAGSystem(test_config)\n        rag.initialize()\n    \n    # CoT ìƒì„±ê¸° ìƒì„±\n    print(\"\\nğŸ§  CoT ìƒì„±ê¸° ì´ˆê¸°í™”...\")\n    cot_gen = ChainOfThoughtGenerator(test_config, rag)\n    cot_gen.load_model()\n    \n    # í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸\n    test_context = \"\"\"\n    ë°”ì ¤III ê·œì œëŠ” ê¸€ë¡œë²Œ ê¸ˆìœµìœ„ê¸° ì´í›„ ì€í–‰ì˜ ìë³¸ ì ì •ì„±ê³¼ ìœ ë™ì„± ê´€ë¦¬ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•´ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.\n    ì£¼ìš” ë‚´ìš©ìœ¼ë¡œëŠ” ë³´í†µì£¼ìë³¸ë¹„ìœ¨ 4.5%, Tier1 ìë³¸ë¹„ìœ¨ 6%, ì´ìë³¸ë¹„ìœ¨ 8% ì´ìƒ ìœ ì§€,\n    ìë³¸ë³´ì „ì™„ì¶©ìë³¸ 2.5% ì¶”ê°€, ê²½ê¸°ëŒ€ì‘ì™„ì¶©ìë³¸ 0~2.5% íƒ„ë ¥ ìš´ì˜ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n    ë˜í•œ ë ˆë²„ë¦¬ì§€ ë¹„ìœ¨ 3% ì´ìƒ, ìœ ë™ì„± ì»¤ë²„ë¦¬ì§€ ë¹„ìœ¨(LCR) 100% ì´ìƒ,\n    ìˆœì•ˆì •ìê¸ˆì¡°ë‹¬ë¹„ìœ¨(NSFR) 100% ì´ìƒì„ ìš”êµ¬í•©ë‹ˆë‹¤.\n    \"\"\"\n    \n    print(\"\\nğŸ”¬ CoT 4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...\")\n    print(\"  1ï¸âƒ£ ì´ˆê¸° ìƒì„±\")\n    print(\"  2ï¸âƒ£ ìê°€ ê²€ì¦\")\n    print(\"  3ï¸âƒ£ ê°œì„  ìƒì„±\")\n    print(\"  4ï¸âƒ£ ìµœì¢… ê²€ì¦\")\n    \n    # CoT ìƒì„± ì‹¤í–‰\n    result = cot_gen.generate_qa_pair(test_context, \"ì£¼ê´€ì‹\")\n    \n    if result:\n        print(\"\\nâœ… CoT ìƒì„± ì„±ê³µ\\!\")\n        print(f\"\\nğŸ“ ìƒì„±ëœ ë¬¸ì œ:\")\n        print(f\"  {result['question'][:200]}...\")\n        print(f\"\\nğŸ’¡ ìƒì„±ëœ ë‹µë³€:\")\n        print(f\"  {result['answer'][:200]}...\")\n        print(f\"\\nğŸ“Š í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0)}/100\")\n        print(f\"ğŸ”„ ê°œì„  ë°˜ë³µ íšŸìˆ˜: {result.get('iterations', 0)}íšŒ\")\n        \n        # í†µê³„ ì¶œë ¥\n        stats = cot_gen.get_stats()\n        print(f\"\\nğŸ“ˆ CoT í†µê³„:\")\n        print(f\"  - ì„±ê³µë¥ : {stats.get('success_rate', 0):.1f}%\")\n        print(f\"  - ê°œì„ ë¥ : {stats.get('improvement_rate', 0):.1f}%\")\n        print(f\"  - í‰ê·  í’ˆì§ˆ: {stats.get('avg_quality_score', 0):.1f}\")\n        print(f\"  - ìºì‹œ ì ì¤‘ë¥ : {stats.get('cache_hit_rate', 0):.1f}%\")\n    else:\n        print(\"\\nâŒ CoT ìƒì„± ì‹¤íŒ¨\")\n    \n    return result\n\ndef compare_all_modes():\n    \"\"\"\n    í†µí•©í˜• vs ë¶„ë¦¬í˜• vs CoT ì „ì²´ ë¹„êµ\n    \"\"\"\n    print(\"=\"*80)\n    print(\"ğŸ”¬ ì „ì²´ ëª¨ë“œ ë¹„êµ í…ŒìŠ¤íŠ¸\")\n    print(\"=\"*80)\n    \n    # í…ŒìŠ¤íŠ¸ ì„¤ì •\n    test_contexts = [\n        \"\"\"ê¸ˆìœµë³´ì•ˆì›(FSI)ì€ êµ­ë‚´ ê¸ˆìœµ IT ë³´ì•ˆì„ ì´ê´„í•˜ëŠ” ì „ë¬¸ê¸°ê´€ìœ¼ë¡œ,\n        ê¸ˆìœµê¶Œ ì‚¬ì´ë²„ ë³´ì•ˆ ê°•í™”ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ì •ì±…ê³¼ ê¸°ìˆ ì„ ê°œë°œí•©ë‹ˆë‹¤.\"\"\",\n        \n        \"\"\"íŒŒìƒìƒí’ˆì€ ê¸°ì´ˆìì‚°ì˜ ê°€ê²© ë³€ë™ì— ë”°ë¼ ê°€ì¹˜ê°€ ê²°ì •ë˜ëŠ” ê¸ˆìœµìƒí’ˆìœ¼ë¡œ,\n        ì„ ë¬¼, ì˜µì…˜, ìŠ¤ì™‘ ë“±ì´ ëŒ€í‘œì ì…ë‹ˆë‹¤. ìœ„í—˜ í—¤ì§€ì™€ íˆ¬ê¸° ëª©ì ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤.\"\"\"\n    ]\n    \n    modes = [\"integrated\", \"separated\", \"cot\"]\n    results = {mode: [] for mode in modes}\n    \n    for mode in modes:\n        print(f\"\\n{'='*40}\")\n        print(f\"ğŸ“‹ {mode.upper()} ëª¨ë“œ í…ŒìŠ¤íŠ¸\")\n        print(f\"{'='*40}\")\n        \n        # ì„¤ì • ìƒì„±\n        config = ExperimentConfig()\n        config.GENERATION_MODE = mode\n        if mode == \"cot\":\n            config.COT_CONFIG['use_cot'] = True\n            config.CURRENT_COT_PRESET = \"balanced\"\n        \n        # ìƒì„±ê¸° ìƒì„±\n        if mode == \"cot\":\n            generator = ChainOfThoughtGenerator(config)\n        else:\n            generator = AnswerGuaranteedGenerator(config)\n        \n        generator.load_model()\n        \n        # ê° ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸\n        for i, context in enumerate(test_contexts):\n            print(f\"\\ní…ŒìŠ¤íŠ¸ {i+1}:\")\n            start_time = time.time()\n            \n            result = generator.generate_qa_pair(context, \"ì£¼ê´€ì‹\")\n            elapsed = time.time() - start_time\n            \n            if result:\n                results[mode].append({\n                    'result': result,\n                    'time': elapsed,\n                    'quality': result.get('quality_score', 70)\n                })\n                print(f\"  âœ… ì„±ê³µ ({elapsed:.2f}ì´ˆ)\")\n                print(f\"  í’ˆì§ˆ: {result.get('quality_score', 70)}/100\")\n            else:\n                print(f\"  âŒ ì‹¤íŒ¨\")\n    \n    # ê²°ê³¼ ë¹„êµ\n    print(f\"\\n{'='*80}\")\n    print(\"ğŸ“Š ë¹„êµ ê²°ê³¼\")\n    print(f\"{'='*80}\")\n    \n    for mode in modes:\n        if results[mode]:\n            avg_time = np.mean([r['time'] for r in results[mode]])\n            avg_quality = np.mean([r['quality'] for r in results[mode]])\n            success_rate = len(results[mode]) / len(test_contexts) * 100\n            \n            print(f\"\\n{mode.upper()} ëª¨ë“œ:\")\n            print(f\"  - ì„±ê³µë¥ : {success_rate:.0f}%\")\n            print(f\"  - í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ\")\n            print(f\"  - í‰ê·  í’ˆì§ˆ: {avg_quality:.1f}/100\")\n            \n            # ëª¨ë“œë³„ íŠ¹ì§•\n            if mode == \"integrated\":\n                print(\"  - íŠ¹ì§•: ë¹ ë¥´ê³  ì¼ê´€ì„± ìˆëŠ” ìƒì„±\")\n            elif mode == \"separated\":\n                print(\"  - íŠ¹ì§•: ì •í™•í•œ ë‹µë³€, ëŠë¦° ì†ë„\")\n            elif mode == \"cot\":\n                print(\"  - íŠ¹ì§•: ìµœê³  í’ˆì§ˆ, 4ë‹¨ê³„ ê²€ì¦\")\n    \n    return results\n\ndef run_cot_experiments():\n    \"\"\"\n    CoT ì‹¤í—˜ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸\n    \"\"\"\n    print(\"=\"*80)\n    print(\"ğŸ”¬ CoT ì‹¤í—˜ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸\")\n    print(\"=\"*80)\n    \n    # í…ŒìŠ¤íŠ¸í•  í”„ë¦¬ì…‹ë“¤\n    presets = [\"fast\", \"balanced\", \"quality\"]\n    preset_results = {}\n    \n    test_context = \"\"\"\n    ì¤‘ì•™ì€í–‰ì˜ í†µí™”ì •ì±…ì€ ê²½ì œ ì•ˆì •í™”ë¥¼ ìœ„í•œ í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤.\n    ê¸ˆë¦¬ ì¡°ì ˆ, ê³µê°œì‹œì¥ì¡°ì‘, ì§€ê¸‰ì¤€ë¹„ìœ¨ ì¡°ì • ë“±ì„ í†µí•´\n    ë¬¼ê°€ ì•ˆì •ê³¼ ì™„ì „ ê³ ìš©ì„ ì¶”êµ¬í•©ë‹ˆë‹¤.\n    \"\"\"\n    \n    for preset in presets:\n        print(f\"\\nğŸ§ª í”„ë¦¬ì…‹ í…ŒìŠ¤íŠ¸: {preset}\")\n        print(\"-\"*40)\n        \n        # ì„¤ì •\n        config = ExperimentConfig()\n        config.GENERATION_MODE = \"cot\"\n        config.CURRENT_COT_PRESET = preset\n        \n        # í”„ë¦¬ì…‹ ì ìš©\n        if preset in config.COT_PRESETS:\n            for key, value in config.COT_PRESETS[preset].items():\n                if key in config.COT_CONFIG:\n                    config.COT_CONFIG[key] = value\n        \n        print(f\"  ì„¤ì •:\")\n        print(f\"    - ìµœëŒ€ ë°˜ë³µ: {config.COT_CONFIG['max_iterations']}\")\n        print(f\"    - í’ˆì§ˆ ì„ê³„ê°’: {config.COT_CONFIG['quality_threshold']}\")\n        print(f\"    - ê°œì„  ì‚¬ìš©: {config.COT_CONFIG['use_improvement']}\")\n        \n        # ìƒì„±ê¸° ìƒì„±\n        cot_gen = ChainOfThoughtGenerator(config)\n        cot_gen.load_model()\n        \n        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n        start_time = time.time()\n        result = cot_gen.generate_qa_pair(test_context, \"ì„œìˆ í˜•\")\n        elapsed = time.time() - start_time\n        \n        if result:\n            preset_results[preset] = {\n                'time': elapsed,\n                'quality': result.get('quality_score', 0),\n                'iterations': result.get('iterations', 0)\n            }\n            \n            print(f\"\\n  ê²°ê³¼:\")\n            print(f\"    - ì‹œê°„: {elapsed:.2f}ì´ˆ\")\n            print(f\"    - í’ˆì§ˆ: {result.get('quality_score', 0)}/100\")\n            print(f\"    - ë°˜ë³µ: {result.get('iterations', 0)}íšŒ\")\n    \n    # í”„ë¦¬ì…‹ ë¹„êµ\n    print(f\"\\n{'='*80}\")\n    print(\"ğŸ“Š í”„ë¦¬ì…‹ ë¹„êµ ê²°ê³¼\")\n    print(f\"{'='*80}\")\n    \n    print(f\"\\n{'í”„ë¦¬ì…‹':<12} {'ì‹œê°„(ì´ˆ)':<10} {'í’ˆì§ˆ':<10} {'ë°˜ë³µ':<10}\")\n    print(\"-\"*42)\n    for preset, data in preset_results.items():\n        print(f\"{preset:<12} {data['time']:<10.2f} {data['quality']:<10.1f} {data['iterations']:<10}\")\n    \n    # ê¶Œì¥ì‚¬í•­\n    print(f\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n    print(\"  - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘: 'fast' í”„ë¦¬ì…‹\")\n    print(\"  - ì¼ë°˜ ì‚¬ìš©: 'balanced' í”„ë¦¬ì…‹\")\n    print(\"  - ìµœê³  í’ˆì§ˆ: 'quality' í”„ë¦¬ì…‹\")\n    print(\"  - ì—°êµ¬/ë…¼ë¬¸: 'research' í”„ë¦¬ì…‹ (ë³„ë„ ì„¤ì •)\")\n    \n    return preset_results\n\n# ì‹¤í–‰ ë©”ë‰´\ndef show_cot_menu():\n    \"\"\"\n    CoT ì‹¤í—˜ ë©”ë‰´\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ§  Chain-of-Thought (CoT) ì‹¤í—˜ ë©”ë‰´\")\n    print(\"=\"*80)\n    print(\"\\nì‹¤í–‰í•  ì‹¤í—˜ì„ ì„ íƒí•˜ì„¸ìš”:\")\n    print(\"\\n1. test_cot_generation()\")\n    print(\"   - CoT ë‹¨ì¼ í…ŒìŠ¤íŠ¸\")\n    print(\"\\n2. compare_all_modes()\")\n    print(\"   - í†µí•©í˜• vs ë¶„ë¦¬í˜• vs CoT ë¹„êµ\")\n    print(\"\\n3. run_cot_experiments()\")\n    print(\"   - CoT í”„ë¦¬ì…‹ ì‹¤í—˜\")\n    print(\"\\n4. run_bulk_generation()\")\n    print(\"   - ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜)\")\n    print(\"\\nì˜ˆì‹œ: test_cot_generation()\")\n\nshow_cot_menu()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ğŸ“ ì‚¬ìš© ê°€ì´ë“œ ë° íŒ\n",
    "\n",
    "### ì‹¤í—˜ ê°€ëŠ¥í•œ ì„¤ì •ë“¤\n",
    "\n",
    "1. **ìƒì„± ëª¨ë“œ ë³€ê²½**\n",
    "```python\n",
    "config.GENERATION_MODE = \"integrated\"  # ë˜ëŠ” \"separated\"\n",
    "```\n",
    "\n",
    "2. **Temperature ì¡°ì •** (ì°½ì˜ì„±)\n",
    "```python\n",
    "config.GENERATION_PARAMS['temperature'] = 0.3  # ë³´ìˆ˜ì \n",
    "config.GENERATION_PARAMS['temperature'] = 0.8  # ê· í˜•\n",
    "config.GENERATION_PARAMS['temperature'] = 1.0  # ì°½ì˜ì \n",
    "```\n",
    "\n",
    "3. **í’ˆì§ˆ ì„ê³„ê°’ ë³€ê²½**\n",
    "```python\n",
    "config.QUALITY_CONFIG['quality_threshold'] = 50  # ë‚®ì€ ê¸°ì¤€\n",
    "config.QUALITY_CONFIG['quality_threshold'] = 70  # ì¤‘ê°„\n",
    "config.QUALITY_CONFIG['quality_threshold'] = 90  # ë†’ì€ ê¸°ì¤€\n",
    "```\n",
    "\n",
    "4. **ëª¨ë¸ ë³€ê²½**\n",
    "```python\n",
    "config.MODEL_NAME = \"upstage/SOLAR-10.7B-v1.0\"  # ë” í° ëª¨ë¸\n",
    "```\n",
    "\n",
    "5. **RAG í™œì„±í™”/ë¹„í™œì„±í™”**\n",
    "```python\n",
    "config.RAG_CONFIG['use_rag'] = False  # RAG ì—†ì´\n",
    "```\n",
    "\n",
    "### ì„±ëŠ¥ ìµœì í™” íŒ\n",
    "\n",
    "1. **ì†ë„ ìš°ì„ **\n",
    "   - í†µí•©í˜• ëª¨ë“œ ì‚¬ìš©\n",
    "   - Temperature ë‚®ê²Œ (0.3~0.5)\n",
    "   - max_new_tokens ì¤„ì´ê¸° (200~300)\n",
    "   - RAG ë¹„í™œì„±í™”\n",
    "\n",
    "2. **í’ˆì§ˆ ìš°ì„ **\n",
    "   - ë¶„ë¦¬í˜• ëª¨ë“œ ì‚¬ìš©\n",
    "   - Temperature ì¤‘ê°„ (0.7~0.8)\n",
    "   - í’ˆì§ˆ ì„ê³„ê°’ ë†’ê²Œ (80+)\n",
    "   - RAG í™œì„±í™”\n",
    "\n",
    "3. **ë©”ëª¨ë¦¬ ì ˆì•½**\n",
    "   - ì–‘ìí™” í™œì„±í™”\n",
    "   - ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°\n",
    "   - ì‘ì€ ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "### ë¬¸ì œ í•´ê²°\n",
    "\n",
    "1. **ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì„ ë•Œ**\n",
    "   - ë¶„ë¦¬í˜• ëª¨ë“œ ì‹œë„\n",
    "   - Temperature ë†’ì´ê¸°\n",
    "   - ì¬ì‹œë„ íšŸìˆ˜ ëŠ˜ë¦¬ê¸°\n",
    "\n",
    "2. **í’ˆì§ˆì´ ë‚®ì„ ë•Œ**\n",
    "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì„ 'expert'ë¡œ ë³€ê²½\n",
    "   - ë” í° ëª¨ë¸ ì‚¬ìš©\n",
    "   - RAG í™œì„±í™”\n",
    "\n",
    "3. **ì†ë„ê°€ ëŠë¦´ ë•Œ**\n",
    "   - í†µí•©í˜• ëª¨ë“œ ì‚¬ìš©\n",
    "   - max_new_tokens ì¤„ì´ê¸°\n",
    "   - num_beams = 1ë¡œ ì„¤ì •"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}