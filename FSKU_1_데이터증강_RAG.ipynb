{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSKU 데이터 증강 및 RAG 시스템 (통합본)\n",
    "## 답변 생성 보장 + 통합형/분리형 선택 가능\n",
    "\n",
    "### 🎯 주요 특징\n",
    "1. **통합형 생성**: 질문과 답변을 한 번에 생성 (빠름, 일관성)\n",
    "2. **분리형 생성**: 질문 생성 후 답변 생성 (정확함, 유연함)\n",
    "3. **답변 보장**: 3단계 폴백 시스템으로 답변 생성 보장\n",
    "4. **RAG 통합**: 문서 기반 정확한 답변 생성\n",
    "5. **실험 가능한 설정**: 모든 파라미터 조정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🗺️ 코드 실행 흐름 가이드 (백엔드 개발자를 위한)\n\n## 🎯 이 노트북의 목적\n금융 AI 모델을 학습시키기 위한 **고품질 데이터를 자동으로 생성**하는 파이프라인 구축\n\n## 📋 실행 순서와 각 단계의 역할\n\n### 1️⃣ **환경 설정** (Setup)\n```\n백엔드 비유: 서버 초기화 및 의존성 주입\n```\n- GPU 설정 = 서버 리소스 할당\n- 라이브러리 임포트 = npm install / pip install\n- 디렉토리 구조 = 프로젝트 스캐폴딩\n\n### 2️⃣ **설정 클래스** (Configuration) \n```\n백엔드 비유: application.yml / config.json\n```\n- ExperimentConfig = 환경변수 관리\n- 하이퍼파라미터 = API rate limit, timeout 설정\n- 프리셋 = dev/staging/prod 환경 설정\n\n### 3️⃣ **RAG 시스템** (Knowledge Base)\n```\n백엔드 비유: 외부 DB 연결 및 캐싱 레이어\n```\n- PDF 로드 = 데이터베이스 연결\n- 벡터 인덱싱 = Elasticsearch 인덱싱  \n- 유사도 검색 = Full-text search\n- 캐싱 = Redis 캐싱\n\n### 4️⃣ **프롬프트 템플릿** (Templates)\n```\n백엔드 비유: API 요청/응답 스키마 정의\n```\n- 프롬프트 = Request DTO\n- 생성된 텍스트 = Response DTO\n- 파싱 = Serialization/Deserialization\n\n### 5️⃣ **데이터 생성기** (Generator)\n```\n백엔드 비유: 비즈니스 로직 레이어\n```\n- 모델 로드 = 서비스 초기화\n- generate_text() = 핵심 비즈니스 로직\n- 통합형/분리형/CoT = 다른 알고리즘 전략 패턴\n\n### 6️⃣ **품질 검증** (Validation)\n```\n백엔드 비유: 유효성 검사 및 에러 핸들링\n```\n- 품질 점수 = Validation rules\n- 재시도 로직 = Retry mechanism\n- 폴백 = Circuit breaker pattern\n\n### 7️⃣ **배치 처리** (Batch Processing)\n```\n백엔드 비유: 배치 잡 실행\n```\n- 대량 생성 = Batch job\n- 진행상황 추적 = Job monitoring\n- 중간 저장 = Checkpointing\n\n## 💡 핵심 실행 경로\n\n```python\n# 메인 실행 흐름\n1. config = ExperimentConfig()          # 설정 로드\n2. rag = RAGSystem(config)              # RAG 초기화\n3. rag.initialize()                     # 인덱스 구축/로드\n4. generator = AnswerGuaranteedGenerator(config, rag)  # 생성기 생성\n5. generator.load_model()               # 모델 로드\n6. for context in contexts:\n      qa_pair = generator.generate_qa_pair(context)  # 데이터 생성\n7. save_results(qa_pairs)              # 결과 저장\n```\n\n## 🔄 각 모드별 내부 흐름\n\n### 통합형 (Integrated)\n```\nContext → LLM → Q&A 동시 생성 → 파싱 → 검증 → 저장\n```\n\n### 분리형 (Separated) - 더 정확\\!\n```\nContext → LLM → 질문 생성\n    ↓\n질문으로 RAG 재검색\n    ↓\nContext + RAG 결과 → LLM → 답변 생성\n    ↓\n검증 → 저장\n```\n\n### CoT (Chain-of-Thought) - 최고 품질\\!\n```\n초기 생성 → 자가 검증 → 개선 → 최종 검증\n   ↓           ↓           ↓         ↓\n  70점?      문제 발견    수정      85점?\n```\n\n## 🎮 실전 사용법\n\n```python\n# 1. 빠른 테스트 (5분)\nconfig.GENERATION_MODE = \"integrated\"\nconfig.BATCH_CONFIG['target_count'] = 10\n\n# 2. 품질 우선 (30분)\nconfig.GENERATION_MODE = \"cot\"\nconfig.COT_CONFIG['use_cot'] = True\nconfig.CURRENT_COT_PRESET = \"quality\"\n\n# 3. 대회 제출용 (2시간)\nconfig.GENERATION_MODE = \"separated\"\nconfig.BATCH_CONFIG['target_count'] = 1000\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🎓 백엔드 개발자를 위한 AI 핵심 개념\n\n## 🤖 LLM (Large Language Model)\n```\n백엔드 비유: 초거대 함수\n- 입력: 텍스트 (Request)\n- 처리: 수십억 개 파라미터로 계산\n- 출력: 텍스트 (Response)\n```\n\n### 주요 개념 매핑\n| AI 용어 | 백엔드 용어 | 설명 |\n|---------|------------|------|\n| Model | Service/Engine | 실제 처리를 담당하는 핵심 컴포넌트 |\n| Tokenizer | Parser/Serializer | 텍스트 ↔ 숫자 변환 |\n| Inference | API Call | 모델에 요청 보내고 응답 받기 |\n| Fine-tuning | Customization | 특정 도메인에 맞게 커스터마이징 |\n| Prompt | Request Body | 모델에 보내는 입력 |\n| Temperature | Randomness Config | 응답의 다양성 조절 (0=결정적, 1=창의적) |\n| Batch Size | Connection Pool Size | 동시 처리 개수 |\n| Learning Rate | Update Speed | 학습 속도 (너무 빠르면 불안정) |\n\n## 🧮 양자화 (Quantization)\n```python\n# 백엔드 비유: 데이터 압축\n# 원본: {\"price\": 123.456789} (float64)\n# 압축: {\"price\": 123.46} (float16)\n# 더 압축: {\"price\": 123} (int8)\n\n# AI에서의 양자화\n원본 모델: 28GB (FP32)\n↓ 양자화\n압축 모델: 3.5GB (INT4)\n# 메모리 87.5% 절약\\!\n```\n\n## 🔗 LoRA (Low-Rank Adaptation)\n```python\n# 백엔드 비유: 어댑터 패턴\n\nclass OriginalService:  # 거대한 원본 모델 (수정 불가)\n    def process(self, input):\n        return expensive_computation(input)\n\nclass LoRAAdapter:  # 작은 어댑터 (학습 가능)\n    def __init__(self, rank=16):  # rank = 어댑터 크기\n        self.adapter_weights = small_matrix(rank)\n    \n    def process(self, input):\n        original_output = OriginalService.process(input)\n        adapter_output = self.adapter_weights @ input\n        return original_output + adapter_output  # 원본 + 어댑터\n\n# 장점: 원본은 그대로, 어댑터만 학습 → 메모리 99% 절약\\!\n```\n\n## 📚 RAG (Retrieval-Augmented Generation)\n```python\n# 백엔드 비유: 캐시 + 외부 API 패턴\n\ndef generate_answer(question):\n    # 1. 캐시(DB) 검색\n    relevant_docs = search_database(question)  # SELECT * FROM docs WHERE ...\n    \n    # 2. 컨텍스트 보강\n    context = f\"{question}\\n관련 정보: {relevant_docs}\"\n    \n    # 3. LLM 호출\n    answer = llm.generate(context)  # 외부 API 호출처럼\n    \n    return answer\n\n# RAG 없이: \"바젤III가 뭐야?\" → LLM이 학습한 내용만으로 답변\n# RAG 있으면: \"바젤III가 뭐야?\" → DB 검색 → 최신 정보 포함해서 답변\n```\n\n## 🔄 Fine-tuning 프로세스\n```python\n# 백엔드 비유: 점진적 배포 (Canary Deployment)\n\n# 1. Pre-trained Model (기본 모델)\nbase_model = load_model(\"gpt-base\")  # npm install express\n\n# 2. Add Custom Layer (커스텀 레이어)\ncustom_layer = LoRAAdapter()  # 우리 비즈니스 로직\n\n# 3. Training Loop (학습 루프)\nfor epoch in range(3):  # 3번 반복\n    for batch in training_data:  # 배치 단위 처리\n        loss = compute_loss(batch)  # 에러 계산\n        update_weights(loss)  # 가중치 업데이트\n        \n        # 백엔드의 모니터링처럼\n        if step % 100 == 0:\n            log_metrics(loss, accuracy)\n            save_checkpoint()  # 중간 저장\n```\n\n## 💾 메모리 관리 전략\n```python\n# RTX 4090 (24GB) 기준\n\n# ❌ 나쁜 예: OOM (Out of Memory)\nmodel = load_model(\"70B-model\")  # 70B = 280GB 필요\\!\n\n# ✅ 좋은 예: 메모리 최적화\nmodel = load_model(\"7B-model\", quantization=\"4bit\")  # 3.5GB만 사용\noptimizer = \"paged_adamw\"  # 페이징으로 메모리 절약\ngradient_checkpointing = True  # 메모리 ↔ 속도 트레이드오프\n\n# 백엔드 비유: \n# - Quantization = Response 압축 (gzip)\n# - Gradient Checkpointing = Lazy Loading\n# - Paged Optimizer = Swap 메모리 활용\n```\n\n## 🎯 하이퍼파라미터 튜닝 가이드\n```python\n# 백엔드의 성능 튜닝과 유사\n\n# 1. Learning Rate (처리 속도)\n# - 너무 높음 = 429 Too Many Requests (발산)\n# - 너무 낮음 = 408 Request Timeout (학습 안됨)\n# - 적절함 = 200 OK\n\n# 2. Batch Size (동시 요청 수)\n# - 너무 큼 = 503 Service Unavailable (OOM)\n# - 너무 작음 = 비효율적 (느림)\n# - 적절함 = Thread Pool Size처럼 조절\n\n# 3. Temperature (응답 다양성)\n# - 0.1 = Deterministic (항상 같은 응답)\n# - 0.7 = Balanced (적당한 변화)\n# - 1.5 = Creative (예측 불가능)\n```"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# 🎓 AI 개발 실습: 환경 설정 및 라이브러리 임포트\n# ========================================\n# \n# 💡 이 섹션에서 배우게 될 내용:\n#   1. AI 개발에 필요한 핵심 라이브러리들의 역할\n#   2. GPU 메모리 관리 방법\n#   3. 프로젝트 구조 설정 방법\n\nimport os\nimport json\nimport time\nimport pickle\nimport warnings\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm  # 진행 상황을 보여주는 프로그레스 바 라이브러리\n\n# 🔍 딥러닝 프레임워크 임포트\nimport torch  # PyTorch: 딥러닝의 핵심 프레임워크\nimport torch.nn as nn  # 신경망 모듈들 (실제로는 Transformer 모델이 이미 구현되어 있어 직접 사용할 일은 적음)\n\n# 🤗 Hugging Face 라이브러리들 - LLM 사용의 핵심\nfrom transformers import (\n    AutoModelForCausalLM,     # 자동으로 모델을 로드하는 클래스 (GPT 스타일 모델용)\n    AutoTokenizer,            # 텍스트를 토큰으로 변환하는 도구\n    BitsAndBytesConfig,       # 양자화(Quantization) 설정 - 메모리 절약의 핵심\\!\n)\n\n# 🔧 PEFT (Parameter-Efficient Fine-Tuning) - LoRA의 핵심\nfrom peft import (\n    LoraConfig,               # LoRA 설정 클래스\n    get_peft_model,          # 일반 모델을 LoRA 모델로 변환\n    prepare_model_for_kbit_training,  # 양자화된 모델을 학습 가능하게 만듦\n    TaskType                  # 작업 유형 정의 (예: 언어 생성)\n)\n\n# 📚 RAG (Retrieval-Augmented Generation) 관련\nfrom sentence_transformers import SentenceTransformer  # 문장을 벡터로 변환 (임베딩)\nimport faiss  # Facebook의 벡터 검색 라이브러리 (매우 빠름\\!)\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter  # 문서를 청크로 나누기\nimport PyPDF2  # PDF 파일 읽기\n\n# ⚠️ 경고 메시지 숨기기 (개발 시에는 주석 처리하는 것이 좋음)\nwarnings.filterwarnings('ignore')\n\n# 📂 프로젝트 디렉토리 구조 설정\n# 💡 Path 객체를 사용하면 OS에 관계없이 경로를 다룰 수 있음\nBASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"data\"\nEXTERNAL_DIR = DATA_DIR / \"external\"  # 외부 문서 (PDF, Excel 등)\nAUGMENTED_DIR = DATA_DIR / \"augmented\"  # 생성된 데이터\nVECTORDB_DIR = DATA_DIR / \"vectordb\"  # RAG 인덱스 저장\nMODELS_DIR = BASE_DIR / \"models\"  # 학습된 모델 저장\nRESULTS_DIR = BASE_DIR / \"results\"  # 실행 결과\n\n# 디렉토리 생성 (exist_ok=True: 이미 있어도 에러 안 남)\nfor dir_path in [DATA_DIR, EXTERNAL_DIR, AUGMENTED_DIR, VECTORDB_DIR, MODELS_DIR, RESULTS_DIR]:\n    dir_path.mkdir(parents=True, exist_ok=True)\n\n# 🔧 로깅 설정\nimport logging\nlogging.basicConfig(\n    level=logging.INFO,  # INFO 레벨 이상만 출력\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('training.log'),  # 파일에 저장\n        logging.StreamHandler()  # 콘솔에도 출력\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# 🎯 GPU 설정 및 메모리 관리\nif torch.cuda.is_available():\n    # GPU 사용 가능\n    device = torch.device(\"cuda\")\n    print(f\"🎮 GPU 사용: {torch.cuda.get_device_name(0)}\")\n    \n    # GPU 메모리 정보 출력 (RTX 4090은 24GB)\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    allocated = torch.cuda.memory_allocated() / 1024**3\n    print(f\"💾 GPU 메모리: {allocated:.2f}GB / {total_memory:.2f}GB\")\n    \n    # 💡 메모리 최적화 팁: 캐시 비우기\n    torch.cuda.empty_cache()\nelse:\n    device = torch.device(\"cpu\")\n    print(\"⚠️ GPU를 사용할 수 없습니다. CPU로 실행됩니다.\")\n\n# 🌱 재현성을 위한 시드 고정\n# 💡 왜 중요한가? 동일한 결과를 재현하기 위해 필수\\!\ndef set_seed(seed: int = 42):\n    \"\"\"\n    모든 랜덤 시드를 고정합니다.\n    딥러닝에서는 여러 라이브러리가 랜덤을 사용하므로 모두 고정해야 함.\n    \"\"\"\n    import random\n    \n    random.seed(seed)  # Python 기본 random\n    np.random.seed(seed)  # NumPy random\n    torch.manual_seed(seed)  # PyTorch CPU\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)  # PyTorch GPU\n        torch.cuda.manual_seed_all(seed)  # 멀티 GPU\n        # Deterministic 연산 강제 (약간 느려질 수 있음)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\nprint(\"✅ 환경 설정 완료\\!\")\nprint(f\"📂 작업 디렉토리: {BASE_DIR.absolute()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# PDF 처리\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# 벡터 DB\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 경로 설정\n",
    "PROJECT_ROOT = Path(\"/Users/gunwoo/Downloads/project/ai-dacon\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "EXTERNAL_DIR = DATA_DIR / \"external\"\n",
    "AUGMENTED_DIR = DATA_DIR / \"augmented\"\n",
    "VECTORDB_DIR = DATA_DIR / \"vectordb\"\n",
    "\n",
    "# 디렉토리 생성\n",
    "for dir_path in [EXTERNAL_DIR, AUGMENTED_DIR, VECTORDB_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"✅ 환경 설정 완료!\")\n",
    "print(f\"📁 프로젝트 루트: {PROJECT_ROOT}\")\n",
    "print(f\"📁 외부 데이터: {EXTERNAL_DIR}\")\n",
    "print(f\"📁 증강 데이터: {AUGMENTED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 🔧 실험 설정 (모든 파라미터 조정 가능)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# 🎓 실험 설정 클래스 - AI 개발의 핵심: 하이퍼파라미터 관리\n# ========================================\n#\n# 💡 왜 설정 클래스가 중요한가?\n#   1. 실험 재현성: 동일한 설정으로 동일한 결과를 얻을 수 있음\n#   2. A/B 테스트: 설정만 바꿔가며 성능 비교 가능\n#   3. 협업: 팀원들과 설정 공유가 쉬움\n\nclass ExperimentConfig:\n    \"\"\"\n    실험 설정 클래스 - 모든 파라미터를 한 곳에서 관리\n    \n    💡 클래스 변수 vs 인스턴스 변수:\n    - 여기서는 클래스 변수를 사용 (모든 인스턴스가 공유)\n    - 장점: 메모리 효율적, 전역 설정으로 사용하기 좋음\n    \"\"\"\n    \n    # ===== 모델 선택 =====\n    # 🤔 어떤 모델을 선택해야 할까?\n    # - 한국어 성능: korean_optimized, exaone이 좋음\n    # - 범용성: qwen, mistral이 좋음\n    # - 성능: solar가 가장 크고 성능이 좋지만 메모리도 많이 필요\n    MODEL_OPTIONS = {\n        \"korean_optimized\": \"beomi/llama-2-ko-7b\",      # 7B = 70억 개 파라미터\n        \"solar\": \"upstage/SOLAR-10.7B-v1.0\",            # 10.7B = 107억 개\n        \"exaone\": \"LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n        \"qwen\": \"Qwen/Qwen2.5-7B-Instruct\",\n        \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\"\n    }\n    \n    MODEL_NAME = MODEL_OPTIONS[\"korean_optimized\"]\n    \n    # ===== 생성 모드 설정 =====\n    # 🎯 각 모드의 특징과 사용 시점:\n    GENERATION_MODE = \"integrated\"  # 기본값: 통합형\n    # - \"integrated\": 질문과 답변을 한 번에 생성 (빠르지만 답변 품질이 낮을 수 있음)\n    # - \"separated\": 질문 먼저, 답변은 RAG 검색 후 생성 (느리지만 정확)\n    # - \"cot\": Chain-of-Thought, 단계별 검증 (매우 느리지만 최고 품질)\n    \n    # ===== CoT (Chain-of-Thought) 설정 =====\n    # 🧠 CoT란? LLM이 \"생각의 과정\"을 거쳐 답변하도록 하는 기법\n    COT_CONFIG = {\n        \"use_cot\": False,           # CoT 사용 여부\n        \"max_iterations\": 3,        # 개선 반복 횟수 (많을수록 품질↑ 속도↓)\n        \"quality_threshold\": 80,    # 품질 기준 (100점 만점)\n        \n        # 🔍 CoT의 4단계 프로세스:\n        # 1. 초기 생성 (Initial Generation)\n        # 2. 자가 검증 (Self-Verification) \n        # 3. 개선 (Improvement)\n        # 4. 최종 확인 (Final Check)\n        \"use_self_verification\": True,\n        \"use_improvement\": True,\n        \"use_final_check\": True,\n        \n        # 💾 캐싱: 동일한 입력에 대해 결과 재사용\n        \"cache_results\": True,\n        \n        # 실험용 세부 설정들\n        \"verification_strictness\": \"medium\",  # 검증 엄격도\n        \"focus_areas\": [\"accuracy\", \"clarity\", \"completeness\"],  # 집중 영역\n        \"reasoning_depth\": 2,       # 추론 깊이 (1~3)\n        \"multi_perspective\": True,  # 다각도 검증\n        \"self_critique_level\": 2,  # 자기 비판 수준 (0~3)\n    }\n    \n    # CoT 프리셋 - 빠르게 설정 전환 가능\n    # 💡 프리셋을 사용하면 일일이 설정을 바꾸지 않아도 됨\\!\n    COT_PRESETS = {\n        \"fast\": {  # 빠른 프로토타이핑용\n            \"max_iterations\": 1,\n            \"quality_threshold\": 70,\n            \"use_improvement\": False,\n        },\n        \"balanced\": {  # 균형잡힌 설정 (추천\\!)\n            \"max_iterations\": 2,\n            \"quality_threshold\": 75,\n            \"use_improvement\": True,\n        },\n        \"quality\": {  # 품질 우선\n            \"max_iterations\": 4,\n            \"quality_threshold\": 85,\n            \"use_improvement\": True,\n            \"multi_perspective\": True,\n        },\n        \"research\": {  # 연구/논문용 (매우 느림)\n            \"max_iterations\": 5,\n            \"quality_threshold\": 90,\n            \"use_improvement\": True,\n            \"example_generation\": True,\n        }\n    }\n    \n    CURRENT_COT_PRESET = \"balanced\"\n    \n    # 프리셋 적용 로직\n    # 💡 Python의 딕셔너리 업데이트 패턴\n    if CURRENT_COT_PRESET in COT_PRESETS:\n        for key, value in COT_PRESETS[CURRENT_COT_PRESET].items():\n            if key in COT_CONFIG:\n                COT_CONFIG[key] = value\n    \n    # ===== 양자화(Quantization) 설정 =====\n    # 🎯 양자화란? 모델의 가중치를 압축하여 메모리 사용량을 줄이는 기법\n    # - FP32 (32비트) → FP16 (16비트) → INT8 (8비트) → INT4 (4비트)\n    # - 4비트 양자화 시 메모리 사용량이 1/8로 줄어듦\\!\n    USE_QUANTIZATION = True  # RTX 4090 24GB에서는 필수\\!\n    \n    QUANTIZATION_CONFIG = {\n        \"load_in_4bit\": True,  # 4비트로 로드\n        \"bnb_4bit_quant_type\": \"nf4\",  # NormalFloat4 - 정규분포 기반 양자화\n        \"bnb_4bit_compute_dtype\": torch.float16,  # 계산은 FP16으로\n        \"bnb_4bit_use_double_quant\": True  # 이중 양자화 (더 압축\\!)\n    }\n    \n    # ===== 생성 파라미터 (매우 중요\\!) =====\n    # 🎨 텍스트 생성의 품질을 결정하는 핵심 파라미터들\n    GENERATION_PARAMS = {\n        \"max_new_tokens\": 400,      # 생성할 최대 토큰 수 (1토큰 ≈ 0.75단어)\n        \n        # 🌡️ Temperature: 창의성 조절 (0.1~2.0)\n        # - 낮을수록 (0.1): 안전하고 예측 가능한 답변\n        # - 높을수록 (1.5): 창의적이지만 때로는 이상한 답변\n        \"temperature\": 0.8,\n        \n        # 🎯 Sampling 전략들:\n        \"top_p\": 0.9,              # Nucleus sampling - 상위 90% 확률의 토큰만 고려\n        \"top_k\": 50,               # 상위 50개 토큰만 고려\n        \"do_sample\": True,         # 샘플링 사용 (False면 항상 가장 확률 높은 토큰 선택)\n        \n        # 🔁 반복 방지\n        \"repetition_penalty\": 1.2,  # 이미 나온 토큰의 확률을 낮춤 (1.0 = 패널티 없음)\n        \n        # 🔍 Beam Search (비활성화됨)\n        # - num_beams > 1이면 여러 경로를 동시에 탐색\n        # - 품질은 좋아지지만 속도가 느려짐\n        \"num_beams\": 1,\n    }\n    \n    # CoT 모드에서는 단계별로 다른 Temperature 사용\n    # 💡 왜? 검증 단계에서는 정확성이 중요하므로 낮은 온도 사용\n    COT_GENERATION_PARAMS = {\n        \"temperature_initial\": 0.7,     # 초기 생성 (약간 창의적)\n        \"temperature_verification\": 0.3, # 검증 (매우 보수적)\n        \"temperature_improvement\": 0.5,  # 개선 (중간)\n        \"temperature_final\": 0.3,        # 최종 확인 (보수적)\n    }\n    \n    # ===== RAG (Retrieval-Augmented Generation) 설정 =====\n    # 📚 RAG란? 외부 문서를 검색해서 LLM의 답변 품질을 높이는 기법\n    RAG_CONFIG = {\n        \"use_rag\": True,           # RAG 사용 여부\n        \"top_k_retrieval\": 3,      # 검색할 문서 수 (많을수록 정보는 많지만 노이즈도 증가)\n        \n        # 청킹(Chunking) 설정 - 문서를 작은 조각으로 나누기\n        # 💡 왜 나누나? 전체 문서는 너무 커서 한 번에 처리 불가\n        \"chunk_size\": 500,         # 각 청크의 크기 (문자 수)\n        \"chunk_overlap\": 50,       # 청크 간 겹침 (문맥 유지용)\n        \n        # 임베딩 모델 - 텍스트를 벡터로 변환\n        # 💡 작은 모델이지만 성능이 좋음 (384차원 벡터)\n        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n        \n        \"use_cache\": True,         # 인덱스 캐싱 (재실행 시 빠름\\!)\n    }\n    \n    # ===== 품질 관리 =====\n    QUALITY_CONFIG = {\n        \"min_answer_length\": 10,   # 최소 답변 길이 (너무 짧은 답변 방지)\n        \"max_retry_attempts\": 3,   # 실패 시 재시도 횟수\n        \"quality_threshold\": 70,   # 품질 점수 기준 (100점 만점)\n        \"use_validation\": True,    # 품질 검증 사용\n        \"use_fallback\": True,      # 실패 시 대체 방법 사용\n    }\n    \n    # ===== 문제 유형 분포 =====\n    # 💡 다양한 유형의 문제를 생성하여 모델의 범용성 향상\n    QUESTION_TYPE_DISTRIBUTION = {\n        \"객관식\": 0.30,    # 30% - 선택지에서 고르기\n        \"주관식\": 0.30,    # 30% - 자유롭게 서술\n        \"단답형\": 0.15,    # 15% - 짧은 답변\n        \"서술형\": 0.15,    # 15% - 긴 설명\n        \"계산형\": 0.05,    # 5% - 수치 계산\n        \"사례분석\": 0.05,  # 5% - 실제 사례 분석\n    }\n    \n    # ===== 배치 처리 설정 =====\n    # 🚀 대량 데이터 생성 시 중요\\!\n    BATCH_CONFIG = {\n        \"batch_size\": 4,          # 동시 처리 개수 (메모리와 트레이드오프)\n        \"target_count\": 100,      # 목표 생성 개수\n        \"max_attempts_ratio\": 3,  # 최대 시도 = target * ratio\n        \"save_interval\": 20,      # N개마다 중간 저장 (안전장치\\!)\n    }\n    \n    # ===== 실험 모드 =====\n    EXPERIMENT_MODE = {\n        \"verbose\": True,          # 상세 로그 출력\n        \"debug\": False,           # 디버그 모드 (더 많은 정보 출력)\n        \"dry_run\": False,         # 실제 실행 없이 테스트만\n        \"compare_modes\": False,   # 여러 모드 비교 실행\n        \"save_stats\": True,       # 통계 저장\n    }\n\n# 설정 인스턴스 생성\nconfig = ExperimentConfig()\n\n# 설정 요약 출력\nprint(\"🔬 실험 설정 완료\\!\")\nprint(f\"  📌 모델: {config.MODEL_NAME}\")\nprint(f\"  📌 생성 모드: {config.GENERATION_MODE}\")\nprint(f\"  📌 CoT 사용: {config.COT_CONFIG['use_cot'] or config.GENERATION_MODE == 'cot'}\")\nprint(f\"  📌 CoT 프리셋: {config.CURRENT_COT_PRESET}\")\nprint(f\"  📌 양자화: {config.USE_QUANTIZATION}\")\nprint(f\"  📌 Temperature: {config.GENERATION_PARAMS['temperature']}\")\nprint(f\"  📌 RAG 사용: {config.RAG_CONFIG['use_rag']}\")\nprint(f\"  📌 품질 임계값: {config.QUALITY_CONFIG['quality_threshold']}\")\nprint(\"\\n💡 이 설정들을 자유롭게 변경하며 실험해보세요\\!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG 시스템 (문서 검색 및 캐싱)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# 🎓 RAG 시스템 구현 - 외부 지식을 활용한 AI의 핵심\n# ========================================\n#\n# 💡 RAG(Retrieval-Augmented Generation)란?\n#   - LLM의 한계: 학습 데이터에만 의존, 최신 정보 부족\n#   - RAG의 해결책: 외부 문서를 검색해서 답변에 활용\n#   - 비유: 시험 볼 때 오픈북으로 보는 것과 같음\\!\n\nclass RAGSystem:\n    \"\"\"\n    RAG 시스템 - 문서 검색 및 캐싱 지원\n    \n    🔍 RAG의 3단계 프로세스:\n    1. 문서 준비 (Indexing): PDF/텍스트를 벡터로 변환\n    2. 검색 (Retrieval): 질문과 유사한 문서 찾기\n    3. 생성 (Generation): 검색된 문서를 참고해 답변 생성\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig):\n        self.config = config.RAG_CONFIG\n        \n        # 🧠 임베딩 모델: 텍스트를 벡터(숫자 배열)로 변환\n        # 예: \"금융\" → [0.1, -0.3, 0.5, ...] (384차원)\n        self.embedding_model = None\n        \n        # 🗂️ 벡터 인덱스: 빠른 유사도 검색을 위한 자료구조\n        # FAISS는 Facebook이 만든 초고속 벡터 검색 라이브러리\n        self.index = None\n        \n        # 📚 원본 문서들 저장 (인덱스는 벡터만, 실제 텍스트는 여기에)\n        self.documents = []\n        \n        # 💾 캐시 파일 경로 (한 번 만든 인덱스 재사용)\n        self.index_path = VECTORDB_DIR / \"index.pkl\"\n        \n    def initialize(self):\n        \"\"\"초기화 - 임베딩 모델 로드 및 인덱스 준비\"\"\"\n        print(\"🔍 RAG 시스템 초기화 중...\")\n        \n        # 1. 임베딩 모델 로드\n        # 💡 SentenceTransformer: 문장 전체의 의미를 벡터로 표현\n        # all-MiniLM-L6-v2는 작지만 성능이 좋은 모델 (50MB)\n        self.embedding_model = SentenceTransformer(self.config['embedding_model'])\n        \n        # 2. 캐시 확인 - 이미 만든 인덱스가 있으면 재사용\n        if self.config['use_cache'] and self.index_path.exists():\n            # 🚀 캐싱의 효과: 46초 → 0.02초 (2,300배 빨라짐\\!)\n            self.load_index()\n        else:\n            # 처음 실행이면 인덱스 구축\n            self.build_index()\n    \n    def build_index(self):\n        \"\"\"인덱스 구축 - PDF 문서를 벡터 DB로 변환\"\"\"\n        print(\"📚 문서 인덱스 구축 중...\")\n        \n        # 1. PDF 문서 로드\n        documents = self.load_documents()\n        \n        if not documents:\n            print(\"⚠️ 문서가 없습니다. data/external/ 폴더에 PDF를 추가하세요.\")\n            return\n        \n        # 2. 문서를 청크로 분할\n        # 💡 왜 분할? LLM의 컨텍스트 길이 제한 때문\n        # 청크: 문서를 작은 조각으로 나눈 것\n        chunks = self.split_documents(documents)\n        print(f\"  📄 {len(chunks)}개 청크 생성\")\n        \n        # 3. 각 청크를 벡터로 변환 (임베딩)\n        # 🎯 이 과정이 시간이 오래 걸림 (GPU 있으면 빠름)\n        print(\"  🧮 임베딩 생성 중... (첫 실행 시 1-2분 소요)\")\n        embeddings = self.embedding_model.encode(\n            chunks,\n            show_progress_bar=True,  # 진행 상황 표시\n            batch_size=32  # 배치 처리로 속도 향상\n        )\n        \n        # 4. FAISS 인덱스 생성\n        # 💡 FAISS: 수백만 개 벡터도 밀리초 단위로 검색 가능\\!\n        dimension = embeddings.shape[1]  # 벡터 차원 (보통 384)\n        \n        # IndexFlatL2: L2 거리(유클리드 거리) 기반 검색\n        # 가장 정확하지만 대용량에서는 느릴 수 있음\n        self.index = faiss.IndexFlatL2(dimension)\n        \n        # 벡터 추가\n        self.index.add(embeddings.astype('float32'))\n        \n        # 원본 텍스트 저장 (인덱스는 벡터만 저장하므로)\n        self.documents = chunks\n        \n        # 5. 캐시 저장\n        if self.config['use_cache']:\n            self.save_index()\n            \n        print(f\"✅ 인덱스 구축 완료\\! ({len(chunks)}개 문서)\")\n        \n    def load_documents(self) -> List[str]:\n        \"\"\"PDF 문서 로드\"\"\"\n        documents = []\n        pdf_files = list(EXTERNAL_DIR.glob(\"*.pdf\"))\n        \n        if not pdf_files:\n            # PDF가 없으면 샘플 텍스트라도 사용\n            return self._get_sample_documents()\n        \n        for pdf_path in pdf_files:\n            try:\n                # PyPDF2로 PDF 읽기\n                with open(pdf_path, 'rb') as file:\n                    pdf_reader = PyPDF2.PdfReader(file)\n                    text = \"\"\n                    \n                    # 모든 페이지의 텍스트 추출\n                    for page in pdf_reader.pages:\n                        text += page.extract_text() + \"\\n\"\n                    \n                    documents.append(text)\n                    print(f\"  ✅ {pdf_path.name} 로드 완료\")\n                    \n            except Exception as e:\n                print(f\"  ❌ {pdf_path.name} 로드 실패: {e}\")\n                \n        return documents\n    \n    def split_documents(self, documents: List[str]) -> List[str]:\n        \"\"\"\n        문서를 청크로 분할\n        \n        💡 청킹 전략이 RAG 성능에 큰 영향\\!\n        - 너무 작으면: 문맥 정보 부족\n        - 너무 크면: 노이즈 증가, 정확도 하락\n        \"\"\"\n        # RecursiveCharacterTextSplitter: 문장 → 단락 → 페이지 순으로 분할\n        # 가장 자연스러운 분할 방법\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.config['chunk_size'],      # 각 청크 크기\n            chunk_overlap=self.config['chunk_overlap'], # 청크 간 겹침\n            length_function=len,  # 길이 계산 함수\n            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # 분할 우선순위\n        )\n        \n        chunks = []\n        for doc in documents:\n            # 문서를 청크로 분할\n            doc_chunks = text_splitter.split_text(doc)\n            chunks.extend(doc_chunks)\n            \n        return chunks\n    \n    def search(self, query: str, top_k: int = None) -> List[Dict]:\n        \"\"\"\n        유사한 문서 검색\n        \n        🔍 벡터 유사도 검색 과정:\n        1. 질문을 벡터로 변환\n        2. 모든 문서 벡터와 거리 계산\n        3. 가장 가까운 k개 선택\n        \"\"\"\n        if self.index is None or not self.documents:\n            print(\"⚠️ 인덱스가 없습니다. 먼저 initialize()를 실행하세요.\")\n            return []\n        \n        top_k = top_k or self.config['top_k_retrieval']\n        \n        # 1. 질문을 벡터로 변환\n        # 💡 질문과 문서를 같은 벡터 공간에 매핑\n        query_embedding = self.embedding_model.encode([query])\n        \n        # 2. 가장 유사한 문서 검색\n        # D: 거리(작을수록 유사), I: 인덱스\n        distances, indices = self.index.search(\n            query_embedding.astype('float32'), \n            top_k\n        )\n        \n        # 3. 결과 정리\n        results = []\n        for idx, distance in zip(indices[0], distances[0]):\n            if idx < len(self.documents):  # 범위 체크\n                results.append({\n                    'text': self.documents[idx],\n                    'distance': float(distance),  # L2 거리\n                    'similarity': 1 / (1 + float(distance))  # 유사도 점수로 변환\n                })\n        \n        # 유사도 순으로 정렬\n        results.sort(key=lambda x: x['similarity'], reverse=True)\n        \n        return results\n    \n    def save_index(self):\n        \"\"\"인덱스 캐시 저장\"\"\"\n        print(\"💾 인덱스 캐시 저장 중...\")\n        \n        # pickle로 저장 (Python 객체 직렬화)\n        cache_data = {\n            'index': faiss.serialize_index(self.index),  # FAISS 인덱스\n            'documents': self.documents,  # 원본 텍스트\n            'timestamp': datetime.now().isoformat()  # 생성 시간\n        }\n        \n        with open(self.index_path, 'wb') as f:\n            pickle.dump(cache_data, f)\n            \n        print(f\"✅ 캐시 저장 완료: {self.index_path}\")\n        \n    def load_index(self):\n        \"\"\"인덱스 캐시 로드\"\"\"\n        print(\"📂 캐시된 인덱스 로드 중...\")\n        \n        with open(self.index_path, 'rb') as f:\n            cache_data = pickle.load(f)\n            \n        # FAISS 인덱스 복원\n        self.index = faiss.deserialize_index(cache_data['index'])\n        self.documents = cache_data['documents']\n        \n        print(f\"✅ 캐시 로드 완료\\! ({len(self.documents)}개 문서)\")\n        print(f\"  생성 시간: {cache_data['timestamp']}\")\n        \n    def _get_sample_documents(self) -> List[str]:\n        \"\"\"PDF가 없을 때 사용할 샘플 문서\"\"\"\n        # 💡 실제 프로젝트에서는 반드시 실제 문서를 사용하세요\\!\n        return [\n            \"\"\"바젤III 규제는 2008년 금융위기 이후 도입된 국제 은행 규제 프레임워크입니다.\n            주요 내용으로는 자본 적정성 강화, 레버리지 비율 도입, 유동성 규제 신설 등이 있습니다.\n            보통주자본비율은 4.5%, Tier1 자본비율은 6%, 총자본비율은 8% 이상을 유지해야 합니다.\"\"\",\n            \n            \"\"\"금융보안원(FSI)은 국내 금융 IT 보안을 총괄하는 전문기관입니다.\n            주요 업무로는 금융권 사이버 보안 강화, 전자금융거래 안전성 확보,\n            금융회사 보안 수준 평가 및 점검 등이 있습니다.\"\"\",\n            \n            \"\"\"파생상품은 기초자산의 가격 변동에 따라 가치가 결정되는 금융상품입니다.\n            선물(Futures), 옵션(Options), 스왑(Swaps) 등이 대표적이며,\n            위험 헤지(Hedging)와 투기(Speculation) 목적으로 활용됩니다.\"\"\"\n        ]\n\nprint(\"✅ RAG 시스템 코드 정의 완료\\!\")\nprint(\"💡 사용법:\")\nprint(\"  rag = RAGSystem(config)\")\nprint(\"  rag.initialize()  # 인덱스 구축 또는 로드\")\nprint(\"  results = rag.search('바젤III 자본비율')  # 검색\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 향상된 프롬프트 템플릿 (통합형/분리형)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPromptTemplates:\n",
    "    \"\"\"\n",
    "    답변 생성이 보장된 향상된 프롬프트 템플릿\n",
    "    통합형과 분리형 모두 지원\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, style: str = \"detailed\"):\n",
    "        self.style = style  # simple, detailed, expert\n",
    "        self.templates = self._init_templates()\n",
    "    \n",
    "    def _init_templates(self) -> Dict:\n",
    "        \"\"\"스타일별 템플릿 초기화\"\"\"\n",
    "        \n",
    "        if self.style == \"simple\":\n",
    "            # 간단한 프롬프트 (빠르지만 품질 낮음)\n",
    "            return self._get_simple_templates()\n",
    "        elif self.style == \"expert\":\n",
    "            # 전문가 프롬프트 (느리지만 품질 높음)\n",
    "            return self._get_expert_templates()\n",
    "        else:\n",
    "            # 기본: 상세 프롬프트 (균형)\n",
    "            return self._get_detailed_templates()\n",
    "    \n",
    "    def _get_detailed_templates(self) -> Dict:\n",
    "        \"\"\"상세 템플릿 (기본)\"\"\"\n",
    "        return {\n",
    "            # ===== 통합형 템플릿 (질문+답변 동시) =====\n",
    "            \"integrated\": {\n",
    "                \"객관식\": \"\"\"당신은 금융보안원 FSKU 출제위원입니다.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "위 내용을 바탕으로 FSKU 객관식 문제를 생성하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 문제는 명확하고 구체적이어야 함\n",
    "2. 4개의 선택지를 제시\n",
    "3. 정답은 반드시 하나만 존재\n",
    "4. 오답도 그럴듯해야 함\n",
    "\n",
    "형식 (반드시 이 형식을 지켜주세요):\n",
    "문제: [구체적인 질문]\n",
    "① [선택지1]\n",
    "② [선택지2]\n",
    "③ [선택지3]\n",
    "④ [선택지4]\n",
    "정답: [정답 번호와 내용]\n",
    "해설: [정답 선택 이유]\"\"\",\n",
    "\n",
    "                \"주관식\": \"\"\"당신은 금융보안원 FSKU 출제위원입니다.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "위 내용을 바탕으로 FSKU 주관식 문제를 생성하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 2-3문장으로 답할 수 있는 문제\n",
    "2. 구체적이고 명확한 답변이 가능해야 함\n",
    "3. 핵심 개념을 묻는 문제여야 함\n",
    "\n",
    "형식 (반드시 이 형식을 지켜주세요):\n",
    "문제: [구체적인 질문]\n",
    "정답: [2-3문장의 완전한 답변]\n",
    "핵심 키워드: [중요 키워드 3-5개]\n",
    "채점 기준: [평가 기준]\"\"\",\n",
    "\n",
    "                \"단답형\": \"\"\"당신은 금융보안원 FSKU 출제위원입니다.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "위 내용을 바탕으로 FSKU 단답형 문제를 생성하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 한 단어나 짧은 구로 답할 수 있는 문제\n",
    "2. 명확한 정답이 존재해야 함\n",
    "3. 용어, 수치, 개념명을 묻는 문제\n",
    "\n",
    "형식 (반드시 이 형식을 지켜주세요):\n",
    "문제: [구체적인 질문]\n",
    "정답: [단답형 정답]\n",
    "허용 답안: [유사 정답들]\"\"\",\n",
    "\n",
    "                \"서술형\": \"\"\"당신은 금융보안원 FSKU 출제위원입니다.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "위 내용을 바탕으로 FSKU 서술형 문제를 생성하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 상세한 설명이 필요한 문제\n",
    "2. 논리적 전개가 중요한 답변\n",
    "3. 5문장 이상의 답변 필요\n",
    "\n",
    "형식 (반드시 이 형식을 지켜주세요):\n",
    "문제: [구체적인 질문]\n",
    "모범 답안: [5문장 이상의 상세한 답변]\n",
    "핵심 평가 요소: [평가할 요소들]\"\"\",\n",
    "            },\n",
    "            \n",
    "            # ===== 분리형 템플릿 - 질문 생성용 =====\n",
    "            \"question\": {\n",
    "                \"객관식\": \"\"\"금융보안원 FSKU 시험을 위한 객관식 문제를 생성하세요.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "위 내용에서 중요한 개념을 선택하여 4지선다 문제를 만드세요.\n",
    "문제와 선택지만 생성하고, 정답은 생성하지 마세요.\n",
    "\n",
    "형식:\n",
    "문제: [질문]\n",
    "① [선택지1]\n",
    "② [선택지2]\n",
    "③ [선택지3]\n",
    "④ [선택지4]\"\"\",\n",
    "\n",
    "                \"주관식\": \"\"\"금융보안원 FSKU 시험을 위한 주관식 문제를 생성하세요.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "위 내용에서 중요한 개념을 설명하도록 요구하는 문제를 만드세요.\n",
    "문제만 생성하고, 답변은 생성하지 마세요.\n",
    "\n",
    "형식:\n",
    "문제: [서술형 질문]\"\"\",\n",
    "            },\n",
    "            \n",
    "            # ===== 분리형 템플릿 - 답변 생성용 =====\n",
    "            \"answer\": {\n",
    "                \"객관식\": \"\"\"다음 문제의 정답을 선택하고 설명하세요.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "문제:\n",
    "{question}\n",
    "\n",
    "위 문제의 정답을 선택하고, 그 이유를 설명하세요.\n",
    "\n",
    "형식:\n",
    "정답: [정답 번호와 내용]\n",
    "해설: [선택 이유와 다른 선택지가 틀린 이유]\"\"\",\n",
    "\n",
    "                \"주관식\": \"\"\"다음 문제에 대한 완전하고 정확한 답변을 작성하세요.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "문제:\n",
    "{question}\n",
    "\n",
    "위 문제에 대해 2-3문장으로 명확하게 답변하세요.\n",
    "\n",
    "형식:\n",
    "정답: [완전한 답변]\n",
    "핵심 키워드: [중요 용어들]\"\"\",\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_simple_templates(self) -> Dict:\n",
    "        \"\"\"간단한 템플릿 (빠름)\"\"\"\n",
    "        return {\n",
    "            \"integrated\": {\n",
    "                \"객관식\": \"\"\"참고: {context}\n",
    "\n",
    "객관식 문제 생성:\n",
    "문제:\n",
    "①\n",
    "②\n",
    "③\n",
    "④\n",
    "정답:\"\"\",\n",
    "                \"주관식\": \"\"\"참고: {context}\n",
    "\n",
    "주관식 문제 생성:\n",
    "문제:\n",
    "정답:\"\"\",\n",
    "            },\n",
    "            \"question\": {\n",
    "                \"객관식\": \"\"\"참고: {context}\n",
    "객관식 문제만:\"\"\",\n",
    "                \"주관식\": \"\"\"참고: {context}\n",
    "주관식 문제만:\"\"\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"객관식\": \"\"\"문제: {question}\n",
    "정답:\"\"\",\n",
    "                \"주관식\": \"\"\"문제: {question}\n",
    "정답:\"\"\",\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_expert_templates(self) -> Dict:\n",
    "        \"\"\"전문가 템플릿 (고품질)\"\"\"\n",
    "        return {\n",
    "            \"integrated\": {\n",
    "                \"객관식\": \"\"\"[전문가 모드]\n",
    "당신은 20년 경력의 FSKU 출제위원장입니다.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "다음 기준에 따라 최고 품질의 객관식 문제를 생성하세요:\n",
    "1. Bloom's Taxonomy 상위 수준 (분석, 평가, 창조)\n",
    "2. 실무 상황 기반\n",
    "3. 함정 선택지 포함\n",
    "4. 명확한 변별력\n",
    "\n",
    "필수 포함 요소:\n",
    "- 문제 상황 설정\n",
    "- 구체적 조건 제시\n",
    "- 4개 선택지 (모두 그럴듯함)\n",
    "- 정답과 상세 해설\n",
    "\n",
    "형식:\n",
    "문제: [상황 설정 + 구체적 질문]\n",
    "① [선택지1 - 매력적인 오답]\n",
    "② [선택지2 - 부분적으로 맞는 오답]\n",
    "③ [선택지3 - 정답 또는 오답]\n",
    "④ [선택지4 - 혼동하기 쉬운 오답]\n",
    "정답: [번호와 내용]\n",
    "해설: [각 선택지별 상세 설명]\n",
    "난이도: [상/중/하]\n",
    "출제 의도: [평가하고자 하는 역량]\"\"\",\n",
    "                \n",
    "                # 다른 유형들도 유사하게 전문가 수준으로...\n",
    "            },\n",
    "            # question, answer도 전문가 수준으로 확장...\n",
    "        }\n",
    "    \n",
    "    def get_template(self, mode: str, question_type: str, template_type: str = \"integrated\") -> str:\n",
    "        \"\"\"\n",
    "        템플릿 가져오기\n",
    "        \n",
    "        Args:\n",
    "            mode: generation mode (integrated/separated)\n",
    "            question_type: 문제 유형\n",
    "            template_type: integrated/question/answer\n",
    "        \"\"\"\n",
    "        if mode == \"integrated\":\n",
    "            return self.templates[\"integrated\"].get(\n",
    "                question_type, \n",
    "                self.templates[\"integrated\"][\"주관식\"]\n",
    "            )\n",
    "        else:  # separated\n",
    "            return self.templates[template_type].get(\n",
    "                question_type,\n",
    "                self.templates[template_type][\"주관식\"]\n",
    "            )\n",
    "\n",
    "print(\"✅ 프롬프트 템플릿 클래스 정의 완료\")\n",
    "print(\"📝 스타일 옵션: simple, detailed, expert\")\n",
    "print(\"📝 모드: integrated (통합형), separated (분리형)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# 🎓 데이터 생성기 - LLM을 활용한 학습 데이터 자동 생성\n# ========================================\n#\n# 💡 왜 데이터 생성이 필요한가?\n#   - 문제: 고품질 학습 데이터 부족 (특히 한국어 금융 분야)\n#   - 해결: LLM을 사용해 자동으로 Q&A 쌍 생성\n#   - 주의: 생성된 데이터의 품질 검증 필수\\!\n\nclass AnswerGuaranteedGenerator:\n    \"\"\"\n    답변 생성이 보장된 데이터 생성기\n    \n    🎯 핵심 기능:\n    1. 통합형/분리형/CoT 모드 지원\n    2. 답변 없을 시 3단계 폴백 시스템\n    3. 품질 검증 포함\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n        self.config = config\n        self.generation_mode = config.GENERATION_MODE\n        self.rag = rag_system\n        \n        # 🤖 모델과 토크나이저 (나중에 로드)\n        self.model = None\n        self.tokenizer = None\n        self.model_loaded = False\n        \n        # 📊 통계 추적 (성능 분석용)\n        self.stats = {\n            'total_attempts': 0,      # 총 시도 횟수\n            'successful': 0,           # 성공 횟수\n            'failed': 0,              # 실패 횟수\n            'retry_count': 0,         # 재시도 횟수\n            'with_answer': 0,         # 답변 있는 경우\n            'without_answer': 0,      # 답변 없는 경우\n            'fallback_used': 0,       # 폴백 사용 횟수\n        }\n    \n    def load_model(self):\n        \"\"\"\n        모델 로드 - 메모리 효율적으로 LLM 로드\n        \n        💡 모델 로드 시 고려사항:\n        1. 메모리 제한: RTX 4090은 24GB\n        2. 양자화 사용: 4bit로 압축하면 7B 모델도 로드 가능\n        3. device_map=\"auto\": 자동으로 GPU/CPU 분배\n        \"\"\"\n        if self.model_loaded:\n            return  # 이미 로드됨\n        \n        print(f\"🚀 모델 로딩: {self.config.MODEL_NAME}\")\n        print(f\"  양자화: {self.config.USE_QUANTIZATION}\")\n        \n        try:\n            # 1. 토크나이저 로드\n            # 💡 토크나이저: 텍스트 ↔ 토큰 변환\n            # 예: \"안녕하세요\" → [1234, 5678, 9012]\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.config.MODEL_NAME,\n                trust_remote_code=True  # 커스텀 코드 허용 (일부 모델 필요)\n            )\n            \n            # 패딩 토큰 설정 (없으면 EOS 토큰 사용)\n            # 💡 패딩: 배치 처리 시 길이를 맞추기 위한 특수 토큰\n            if not self.tokenizer.pad_token:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # 2. 모델 로드 (양자화 여부에 따라 다르게)\n            if self.config.USE_QUANTIZATION:\n                # 🔥 QLoRA 방식: 4bit 양자화\n                # 메모리 사용량: 7B 모델 → 약 4GB\n                bnb_config = BitsAndBytesConfig(**self.config.QUANTIZATION_CONFIG)\n                \n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.config.MODEL_NAME,\n                    quantization_config=bnb_config,  # 양자화 설정\n                    device_map=\"auto\",  # GPU/CPU 자동 분배\n                    trust_remote_code=True\n                )\n            else:\n                # 일반 방식: FP16 (반정밀도)\n                # 메모리 사용량: 7B 모델 → 약 14GB\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.config.MODEL_NAME,\n                    torch_dtype=torch.float16,  # FP32 → FP16 (메모리 절반)\n                    device_map=\"auto\",\n                    trust_remote_code=True\n                )\n            \n            self.model_loaded = True\n            print(\"✅ 모델 로드 완료\\!\")\n            \n            # GPU 메모리 사용량 출력\n            if torch.cuda.is_available():\n                memory = torch.cuda.memory_allocated() / 1024**3\n                print(f\"💾 GPU 메모리 사용: {memory:.2f}GB\")\n                \n        except Exception as e:\n            logger.error(f\"모델 로드 실패: {e}\")\n            # 💡 일반적인 오류 원인:\n            # 1. 메모리 부족 → 양자화 사용 또는 더 작은 모델\n            # 2. 모델명 오타 → MODEL_NAME 확인\n            # 3. 인터넷 연결 → 첫 다운로드 시 필요\n            raise\n    \n    def generate_text(self, prompt: str, max_tokens: int = None) -> str:\n        \"\"\"\n        텍스트 생성 - LLM의 핵심 기능\n        \n        🎨 생성 과정:\n        1. 프롬프트 → 토큰화\n        2. 모델 추론 (Forward pass)\n        3. 토큰 샘플링 (Temperature, Top-p 등 적용)\n        4. 토큰 → 텍스트 변환\n        \"\"\"\n        if not self.model_loaded:\n            self.load_model()\n        \n        max_tokens = max_tokens or self.config.GENERATION_PARAMS['max_new_tokens']\n        \n        # 1. 토큰화\n        # 💡 return_tensors=\"pt\": PyTorch 텐서로 반환\n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,  # 너무 길면 자르기\n            max_length=2000   # 최대 입력 길이\n        )\n        \n        # GPU로 이동 (가능한 경우)\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n        \n        # 2. 생성 (추론)\n        # 💡 torch.no_grad(): 그래디언트 계산 비활성화 (메모리 절약)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,  # input_ids, attention_mask 등\n                max_new_tokens=max_tokens,\n                \n                # 샘플링 파라미터들 (품질 결정\\!)\n                temperature=self.config.GENERATION_PARAMS['temperature'],\n                top_p=self.config.GENERATION_PARAMS['top_p'],\n                top_k=self.config.GENERATION_PARAMS['top_k'],\n                do_sample=self.config.GENERATION_PARAMS['do_sample'],\n                repetition_penalty=self.config.GENERATION_PARAMS['repetition_penalty'],\n                \n                # 특수 토큰 ID\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n            )\n        \n        # 3. 디코딩 (토큰 → 텍스트)\n        # 입력 부분 제외하고 생성된 부분만 추출\n        generated = self.tokenizer.decode(\n            outputs[0][inputs['input_ids'].shape[1]:],  # 입력 길이 이후부터\n            skip_special_tokens=True  # <pad>, <eos> 등 제거\n        )\n        \n        return generated.strip()\n    \n    def generate_qa_pair(self, context: str, question_type: str = \"주관식\") -> Optional[Dict]:\n        \"\"\"\n        QA 쌍 생성 - 메인 함수\n        \n        🔄 생성 모드별 차이:\n        1. integrated: 한 번에 Q&A 생성 (빠름)\n        2. separated: Q 생성 → RAG 검색 → A 생성 (정확)\n        3. cot: 4단계 검증 과정 (최고 품질)\n        \"\"\"\n        self.stats['total_attempts'] += 1\n        \n        try:\n            if self.generation_mode == \"integrated\":\n                # 통합형: 프롬프트 하나로 Q&A 동시 생성\n                return self._generate_integrated(context, question_type)\n                \n            elif self.generation_mode == \"separated\":\n                # 분리형: 질문 먼저, 답변은 따로\n                return self._generate_separated(context, question_type)\n                \n            elif self.generation_mode == \"cot\":\n                # CoT: Chain-of-Thought로 단계별 검증\n                # 별도 클래스에서 처리 (ChainOfThoughtGenerator)\n                pass\n                \n        except Exception as e:\n            logger.error(f\"QA 생성 오류: {e}\")\n            self.stats['failed'] += 1\n            return None\n    \n    def _generate_integrated(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        통합형 생성 - 한 번에 Q&A 생성\n        \n        장점: 빠름, 일관성 있음\n        단점: 답변 품질이 낮을 수 있음\n        \"\"\"\n        # 프롬프트 구성\n        # 💡 프롬프트 엔지니어링이 품질의 80%를 결정\\!\n        prompt = f\"\"\"당신은 한국 금융감독원의 FSKU 시험 출제위원입니다.\n다음 문서를 참고하여 {question_type} 문제와 답변을 생성하세요.\n\n### 참고 문서:\n{context[:1000]}  # 너무 길면 잘라서 사용\n\n### 생성 지침:\n1. 문제는 명확하고 구체적으로\n2. 답변은 완전하고 정확하게\n3. 금융 전문 용어를 적절히 사용\n4. 실무에서 중요한 내용 위주로\n\n### 형식:\n문제: [여기에 질문 작성]\n정답: [여기에 답변 작성]\n\n### 생성:\"\"\"\n        \n        # 텍스트 생성\n        generated = self.generate_text(prompt)\n        \n        # 파싱 (생성된 텍스트에서 Q&A 추출)\n        result = self._parse_qa(generated)\n        \n        if result:\n            self.stats['successful'] += 1\n            if result.get('answer'):\n                self.stats['with_answer'] += 1\n            else:\n                self.stats['without_answer'] += 1\n                \n        return result\n    \n    def _generate_separated(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        분리형 생성 - 질문과 답변을 따로 생성\n        \n        🔍 개선된 프로세스:\n        1. 컨텍스트로 질문 생성\n        2. 생성된 질문으로 RAG 재검색 ← 핵심\\!\n        3. 원본 + 검색 결과로 답변 생성\n        \"\"\"\n        # 1단계: 질문 생성\n        question_prompt = f\"\"\"문서를 읽고 {question_type} 질문을 하나만 생성하세요.\n\n문서: {context[:500]}\n\n질문:\"\"\"\n        \n        question = self.generate_text(question_prompt, max_tokens=100)\n        \n        if not question:\n            return None\n        \n        # 2단계: RAG 검색 (생성된 질문 기반)\n        # 💡 이것이 분리형의 핵심 개선\\!\n        enhanced_context = context\n        if self.rag:\n            # 질문으로 관련 문서 검색\n            retrieved = self.rag.search(question, top_k=3)\n            if retrieved:\n                # 검색 결과를 컨텍스트에 추가\n                rag_context = \"\\n\".join([r['text'][:200] for r in retrieved])\n                enhanced_context = f\"{context}\\n\\n관련 정보:\\n{rag_context}\"\n        \n        # 3단계: 답변 생성\n        answer_prompt = f\"\"\"다음 질문에 대한 정확한 답변을 작성하세요.\n\n참고 자료:\n{enhanced_context[:800]}\n\n질문: {question}\n\n답변:\"\"\"\n        \n        answer = self.generate_text(answer_prompt, max_tokens=300)\n        \n        return {\n            'question': question.strip(),\n            'answer': answer.strip(),\n            'context': context[:500],\n            'question_type': question_type,\n            'generation_mode': 'separated',\n            'rag_used': self.rag is not None\n        }\n    \n    def _parse_qa(self, text: str) -> Optional[Dict]:\n        \"\"\"\n        생성된 텍스트에서 Q&A 추출\n        \n        💡 파싱은 의외로 까다로움\\!\n        LLM이 항상 정확한 형식으로 생성하지 않기 때문\n        \"\"\"\n        result = {}\n        \n        # 다양한 형식 처리\n        # LLM마다 선호하는 형식이 다름\n        question_markers = ['문제:', '질문:', 'Q:', 'Question:']\n        answer_markers = ['정답:', '답변:', '답:', 'A:', 'Answer:']\n        \n        # 질문 추출\n        for marker in question_markers:\n            if marker in text:\n                parts = text.split(marker, 1)[1]\n                # 답변 마커까지만 추출\n                for ans_marker in answer_markers:\n                    if ans_marker in parts:\n                        result['question'] = parts.split(ans_marker)[0].strip()\n                        break\n                break\n        \n        # 답변 추출\n        for marker in answer_markers:\n            if marker in text:\n                answer_part = text.split(marker, 1)[1]\n                # 다음 섹션이나 줄바꿈까지\n                result['answer'] = answer_part.split('\\n\\n')[0].strip()\n                break\n        \n        # 둘 다 있어야 유효\n        if 'question' in result and 'answer' in result:\n            return result\n        \n        return None\n    \n    def get_stats(self) -> Dict:\n        \"\"\"통계 반환 - 성능 분석용\"\"\"\n        total = self.stats['total_attempts']\n        if total == 0:\n            return self.stats\n        \n        # 성공률 계산\n        success_rate = self.stats['successful'] / total * 100\n        answer_rate = self.stats['with_answer'] / max(self.stats['successful'], 1) * 100\n        \n        return {\n            **self.stats,\n            'success_rate': f\"{success_rate:.1f}%\",\n            'answer_rate': f\"{answer_rate:.1f}%\",\n            'fallback_rate': f\"{self.stats['fallback_used'] / total * 100:.1f}%\"\n        }\n\nprint(\"✅ 데이터 생성기 정의 완료\\!\")\nprint(\"💡 각 모드의 사용 시점:\")\nprint(\"  - integrated: 빠른 프로토타이핑\")\nprint(\"  - separated: 정확한 답변이 필요할 때\")\nprint(\"  - cot: 최고 품질이 필요할 때\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGuaranteedGenerator:\n",
    "    \"\"\"\n",
    "    답변 생성이 보장된 데이터 생성기\n",
    "    - 통합형과 분리형 모두 지원\n",
    "    - 답변 없을 시 3단계 폴백 시스템\n",
    "    - 품질 검증 포함\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n",
    "        \"\"\"\n",
    "        초기화\n",
    "        \n",
    "        Args:\n",
    "            config: 실험 설정\n",
    "            rag_system: RAG 시스템 (선택적)\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.generation_mode = config.GENERATION_MODE\n",
    "        self.rag = rag_system\n",
    "        \n",
    "        # 프롬프트 템플릿\n",
    "        self.prompts = EnhancedPromptTemplates(config.PROMPT_STYLE)\n",
    "        \n",
    "        # 모델 초기화\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.model_loaded = False\n",
    "        \n",
    "        # 통계\n",
    "        self.stats = {\n",
    "            'total_attempts': 0,\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'retry_count': 0,\n",
    "            'with_answer': 0,\n",
    "            'without_answer': 0,\n",
    "            'fallback_used': 0,\n",
    "            'mode_stats': {'integrated': 0, 'separated': 0}\n",
    "        }\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"모델 로드\"\"\"\n",
    "        if self.model_loaded:\n",
    "            return\n",
    "        \n",
    "        print(f\"🚀 모델 로딩: {self.config.MODEL_NAME}\")\n",
    "        print(f\"  양자화: {self.config.USE_QUANTIZATION}\")\n",
    "        \n",
    "        try:\n",
    "            # 토크나이저 로드\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config.MODEL_NAME,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if not self.tokenizer.pad_token:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # 모델 로드\n",
    "            if self.config.USE_QUANTIZATION:\n",
    "                bnb_config = BitsAndBytesConfig(**self.config.QUANTIZATION_CONFIG)\n",
    "                \n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.config.MODEL_NAME,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            else:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.config.MODEL_NAME,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            \n",
    "            self.model_loaded = True\n",
    "            print(\"✅ 모델 로드 완료!\")\n",
    "            \n",
    "            # GPU 메모리 사용량\n",
    "            if torch.cuda.is_available():\n",
    "                memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"💾 GPU 메모리: {memory:.2f}GB\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"모델 로드 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_text(self, prompt: str, max_tokens: int = None) -> str:\n",
    "        \"\"\"텍스트 생성\"\"\"\n",
    "        if not self.model_loaded:\n",
    "            self.load_model()\n",
    "        \n",
    "        max_tokens = max_tokens or self.config.GENERATION_PARAMS['max_new_tokens']\n",
    "        \n",
    "        # 토큰화\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2000\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # 생성\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=self.config.GENERATION_PARAMS['temperature'],\n",
    "                top_p=self.config.GENERATION_PARAMS['top_p'],\n",
    "                top_k=self.config.GENERATION_PARAMS['top_k'],\n",
    "                do_sample=self.config.GENERATION_PARAMS['do_sample'],\n",
    "                repetition_penalty=self.config.GENERATION_PARAMS['repetition_penalty'],\n",
    "                num_beams=self.config.GENERATION_PARAMS['num_beams'],\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 디코딩\n",
    "        generated = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return generated.strip()\n",
    "    \n",
    "    def generate_integrated(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        통합형 생성 (질문+답변 동시)\n",
    "        빠르고 일관성 있음\n",
    "        \"\"\"\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(f\"  [통합형] {question_type} 생성 중...\")\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        prompt_template = self.prompts.get_template(\n",
    "            \"integrated\", question_type, \"integrated\"\n",
    "        )\n",
    "        prompt = prompt_template.format(context=context[:1000])\n",
    "        \n",
    "        # 재시도 로직\n",
    "        max_retry = self.config.QUALITY_CONFIG['max_retry_attempts']\n",
    "        \n",
    "        for attempt in range(max_retry):\n",
    "            generated = self.generate_text(prompt)\n",
    "            result = self._parse_integrated_result(generated, question_type)\n",
    "            \n",
    "            if result and result.get('answer'):\n",
    "                # 답변 길이 확인\n",
    "                if len(result['answer']) >= self.config.QUALITY_CONFIG['min_answer_length']:\n",
    "                    return result\n",
    "            \n",
    "            self.stats['retry_count'] += 1\n",
    "            \n",
    "            if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                print(f\"    ⚠️ 답변 생성 실패, 재시도 {attempt + 1}/{max_retry}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def generate_separated(self, context: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        분리형 생성 (질문 먼저, 답변 나중에)\n",
    "        느리지만 더 정확한 답변 가능\n",
    "        \"\"\"\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(f\"  [분리형] {question_type} 생성 중...\")\n",
    "        \n",
    "        # 1단계: 질문 생성\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(\"    1/2. 질문 생성...\")\n",
    "        \n",
    "        q_template = self.prompts.get_template(\n",
    "            \"separated\", question_type, \"question\"\n",
    "        )\n",
    "        q_prompt = q_template.format(context=context[:1000])\n",
    "        \n",
    "        question_text = self.generate_text(q_prompt, max_tokens=200)\n",
    "        parsed_question = self._parse_question(question_text, question_type)\n",
    "        \n",
    "        if not parsed_question:\n",
    "            if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                print(\"    ❌ 질문 생성 실패\")\n",
    "            return None\n",
    "        \n",
    "        # 2단계: 답변 생성\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(\"    2/2. 답변 생성...\")\n",
    "        \n",
    "        a_template = self.prompts.get_template(\n",
    "            \"separated\", question_type, \"answer\"\n",
    "        )\n",
    "        a_prompt = a_template.format(\n",
    "            context=context[:1000],\n",
    "            question=parsed_question['question']\n",
    "        )\n",
    "        \n",
    "        answer_text = self.generate_text(a_prompt, max_tokens=300)\n",
    "        parsed_answer = self._parse_answer(answer_text, question_type)\n",
    "        \n",
    "        if not parsed_answer and self.config.QUALITY_CONFIG['use_fallback']:\n",
    "            # 답변 생성 실패 시 폴백\n",
    "            if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                print(\"    ⚠️ 폴백 답변 생성...\")\n",
    "            \n",
    "            parsed_answer = self._generate_fallback_answer(\n",
    "                context, parsed_question['question'], question_type\n",
    "            )\n",
    "            self.stats['fallback_used'] += 1\n",
    "        \n",
    "        # 결과 병합\n",
    "        result = {**parsed_question, **parsed_answer}\n",
    "        return result\n",
    "    \n",
    "    def generate_qa_pair(self, context: str, question_type: str = \"주관식\") -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        QA 쌍 생성 (메인 메서드)\n",
    "        \n",
    "        Args:\n",
    "            context: 참고 문서 내용\n",
    "            question_type: 문제 유형\n",
    "        \n",
    "        Returns:\n",
    "            생성된 QA 쌍\n",
    "        \"\"\"\n",
    "        self.stats['total_attempts'] += 1\n",
    "        \n",
    "        try:\n",
    "            # RAG 사용 시 컨텍스트 보강\n",
    "            if self.rag and self.config.RAG_CONFIG['use_rag']:\n",
    "                # 질문 유형을 쿼리로 사용\n",
    "                query = f\"{question_type} 문제 생성을 위한 {context[:100]}\"\n",
    "                retrieved = self.rag.search(query)\n",
    "                \n",
    "                if retrieved:\n",
    "                    # 검색된 문서 추가\n",
    "                    additional_context = \"\\n\".join([r['text'][:200] for r in retrieved[:2]])\n",
    "                    context = f\"{context}\\n\\n관련 문서:\\n{additional_context}\"\n",
    "            \n",
    "            # 생성 모드에 따라 분기\n",
    "            if self.generation_mode == \"integrated\":\n",
    "                result = self.generate_integrated(context, question_type)\n",
    "                self.stats['mode_stats']['integrated'] += 1\n",
    "            else:\n",
    "                result = self.generate_separated(context, question_type)\n",
    "                self.stats['mode_stats']['separated'] += 1\n",
    "            \n",
    "            if result:\n",
    "                # 답변 존재 확인\n",
    "                if result.get('answer'):\n",
    "                    self.stats['with_answer'] += 1\n",
    "                else:\n",
    "                    self.stats['without_answer'] += 1\n",
    "                    \n",
    "                    # 답변 없으면 기본 답변 생성\n",
    "                    if self.config.QUALITY_CONFIG['use_fallback']:\n",
    "                        result['answer'] = self._generate_basic_answer(\n",
    "                            context, result.get('question', ''), question_type\n",
    "                        )\n",
    "                \n",
    "                # 품질 검증\n",
    "                if self.config.QUALITY_CONFIG['use_validation']:\n",
    "                    quality = self._validate_quality(result)\n",
    "                    result['quality_score'] = quality\n",
    "                    \n",
    "                    # 품질 임계값 확인\n",
    "                    if quality < self.config.QUALITY_CONFIG['quality_threshold']:\n",
    "                        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "                            print(f\"    ⚠️ 품질 미달: {quality}/100\")\n",
    "                        # 품질 미달이어도 일단 반환 (나중에 필터링)\n",
    "                \n",
    "                # 메타데이터 추가\n",
    "                result['context'] = context[:500]  # 컨텍스트 일부만 저장\n",
    "                result['question_type'] = question_type\n",
    "                result['generation_mode'] = self.generation_mode\n",
    "                result['model'] = self.config.MODEL_NAME\n",
    "                result['timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                self.stats['successful'] += 1\n",
    "                return result\n",
    "            else:\n",
    "                self.stats['failed'] += 1\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"생성 오류: {e}\")\n",
    "            self.stats['failed'] += 1\n",
    "            return None\n",
    "    \n",
    "    def _parse_integrated_result(self, text: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"통합 결과 파싱\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        # 문제 추출\n",
    "        if '문제:' in text:\n",
    "            parts = text.split('문제:', 1)[1]\n",
    "            \n",
    "            # 정답 구분자 찾기\n",
    "            if '정답:' in parts:\n",
    "                result['question'] = parts.split('정답:')[0].strip()\n",
    "            elif '모범 답안:' in parts:\n",
    "                result['question'] = parts.split('모범 답안:')[0].strip()\n",
    "            elif '①' in parts:  # 객관식\n",
    "                result['question'] = parts.split('①')[0].strip()\n",
    "            else:\n",
    "                result['question'] = parts.split('\\n')[0].strip()\n",
    "        \n",
    "        # 정답 추출 (중요!)\n",
    "        if '정답:' in text:\n",
    "            answer_part = text.split('정답:', 1)[1]\n",
    "            \n",
    "            # 다음 섹션까지 추출\n",
    "            for delimiter in ['해설:', '핵심', '채점', '\\n\\n', '\\n문제:']:\n",
    "                if delimiter in answer_part:\n",
    "                    result['answer'] = answer_part.split(delimiter)[0].strip()\n",
    "                    break\n",
    "            else:\n",
    "                result['answer'] = answer_part.strip()\n",
    "        \n",
    "        elif '모범 답안:' in text:\n",
    "            answer_part = text.split('모범 답안:', 1)[1]\n",
    "            result['answer'] = answer_part.split('\\n\\n')[0].strip()\n",
    "        \n",
    "        # 추가 정보\n",
    "        if '해설:' in text:\n",
    "            result['explanation'] = text.split('해설:', 1)[1].strip()\n",
    "        \n",
    "        if '핵심 키워드:' in text:\n",
    "            result['keywords'] = text.split('핵심 키워드:', 1)[1].split('\\n')[0].strip()\n",
    "        \n",
    "        # 객관식 선택지\n",
    "        if question_type == \"객관식\":\n",
    "            choices = []\n",
    "            for marker in ['①', '②', '③', '④']:\n",
    "                if marker in text:\n",
    "                    idx = text.index(marker)\n",
    "                    choice_text = text[idx:]\n",
    "                    choice_line = choice_text.split('\\n')[0]\n",
    "                    choices.append(choice_line)\n",
    "            if choices:\n",
    "                result['choices'] = choices\n",
    "        \n",
    "        # 검증: 질문과 답변 모두 있어야 함\n",
    "        if 'question' in result and 'answer' in result:\n",
    "            return result\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _parse_question(self, text: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"질문 파싱 (분리형용)\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        if '문제:' in text:\n",
    "            question_part = text.split('문제:', 1)[1]\n",
    "            \n",
    "            if question_type == \"객관식\":\n",
    "                # 선택지 분리\n",
    "                if '①' in question_part:\n",
    "                    result['question'] = question_part.split('①')[0].strip()\n",
    "                    \n",
    "                    # 선택지 추출\n",
    "                    choices = []\n",
    "                    for marker in ['①', '②', '③', '④']:\n",
    "                        if marker in text:\n",
    "                            idx = text.index(marker)\n",
    "                            choice_text = text[idx:]\n",
    "                            choice_line = choice_text.split('\\n')[0]\n",
    "                            choices.append(choice_line)\n",
    "                    result['choices'] = choices\n",
    "                else:\n",
    "                    result['question'] = question_part.strip()\n",
    "            else:\n",
    "                result['question'] = question_part.strip()\n",
    "        else:\n",
    "            # 문제: 마커가 없는 경우\n",
    "            result['question'] = text.strip()\n",
    "        \n",
    "        return result if 'question' in result else None\n",
    "    \n",
    "    def _parse_answer(self, text: str, question_type: str) -> Optional[Dict]:\n",
    "        \"\"\"답변 파싱 (분리형용)\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        if '정답:' in text:\n",
    "            answer_part = text.split('정답:', 1)[1]\n",
    "            \n",
    "            # 다음 섹션까지 추출\n",
    "            for delimiter in ['해설:', '핵심 키워드:', '채점 기준:', '\\n\\n']:\n",
    "                if delimiter in answer_part:\n",
    "                    result['answer'] = answer_part.split(delimiter)[0].strip()\n",
    "                    break\n",
    "            else:\n",
    "                result['answer'] = answer_part.strip()\n",
    "        else:\n",
    "            # 정답: 마커가 없는 경우\n",
    "            result['answer'] = text.strip()\n",
    "        \n",
    "        # 추가 정보\n",
    "        if '해설:' in text:\n",
    "            result['explanation'] = text.split('해설:', 1)[1].strip()\n",
    "        \n",
    "        if '핵심 키워드:' in text:\n",
    "            result['keywords'] = text.split('핵심 키워드:', 1)[1].split('\\n')[0].strip()\n",
    "        \n",
    "        return result if 'answer' in result else None\n",
    "    \n",
    "    def _generate_fallback_answer(self, context: str, question: str, question_type: str) -> Dict:\n",
    "        \"\"\"폴백 답변 생성 (답변 생성 실패 시)\"\"\"\n",
    "        if self.config.EXPERIMENT_MODE['verbose']:\n",
    "            print(\"      🔄 폴백 답변 생성 중...\")\n",
    "        \n",
    "        # 간단한 프롬프트로 답변만 생성\n",
    "        simple_prompt = f\"\"\"다음 질문에 답하세요.\n",
    "\n",
    "참고: {context[:400]}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    "        \n",
    "        answer = self.generate_text(simple_prompt, max_tokens=200)\n",
    "        \n",
    "        return {\n",
    "            'answer': answer if answer else \"[답변 생성 실패 - 수동 작성 필요]\",\n",
    "            'fallback': True\n",
    "        }\n",
    "    \n",
    "    def _generate_basic_answer(self, context: str, question: str, question_type: str) -> str:\n",
    "        \"\"\"기본 답변 생성 (매우 간단)\"\"\"\n",
    "        # 컨텍스트에서 관련 부분 추출\n",
    "        sentences = context.split('.')\n",
    "        \n",
    "        # 질문과 가장 관련있는 문장 찾기\n",
    "        question_words = set(question.lower().split())\n",
    "        best_sentence = \"\"\n",
    "        best_score = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_words = set(sentence.lower().split())\n",
    "            score = len(question_words & sentence_words)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sentence = sentence.strip()\n",
    "        \n",
    "        if best_sentence:\n",
    "            return best_sentence + \".\"\n",
    "        else:\n",
    "            return \"[컨텍스트 기반 답변 생성 필요]\"\n",
    "    \n",
    "    def _validate_quality(self, qa_pair: Dict) -> float:\n",
    "        \"\"\"품질 검증 (0-100 점수)\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # 기본 점수\n",
    "        if qa_pair.get('question'):\n",
    "            score += 20\n",
    "            if len(qa_pair['question']) > 20:\n",
    "                score += 10\n",
    "        \n",
    "        if qa_pair.get('answer'):\n",
    "            score += 30\n",
    "            if len(qa_pair['answer']) > self.config.QUALITY_CONFIG['min_answer_length']:\n",
    "                score += 20\n",
    "        \n",
    "        # 추가 정보\n",
    "        if qa_pair.get('explanation'):\n",
    "            score += 10\n",
    "        \n",
    "        if qa_pair.get('keywords'):\n",
    "            score += 10\n",
    "        \n",
    "        # 폴백 사용 시 감점\n",
    "        if qa_pair.get('fallback'):\n",
    "            score -= 20\n",
    "        \n",
    "        return min(max(score, 0), 100)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"통계 반환\"\"\"\n",
    "        total = self.stats['total_attempts']\n",
    "        if total == 0:\n",
    "            return self.stats\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            'success_rate': round(self.stats['successful'] / total * 100, 1),\n",
    "            'answer_rate': round(\n",
    "                self.stats['with_answer'] / max(self.stats['successful'], 1) * 100, 1\n",
    "            ),\n",
    "            'retry_rate': round(self.stats['retry_count'] / total * 100, 1),\n",
    "            'fallback_rate': round(self.stats['fallback_used'] / total * 100, 1),\n",
    "        }\n",
    "\n",
    "print(\"✅ 답변 보장 생성기 클래스 정의 완료\")\n",
    "print(\"🔧 주요 기능:\")\n",
    "print(\"  - 통합형/분리형 선택 가능\")\n",
    "print(\"  - 3단계 폴백 시스템\")\n",
    "print(\"  - 품질 검증\")\n",
    "print(\"  - RAG 통합\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# 🎓 Chain-of-Thought (CoT) 생성기 - 단계별 사고 과정\n# ========================================\n#\n# 💡 CoT를 백엔드 관점에서 이해하기:\n#   일반 API: Request → Response (바로 응답)\n#   CoT API: Request → Think → Verify → Improve → Response (검증 후 응답)\n#   \n#   비유: 코드 리뷰 프로세스\n#   1. 초안 작성 (Initial) \n#   2. 셀프 리뷰 (Self-Verification)\n#   3. 수정 (Improvement)\n#   4. 최종 리뷰 (Final Check)\n\nclass ChainOfThoughtGenerator:\n    \"\"\"\n    Chain-of-Thought (CoT) 데이터 생성기\n    \n    🔄 4단계 검증 프로세스:\n    1. 초기 생성: 첫 번째 시도\n    2. 자가 검증: \"이게 맞나?\" 스스로 체크\n    3. 개선: \"이렇게 하면 더 낫겠다\" 수정\n    4. 최종 검증: \"이제 괜찮은가?\" 마지막 체크\n    \n    백엔드 비유: \n    - 초기 생성 = MVP 개발\n    - 자가 검증 = Unit Test\n    - 개선 = Refactoring  \n    - 최종 검증 = Integration Test\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n        \"\"\"\n        초기화\n        \n        💡 의존성 주입 패턴 사용\n        - config: 설정 객체 (application.yml 같은)\n        - rag_system: 선택적 의존성 (Optional dependency)\n        \"\"\"\n        self.config = config\n        self.cot_config = config.COT_CONFIG\n        self.rag = rag_system\n        \n        # 모델 관련 (lazy loading)\n        self.model = None\n        self.tokenizer = None\n        self.model_loaded = False\n        \n        # CoT 프롬프트 템플릿 로드\n        # 💡 각 단계별로 다른 프롬프트 사용 (Strategy Pattern)\n        self.cot_prompts = self._load_cot_prompts()\n        \n        # 캐시 설정 (동일 입력 재사용)\n        # 💡 백엔드의 Redis 캐싱과 유사\n        self.cache = {} if config.COT_CONFIG['cache_results'] else None\n        \n        # 통계 추적 (모니터링용)\n        self.stats = {\n            'total_attempts': 0,      # 총 시도\n            'successful': 0,           # 성공\n            'failed': 0,              # 실패\n            'avg_iterations': 0,      # 평균 반복 횟수\n            'improvement_count': 0,   # 개선 횟수\n            'cache_hits': 0,          # 캐시 히트\n            'quality_scores': []      # 품질 점수들\n        }\n    \n    def generate_qa_pair(self, context: str, question_type: str = \"주관식\") -> Optional[Dict]:\n        \"\"\"\n        CoT 방식으로 QA 쌍 생성 - 메인 엔트리 포인트\n        \n        🔄 실행 흐름:\n        1. 캐시 확인 (있으면 바로 반환)\n        2. 초기 생성\n        3. 품질 체크 루프 (최대 N번)\n           - 자가 검증\n           - 점수 확인\n           - 필요시 개선\n        4. 최종 검증\n        5. 결과 반환\n        \n        Args:\n            context: 참고 문서 (컨텍스트)\n            question_type: 문제 유형\n            \n        Returns:\n            생성된 QA 쌍 또는 None (실패 시)\n        \"\"\"\n        self.stats['total_attempts'] += 1\n        \n        # 1. 캐시 확인 (백엔드의 캐싱 레이어)\n        if self.cache is not None:\n            # 해시 키 생성 (context의 앞 200자로)\n            cache_key = hash(f\"{context[:200]}_{question_type}\")\n            if cache_key in self.cache:\n                self.stats['cache_hits'] += 1\n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(\"  💾 캐시에서 결과 반환 (Cache Hit\\!)\")\n                return self.cache[cache_key]\n        \n        try:\n            # 2. RAG 활용 (선택적)\n            # 💡 백엔드의 외부 API 호출과 유사\n            if self.rag and self.config.RAG_CONFIG['use_rag']:\n                query = f\"{question_type} 문제 생성을 위한 {context[:100]}\"\n                retrieved = self.rag.search(query)  # DB 검색\n                \n                if retrieved:\n                    # 검색 결과를 컨텍스트에 추가\n                    additional = \"\\n\".join([r['text'][:200] for r in retrieved[:2]])\n                    context = f\"{context}\\n\\n관련 문서:\\n{additional}\"\n            \n            # 3. 단계별 실행\n            if self.config.EXPERIMENT_MODE['verbose']:\n                print(f\"  [CoT] {question_type} 생성 시작...\")\n                print(\"    🔄 [1/4] 초기 문제 생성...\")\n            \n            # 3-1. 초기 생성 (첫 시도)\n            initial_qa = self._initial_generation(context, question_type)\n            if not initial_qa:\n                raise ValueError(\"초기 생성 실패\")\n            \n            current_question = initial_qa['question']\n            current_answer = initial_qa['answer']\n            \n            # 3-2. 반복적 개선 루프\n            iteration_count = 0\n            for i in range(self.cot_config['max_iterations']):\n                if not self.cot_config['use_self_verification']:\n                    break  # 자가 검증 비활성화면 스킵\n                    \n                iteration_count += 1\n                \n                # 자가 검증 단계\n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(f\"    🔄 [{2+i*2}/4] 자가 검증...\")\n                \n                # 현재 QA를 검증\n                verification = self._self_verification(current_question, current_answer)\n                \n                # 점수 추출 (1-10 → 1-100으로 변환)\n                score = self._extract_score(verification)\n                \n                # 품질 기준 통과 체크\n                if score >= self.cot_config['quality_threshold']:\n                    if self.config.EXPERIMENT_MODE['verbose']:\n                        print(f\"      ✅ 품질 기준 통과\\! (점수: {score}/100)\")\n                    break  # 충분히 좋으면 종료\n                \n                # 개선 필요\n                if not self.cot_config['use_improvement']:\n                    break  # 개선 비활성화면 스킵\n                    \n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(f\"    🔄 [{3+i*2}/4] 개선 생성...\")\n                    print(f\"      (현재 점수: {score}/100)\")\n                \n                # 피드백 기반 개선\n                improved_qa = self._improvement_generation(\n                    current_question, \n                    current_answer, \n                    verification  # 검증 피드백 전달\n                )\n                \n                if improved_qa:\n                    # 개선된 버전으로 교체\n                    current_question = improved_qa['question']\n                    current_answer = improved_qa['answer']\n                    self.stats['improvement_count'] += 1\n            \n            # 3-3. 최종 검증\n            final_score = 70  # 기본값\n            if self.cot_config['use_final_check']:\n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(\"    🔄 [4/4] 최종 검증...\")\n                \n                final_check = self._final_check(current_question, current_answer)\n                final_score = self._extract_final_score(final_check)\n                \n                if self.config.EXPERIMENT_MODE['verbose']:\n                    print(f\"      최종 품질 점수: {final_score}/100\")\n            \n            # 4. 결과 준비\n            result = {\n                'question': current_question,\n                'answer': current_answer,\n                'context': context[:500],\n                'question_type': question_type,\n                'generation_mode': 'cot',\n                'quality_score': final_score,\n                'iterations': iteration_count,  # 몇 번 개선했는지\n                'model': self.config.MODEL_NAME,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            # 5. 통계 업데이트\n            self.stats['successful'] += 1\n            self.stats['quality_scores'].append(final_score)\n            \n            # 6. 캐시 저장 (품질 좋은 것만)\n            if self.cache is not None and final_score >= self.cot_config['quality_threshold']:\n                cache_key = hash(f\"{context[:200]}_{question_type}\")\n                self.cache[cache_key] = result\n                \n            return result\n            \n        except Exception as e:\n            logger.error(f\"[CoT] 생성 오류: {e}\")\n            self.stats['failed'] += 1\n            return None\n    \n    def _initial_generation(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        1단계: 초기 생성\n        \n        💡 첫 번째 시도 - MVP처럼 빠르게 만들기\n        Temperature를 약간 높게(0.7) 설정해서 창의적인 문제 생성\n        \"\"\"\n        prompt = self.cot_prompts['initial_generation'].format(\n            context=context[:800],  # 너무 길면 잘라서\n            question_type=question_type\n        )\n        \n        # 초기 생성은 약간 창의적으로 (temperature 높게)\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_initial', 0.7)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        # 생성된 텍스트에서 Q&A 추출\n        return self._parse_qa(generated)\n    \n    def _self_verification(self, question: str, answer: str) -> str:\n        \"\"\"\n        2단계: 자가 검증\n        \n        💡 스스로 체크 - Code Review처럼\n        Temperature를 낮게(0.3) 설정해서 비판적으로 검토\n        \n        체크 포인트:\n        - 문제가 명확한가?\n        - 답변이 정확한가?\n        - 금융 용어가 올바른가?\n        \"\"\"\n        prompt = self.cot_prompts['self_verification'].format(\n            question=question,\n            answer=answer\n        )\n        \n        # 검증은 보수적으로 (temperature 낮게)\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_verification', 0.3)\n        verification = self.generate_text(prompt, temperature=temp, max_tokens=300)\n        \n        return verification\n    \n    def _improvement_generation(self, question: str, answer: str, feedback: str) -> Optional[Dict]:\n        \"\"\"\n        3단계: 개선 생성\n        \n        💡 피드백 반영 - Refactoring처럼\n        검증에서 나온 문제점을 수정\n        \n        개선 전략:\n        - 지적된 문제점 수정\n        - 더 명확한 표현 사용\n        - 구체적 예시 추가\n        \"\"\"\n        # 질문 기반 RAG 재검색 (더 나은 답변을 위해)\n        enhanced_context = \"\"\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            # 질문으로 더 정확한 문서 검색\n            retrieved = self.rag.search(question, top_k=5)\n            \n            if retrieved:\n                # 검색 결과를 추가 컨텍스트로\n                enhanced_context = \"\\n\\n### 질문 관련 참고 자료:\\n\" + \"\\n\".join([\n                    f\"- {doc['text'][:200]}\"\n                    for doc in retrieved[:3]\n                ])\n        \n        # 개선 프롬프트 생성\n        prompt = f\"\"\"{self.cot_prompts['improvement']}\n\n{enhanced_context}\"\"\".format(\n            question=question,\n            answer=answer,\n            feedback=feedback\n        )\n        \n        # 개선은 중간 온도로 (균형)\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_improvement', 0.5)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        return self._parse_qa(generated)\n    \n    def _final_check(self, question: str, answer: str) -> str:\n        \"\"\"\n        4단계: 최종 검증\n        \n        💡 마지막 체크 - Production 배포 전 체크처럼\n        FSKU 시험 출제 가능한 수준인지 최종 확인\n        \"\"\"\n        prompt = self.cot_prompts['final_check'].format(\n            question=question,\n            answer=answer\n        )\n        \n        # 최종 검증도 보수적으로\n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_final', 0.3)\n        check_result = self.generate_text(prompt, temperature=temp, max_tokens=200)\n        \n        return check_result\n    \n    def get_stats(self) -> Dict:\n        \"\"\"\n        통계 반환 - 모니터링용\n        \n        💡 백엔드의 메트릭 수집과 유사\n        - 성공률 = API Success Rate\n        - 개선률 = Cache Hit Rate  \n        - 품질 점수 = Response Time\n        \"\"\"\n        total = self.stats['total_attempts']\n        if total == 0:\n            return self.stats\n        \n        avg_quality = np.mean(self.stats['quality_scores']) if self.stats['quality_scores'] else 0\n        \n        return {\n            **self.stats,\n            'success_rate': round(self.stats['successful'] / total * 100, 1),\n            'improvement_rate': round(self.stats['improvement_count'] / max(self.stats['successful'], 1) * 100, 1),\n            'cache_hit_rate': round(self.stats['cache_hits'] / total * 100, 1) if self.cache else 0,\n            'avg_quality_score': round(avg_quality, 1)\n        }\n\nprint(\"✅ Chain-of-Thought (CoT) 생성기 정의 완료\\!\")\nprint(\"\\n🎯 CoT 사용 시점:\")\nprint(\"  - 일반 생성: 대량 데이터가 필요할 때 (속도 우선)\")\nprint(\"  - CoT 생성: 고품질 데이터가 필요할 때 (품질 우선)\")\nprint(\"\\n💡 백엔드 비유:\")\nprint(\"  - 일반 = Sync API (바로 응답)\")\nprint(\"  - CoT = Async + Queue + Retry (신중한 처리)\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 4-1. Chain-of-Thought (CoT) 데이터 생성기 ⭐",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 비교 테스트: 통합형 vs 분리형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_modes():\n",
    "    \"\"\"\n",
    "    통합형과 분리형 비교 테스트\n",
    "    실제로 두 모드를 실행하여 성능 비교\n",
    "    \"\"\"\n",
    "    \n",
    "    # 테스트용 컨텍스트\n",
    "    test_contexts = [\n",
    "        \"\"\"금융보안원(FSI)은 국내 금융 IT 보안을 총괄하는 전문기관으로, \n",
    "        금융권 사이버 보안 강화를 위한 다양한 정책과 기술을 개발하고 있습니다. \n",
    "        특히 전자금융거래의 안전성 확보를 위해 다단계 인증, 암호화 기술, \n",
    "        이상거래 탐지 시스템 등을 운영하고 있으며, 금융회사들의 보안 수준을 \n",
    "        정기적으로 점검하고 평가하는 역할을 수행합니다.\"\"\",\n",
    "        \n",
    "        \"\"\"바젤III 규제는 은행의 자본 적정성을 강화하기 위한 국제 기준으로,\n",
    "        보통주자본비율 4.5%, Tier1 자본비율 6%, 총자본비율 8% 이상을\n",
    "        유지해야 합니다. 또한 유동성 커버리지 비율(LCR)과 순안정자금조달비율(NSFR)을\n",
    "        도입하여 은행의 유동성 리스크 관리를 강화했습니다.\"\"\",\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"📊 통합형 vs 분리형 비교 테스트\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. 통합형 테스트\n",
    "    print(\"\\n1️⃣ 통합형 모드 테스트\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # 통합형 설정\n",
    "    integrated_config = ExperimentConfig()\n",
    "    integrated_config.GENERATION_MODE = \"integrated\"\n",
    "    integrated_config.EXPERIMENT_MODE['verbose'] = True\n",
    "    \n",
    "    integrated_gen = AnswerGuaranteedGenerator(integrated_config)\n",
    "    \n",
    "    integrated_results = []\n",
    "    integrated_times = []\n",
    "    \n",
    "    for i, context in enumerate(test_contexts):\n",
    "        for question_type in [\"객관식\", \"주관식\", \"단답형\"]:\n",
    "            print(f\"\\n테스트 {i+1} - {question_type}:\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = integrated_gen.generate_qa_pair(context, question_type)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if result:\n",
    "                integrated_results.append(result)\n",
    "                integrated_times.append(elapsed)\n",
    "                \n",
    "                print(f\"  ✅ 성공 ({elapsed:.2f}초)\")\n",
    "                print(f\"  답변 길이: {len(result.get('answer', ''))} 글자\")\n",
    "                print(f\"  품질 점수: {result.get('quality_score', 0):.0f}/100\")\n",
    "            else:\n",
    "                print(f\"  ❌ 실패\")\n",
    "    \n",
    "    results['integrated'] = {\n",
    "        'data': integrated_results,\n",
    "        'times': integrated_times,\n",
    "        'stats': integrated_gen.get_stats()\n",
    "    }\n",
    "    \n",
    "    # 2. 분리형 테스트\n",
    "    print(\"\\n\\n2️⃣ 분리형 모드 테스트\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # 분리형 설정\n",
    "    separated_config = ExperimentConfig()\n",
    "    separated_config.GENERATION_MODE = \"separated\"\n",
    "    separated_config.EXPERIMENT_MODE['verbose'] = True\n",
    "    \n",
    "    separated_gen = AnswerGuaranteedGenerator(separated_config)\n",
    "    \n",
    "    separated_results = []\n",
    "    separated_times = []\n",
    "    \n",
    "    for i, context in enumerate(test_contexts):\n",
    "        for question_type in [\"객관식\", \"주관식\", \"단답형\"]:\n",
    "            print(f\"\\n테스트 {i+1} - {question_type}:\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = separated_gen.generate_qa_pair(context, question_type)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if result:\n",
    "                separated_results.append(result)\n",
    "                separated_times.append(elapsed)\n",
    "                \n",
    "                print(f\"  ✅ 성공 ({elapsed:.2f}초)\")\n",
    "                print(f\"  답변 길이: {len(result.get('answer', ''))} 글자\")\n",
    "                print(f\"  품질 점수: {result.get('quality_score', 0):.0f}/100\")\n",
    "                print(f\"  폴백 사용: {result.get('fallback', False)}\")\n",
    "            else:\n",
    "                print(f\"  ❌ 실패\")\n",
    "    \n",
    "    results['separated'] = {\n",
    "        'data': separated_results,\n",
    "        'times': separated_times,\n",
    "        'stats': separated_gen.get_stats()\n",
    "    }\n",
    "    \n",
    "    # 3. 결과 비교\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📈 비교 결과 요약\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 통계 비교 테이블\n",
    "    comparison_data = []\n",
    "    \n",
    "    for mode in ['integrated', 'separated']:\n",
    "        mode_stats = results[mode]['stats']\n",
    "        mode_times = results[mode]['times']\n",
    "        mode_data = results[mode]['data']\n",
    "        \n",
    "        avg_time = np.mean(mode_times) if mode_times else 0\n",
    "        avg_answer_len = np.mean([len(d.get('answer', '')) for d in mode_data]) if mode_data else 0\n",
    "        avg_quality = np.mean([d.get('quality_score', 0) for d in mode_data]) if mode_data else 0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            '모드': mode.upper(),\n",
    "            '성공률': f\"{mode_stats.get('success_rate', 0):.1f}%\",\n",
    "            '답변 포함률': f\"{mode_stats.get('answer_rate', 0):.1f}%\",\n",
    "            '평균 시간': f\"{avg_time:.2f}초\",\n",
    "            '평균 답변 길이': f\"{avg_answer_len:.0f}자\",\n",
    "            '평균 품질': f\"{avg_quality:.0f}/100\",\n",
    "            '재시도율': f\"{mode_stats.get('retry_rate', 0):.1f}%\",\n",
    "            '폴백 사용률': f\"{mode_stats.get('fallback_rate', 0):.1f}%\"\n",
    "        })\n",
    "    \n",
    "    # 테이블 출력\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n비교 테이블:\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # 4. 샘플 출력\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📝 생성된 샘플 비교\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for mode in ['integrated', 'separated']:\n",
    "        print(f\"\\n[{mode.upper()} 모드 샘플]\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        if results[mode]['data']:\n",
    "            sample = results[mode]['data'][0]\n",
    "            print(f\"문제 유형: {sample.get('question_type')}\")\n",
    "            print(f\"문제: {sample.get('question', 'N/A')[:150]}...\")\n",
    "            print(f\"답변: {sample.get('answer', 'N/A')[:150]}...\")\n",
    "            print(f\"품질: {sample.get('quality_score', 0):.0f}/100\")\n",
    "    \n",
    "    # 5. 권장사항\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"💡 권장사항\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 속도 비교\n",
    "    integrated_avg_time = np.mean(results['integrated']['times']) if results['integrated']['times'] else 999\n",
    "    separated_avg_time = np.mean(results['separated']['times']) if results['separated']['times'] else 999\n",
    "    \n",
    "    if integrated_avg_time < separated_avg_time * 0.7:\n",
    "        print(\"🚀 속도 우선: 통합형 모드 추천 (약 {:.0f}% 빠름)\".format(\n",
    "            (1 - integrated_avg_time/separated_avg_time) * 100\n",
    "        ))\n",
    "    elif separated_avg_time < integrated_avg_time * 0.7:\n",
    "        print(\"🚀 속도 우선: 분리형 모드 추천 (약 {:.0f}% 빠름)\".format(\n",
    "            (1 - separated_avg_time/integrated_avg_time) * 100\n",
    "        ))\n",
    "    else:\n",
    "        print(\"⚖️ 속도: 두 모드 비슷함\")\n",
    "    \n",
    "    # 품질 비교\n",
    "    integrated_avg_quality = np.mean([d.get('quality_score', 0) for d in results['integrated']['data']])\n",
    "    separated_avg_quality = np.mean([d.get('quality_score', 0) for d in results['separated']['data']])\n",
    "    \n",
    "    if integrated_avg_quality > separated_avg_quality + 5:\n",
    "        print(\"⭐ 품질 우선: 통합형 모드 추천 (평균 {:.0f}점 높음)\".format(\n",
    "            integrated_avg_quality - separated_avg_quality\n",
    "        ))\n",
    "    elif separated_avg_quality > integrated_avg_quality + 5:\n",
    "        print(\"⭐ 품질 우선: 분리형 모드 추천 (평균 {:.0f}점 높음)\".format(\n",
    "            separated_avg_quality - integrated_avg_quality\n",
    "        ))\n",
    "    else:\n",
    "        print(\"⚖️ 품질: 두 모드 비슷함\")\n",
    "    \n",
    "    print(\"\\n📌 일반적 권장사항:\")\n",
    "    print(\"  - 대량 생성 시: 통합형 (빠름)\")\n",
    "    print(\"  - 고품질 필요 시: 분리형 (정확함)\")\n",
    "    print(\"  - 균형: 통합형 + 품질 검증\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 테스트 실행 (주석 해제하여 실행)\n",
    "# comparison_results = compare_generation_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 대량 데이터 생성 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bulk_generation():\n",
    "    \"\"\"\n",
    "    대량 데이터 생성 실행\n",
    "    설정에 따라 통합형 또는 분리형으로 실행\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"🚀 FSKU 대량 데이터 생성 시작\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 설정 출력\n",
    "    print(\"\\n📋 현재 설정:\")\n",
    "    print(f\"  - 모델: {config.MODEL_NAME}\")\n",
    "    print(f\"  - 생성 모드: {config.GENERATION_MODE}\")\n",
    "    print(f\"  - 목표 개수: {config.BATCH_CONFIG['target_count']}개\")\n",
    "    print(f\"  - Temperature: {config.GENERATION_PARAMS['temperature']}\")\n",
    "    print(f\"  - 품질 임계값: {config.QUALITY_CONFIG['quality_threshold']}\")\n",
    "    print(f\"  - RAG 사용: {config.RAG_CONFIG['use_rag']}\")\n",
    "    \n",
    "    # 1. RAG 시스템 초기화 (선택적)\n",
    "    rag_system = None\n",
    "    if config.RAG_CONFIG['use_rag']:\n",
    "        print(\"\\n📚 RAG 시스템 초기화...\")\n",
    "        rag_system = RAGSystem(config)\n",
    "        rag_system.initialize()\n",
    "    \n",
    "    # 2. 생성기 초기화\n",
    "    print(\"\\n🤖 생성기 초기화...\")\n",
    "    generator = AnswerGuaranteedGenerator(config, rag_system)\n",
    "    generator.load_model()\n",
    "    \n",
    "    # 3. 샘플 컨텍스트 준비\n",
    "    sample_contexts = [\n",
    "        \"\"\"전자금융거래법은 전자금융거래의 안전성과 신뢰성을 확보하기 위한 법률로,\n",
    "        금융회사는 전자금융거래 시 다단계 인증을 적용해야 하며,\n",
    "        거래 내역은 최소 5년간 보관해야 합니다.\"\"\",\n",
    "        \n",
    "        \"\"\"개인정보보호법에 따라 금융회사는 고객의 개인정보를 수집할 때\n",
    "        명확한 동의를 받아야 하며, 수집 목적 외 사용은 금지됩니다.\n",
    "        개인정보 유출 시 24시간 내 신고 의무가 있습니다.\"\"\",\n",
    "        \n",
    "        \"\"\"바젤III 규제는 은행의 자본 적정성을 강화하기 위한 국제 기준으로,\n",
    "        보통주자본비율 4.5%, Tier1 자본비율 6%, 총자본비율 8% 이상을\n",
    "        유지해야 합니다.\"\"\",\n",
    "        \n",
    "        \"\"\"금융실명거래법은 금융거래의 투명성을 확보하기 위한 법률로,\n",
    "        모든 금융거래는 실명으로 이루어져야 하며,\n",
    "        차명거래는 엄격히 금지됩니다.\"\"\",\n",
    "        \n",
    "        \"\"\"자금세탁방지제도(AML)는 불법자금의 세탁을 방지하기 위한 제도로,\n",
    "        금융회사는 고객확인의무(CDD)를 수행해야 하며,\n",
    "        의심거래는 FIU에 보고해야 합니다.\"\"\",\n",
    "    ]\n",
    "    \n",
    "    # 4. 대량 생성\n",
    "    print(f\"\\n📊 데이터 생성 시작 (목표: {config.BATCH_CONFIG['target_count']}개)\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    generated_data = []\n",
    "    attempts = 0\n",
    "    max_attempts = config.BATCH_CONFIG['target_count'] * config.BATCH_CONFIG['max_attempts_ratio']\n",
    "    \n",
    "    # 문제 유형별 목표\n",
    "    type_targets = {\n",
    "        qtype: int(config.BATCH_CONFIG['target_count'] * ratio)\n",
    "        for qtype, ratio in config.QUESTION_TYPE_DISTRIBUTION.items()\n",
    "    }\n",
    "    type_counts = {qtype: 0 for qtype in type_targets}\n",
    "    \n",
    "    # 진행 상황 표시 간격\n",
    "    progress_interval = max(config.BATCH_CONFIG['target_count'] // 10, 1)\n",
    "    save_interval = config.BATCH_CONFIG['save_interval']\n",
    "    \n",
    "    # 시작 시간\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while len(generated_data) < config.BATCH_CONFIG['target_count'] and attempts < max_attempts:\n",
    "        # 컨텍스트 선택\n",
    "        context = np.random.choice(sample_contexts)\n",
    "        \n",
    "        # 문제 유형 선택 (부족한 유형 우선)\n",
    "        remaining_types = [\n",
    "            qtype for qtype, target in type_targets.items()\n",
    "            if type_counts[qtype] < target\n",
    "        ]\n",
    "        \n",
    "        if remaining_types:\n",
    "            question_type = np.random.choice(remaining_types)\n",
    "        else:\n",
    "            question_type = np.random.choice(list(config.QUESTION_TYPE_DISTRIBUTION.keys()))\n",
    "        \n",
    "        # 생성\n",
    "        qa_pair = generator.generate_qa_pair(context, question_type)\n",
    "        \n",
    "        # 검증\n",
    "        if qa_pair and qa_pair.get('answer'):\n",
    "            # 품질 확인\n",
    "            if qa_pair.get('quality_score', 0) >= config.QUALITY_CONFIG['quality_threshold']:\n",
    "                generated_data.append(qa_pair)\n",
    "                type_counts[question_type] += 1\n",
    "                \n",
    "                # 진행 상황 출력\n",
    "                if len(generated_data) % progress_interval == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = len(generated_data) / elapsed\n",
    "                    eta = (config.BATCH_CONFIG['target_count'] - len(generated_data)) / rate\n",
    "                    \n",
    "                    print(f\"  진행: {len(generated_data)}/{config.BATCH_CONFIG['target_count']} \"\n",
    "                          f\"({len(generated_data)/config.BATCH_CONFIG['target_count']*100:.1f}%) \"\n",
    "                          f\"| 속도: {rate:.1f}개/초 | 예상 시간: {eta:.0f}초\")\n",
    "                \n",
    "                # 중간 저장\n",
    "                if len(generated_data) % save_interval == 0:\n",
    "                    temp_path = AUGMENTED_DIR / f\"temp_batch_{len(generated_data)}.json\"\n",
    "                    with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(generated_data, f, ensure_ascii=False, indent=2)\n",
    "                    print(f\"  💾 중간 저장: {temp_path}\")\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    # 생성 완료\n",
    "    elapsed_total = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ 생성 완료!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 통계 출력\n",
    "    print(f\"\\n📊 생성 통계:\")\n",
    "    print(f\"  - 총 생성: {len(generated_data)}개\")\n",
    "    print(f\"  - 총 시도: {attempts}회\")\n",
    "    print(f\"  - 성공률: {len(generated_data)/attempts*100:.1f}%\")\n",
    "    print(f\"  - 소요 시간: {elapsed_total:.1f}초\")\n",
    "    print(f\"  - 평균 속도: {len(generated_data)/elapsed_total:.2f}개/초\")\n",
    "    \n",
    "    print(\"\\n문제 유형별 분포:\")\n",
    "    for qtype, count in type_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {qtype}: {count}개 ({count/len(generated_data)*100:.1f}%)\")\n",
    "    \n",
    "    # 답변 통계\n",
    "    answer_lengths = [len(d['answer']) for d in generated_data]\n",
    "    quality_scores = [d.get('quality_score', 0) for d in generated_data]\n",
    "    \n",
    "    if answer_lengths:\n",
    "        print(f\"\\n답변 길이 통계:\")\n",
    "        print(f\"  - 평균: {np.mean(answer_lengths):.1f} 글자\")\n",
    "        print(f\"  - 최소: {np.min(answer_lengths)} 글자\")\n",
    "        print(f\"  - 최대: {np.max(answer_lengths)} 글자\")\n",
    "    \n",
    "    if quality_scores:\n",
    "        print(f\"\\n품질 점수 통계:\")\n",
    "        print(f\"  - 평균: {np.mean(quality_scores):.1f}/100\")\n",
    "        print(f\"  - 최소: {np.min(quality_scores):.0f}/100\")\n",
    "        print(f\"  - 최대: {np.max(quality_scores):.0f}/100\")\n",
    "    \n",
    "    # 생성기 통계\n",
    "    gen_stats = generator.get_stats()\n",
    "    print(f\"\\n생성기 통계:\")\n",
    "    print(f\"  - 답변 포함률: {gen_stats['answer_rate']:.1f}%\")\n",
    "    print(f\"  - 재시도율: {gen_stats['retry_rate']:.1f}%\")\n",
    "    print(f\"  - 폴백 사용률: {gen_stats['fallback_rate']:.1f}%\")\n",
    "    \n",
    "    # 5. 데이터 저장\n",
    "    if generated_data:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # JSON 저장\n",
    "        json_path = AUGMENTED_DIR / f\"fsku_data_{config.GENERATION_MODE}_{timestamp}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(generated_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # JSONL 저장\n",
    "        jsonl_path = AUGMENTED_DIR / f\"fsku_data_{config.GENERATION_MODE}_{timestamp}.jsonl\"\n",
    "        with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "            for item in generated_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"\\n💾 데이터 저장 완료:\")\n",
    "        print(f\"  - JSON: {json_path}\")\n",
    "        print(f\"  - JSONL: {jsonl_path}\")\n",
    "        print(f\"  - 총 {len(generated_data)}개 항목\")\n",
    "        \n",
    "        # 통계 저장\n",
    "        if config.EXPERIMENT_MODE['save_stats']:\n",
    "            stats_path = AUGMENTED_DIR / f\"stats_{config.GENERATION_MODE}_{timestamp}.json\"\n",
    "            stats_data = {\n",
    "                'config': {\n",
    "                    'model': config.MODEL_NAME,\n",
    "                    'mode': config.GENERATION_MODE,\n",
    "                    'temperature': config.GENERATION_PARAMS['temperature'],\n",
    "                    'quality_threshold': config.QUALITY_CONFIG['quality_threshold'],\n",
    "                },\n",
    "                'results': {\n",
    "                    'total_generated': len(generated_data),\n",
    "                    'total_attempts': attempts,\n",
    "                    'success_rate': len(generated_data)/attempts*100,\n",
    "                    'elapsed_time': elapsed_total,\n",
    "                    'type_distribution': type_counts,\n",
    "                    'answer_length_mean': np.mean(answer_lengths),\n",
    "                    'quality_score_mean': np.mean(quality_scores),\n",
    "                },\n",
    "                'generator_stats': gen_stats,\n",
    "                'timestamp': timestamp\n",
    "            }\n",
    "            \n",
    "            with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(stats_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"  - 통계: {stats_path}\")\n",
    "    \n",
    "    print(\"\\n✨ 모든 작업 완료!\")\n",
    "    return generated_data\n",
    "\n",
    "# 실행 (주석 해제하여 실행)\n",
    "# data = run_bulk_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 🧪 실험 실행 섹션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🧪 실험 실행\n",
    "# 아래 코드의 주석을 해제하여 실행하세요\n",
    "# ========================================\n",
    "\n",
    "print(\"🧪 실험 옵션:\")\n",
    "print(\"1. 단일 테스트: test_single_generation()\")\n",
    "print(\"2. 모드 비교: compare_generation_modes()\")\n",
    "print(\"3. 대량 생성: run_bulk_generation()\")\n",
    "print(\"\\n원하는 실험의 주석을 해제하고 실행하세요!\")\n",
    "\n",
    "# 1. 단일 생성 테스트 (빠른 테스트)\n",
    "def test_single_generation():\n",
    "    \"\"\"단일 QA 생성 테스트\"\"\"\n",
    "    print(\"\\n🔬 단일 생성 테스트\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # 테스트 컨텍스트\n",
    "    test_context = \"\"\"금융보안원은 금융 IT 보안을 담당하는 기관으로,\n",
    "    전자금융거래의 안전성 확보를 위해 다양한 보안 기술을 운영합니다.\"\"\"\n",
    "    \n",
    "    # 생성기 초기화\n",
    "    generator = AnswerGuaranteedGenerator(config)\n",
    "    generator.load_model()\n",
    "    \n",
    "    # 생성\n",
    "    result = generator.generate_qa_pair(test_context, \"주관식\")\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\n✅ 생성 성공!\")\n",
    "        print(f\"문제: {result['question']}\")\n",
    "        print(f\"답변: {result['answer']}\")\n",
    "        print(f\"품질: {result.get('quality_score', 0):.0f}/100\")\n",
    "        print(f\"모드: {result['generation_mode']}\")\n",
    "    else:\n",
    "        print(\"❌ 생성 실패\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# test_single_generation()\n",
    "\n",
    "# 2. 통합형 vs 분리형 비교\n",
    "# compare_generation_modes()\n",
    "\n",
    "# 3. 대량 생성 실행\n",
    "# data = run_bulk_generation()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# 🚀 개선된 RAG 활용 답변 생성기\n# ========================================\n\nclass ImprovedAnswerGenerator:\n    \"\"\"\n    개선된 답변 생성기\n    - 생성된 질문을 기반으로 RAG 재검색\n    - 더 정확한 답변 생성\n    \"\"\"\n    \n    def __init__(self, config: ExperimentConfig, rag_system: RAGSystem = None):\n        self.config = config\n        self.rag = rag_system\n        \n    def generate_answer_with_rag(self, question: str, original_context: str = None) -> str:\n        \"\"\"\n        질문 기반 RAG 검색 후 답변 생성\n        \n        Args:\n            question: 생성된 질문\n            original_context: 원본 컨텍스트 (선택적)\n            \n        Returns:\n            생성된 답변\n        \"\"\"\n        # 1. 질문으로 RAG 검색\n        if self.rag:\n            print(f\"  🔍 질문 기반 RAG 재검색...\")\n            retrieved_docs = self.rag.search(question, top_k=5)\n            \n            # 검색된 문서 결합\n            rag_context = \"\\n\\n\".join([\n                f\"[참고 {i+1}] {doc['text'][:300]}\"\n                for i, doc in enumerate(retrieved_docs[:3])\n            ])\n            \n            # 원본 컨텍스트와 결합\n            if original_context:\n                full_context = f\"\"\"원본 문서:\n{original_context[:500]}\n\n관련 참고 자료:\n{rag_context}\"\"\"\n            else:\n                full_context = rag_context\n                \n        else:\n            full_context = original_context or \"\"\n            \n        # 2. 답변 생성 프롬프트\n        answer_prompt = f\"\"\"당신은 금융 전문가입니다. 다음 질문에 대해 제공된 참고 자료를 활용하여 정확하고 상세한 답변을 작성하세요.\n\n### 참고 자료:\n{full_context}\n\n### 질문:\n{question}\n\n### 답변 작성 지침:\n1. 참고 자료의 정보를 정확히 인용하세요\n2. 금융 전문 용어를 정확히 사용하세요\n3. 구체적인 수치나 규정이 있다면 반드시 포함하세요\n4. 답변은 완전한 문장으로 작성하세요\n5. 핵심 내용을 먼저 제시하고 부가 설명을 추가하세요\n\n### 답변:\"\"\"\n        \n        return answer_prompt\n    \n    def separated_generation_improved(self, context: str, question_type: str) -> Dict:\n        \"\"\"\n        개선된 분리형 생성 (RAG 활용 강화)\n        \"\"\"\n        # 1단계: 질문 생성 (기존과 동일)\n        question_prompt = self.get_question_prompt(context, question_type)\n        question = self.generate_text(question_prompt)\n        \n        # 2단계: 질문 기반 RAG 검색 후 답변 생성\n        answer_prompt = self.generate_answer_with_rag(question, context)\n        answer = self.generate_text(answer_prompt)\n        \n        return {\n            'question': question,\n            'answer': answer,\n            'mode': 'separated_improved',\n            'rag_used': self.rag is not None\n        }\n\n# AnswerGuaranteedGenerator 클래스 개선\nclass AnswerGuaranteedGeneratorV2(AnswerGuaranteedGenerator):\n    \"\"\"\n    개선된 답변 보장 생성기 (RAG 활용 강화)\n    \"\"\"\n    \n    def generate_qa_pair(self, context: str, question_type: str = \"주관식\") -> Optional[Dict]:\n        \"\"\"\n        QA 쌍 생성 (개선된 버전)\n        \"\"\"\n        self.stats['total_attempts'] += 1\n        \n        try:\n            if self.generation_mode == \"integrated\":\n                # 통합형: 기존과 동일\n                return self._generate_integrated(context, question_type)\n                \n            elif self.generation_mode == \"separated\":\n                # 분리형: RAG 개선 적용\n                return self._generate_separated_improved(context, question_type)\n                \n            else:\n                raise ValueError(f\"지원하지 않는 모드: {self.generation_mode}\")\n                \n        except Exception as e:\n            logger.error(f\"QA 생성 오류: {e}\")\n            self.stats['failed'] += 1\n            return None\n    \n    def _generate_separated_improved(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        개선된 분리형 생성 (질문 기반 RAG 재검색)\n        \"\"\"\n        if self.config.EXPERIMENT_MODE['verbose']:\n            print(f\"  [분리형-개선] {question_type} 생성...\")\n            \n        # 1단계: 질문 생성\n        if self.config.EXPERIMENT_MODE['verbose']:\n            print(\"    📝 질문 생성 중...\")\n            \n        question_prompt = self.prompts.get_separated_question_prompt(context, question_type)\n        question = self.generate_text(question_prompt, max_tokens=150)\n        \n        if not question or len(question.strip()) < 10:\n            return None\n            \n        # 2단계: 질문 기반 RAG 재검색\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            if self.config.EXPERIMENT_MODE['verbose']:\n                print(f\"    🔍 질문 기반 RAG 재검색...\")\n                \n            # 질문으로 관련 문서 검색\n            retrieved_docs = self.rag.search(question, top_k=5)\n            \n            # 검색된 문서를 컨텍스트에 추가\n            rag_context = \"\\n\\n\".join([\n                f\"[참고 {i+1}] {doc['text'][:400]}\"\n                for i, doc in enumerate(retrieved_docs[:3])\n            ])\n            \n            # 원본 컨텍스트와 결합\n            enhanced_context = f\"\"\"### 원본 문서:\n{context[:600]}\n\n### 질문 관련 추가 참고 자료:\n{rag_context}\"\"\"\n        else:\n            enhanced_context = context\n            \n        # 3단계: 강화된 컨텍스트로 답변 생성\n        if self.config.EXPERIMENT_MODE['verbose']:\n            print(\"    💡 답변 생성 중 (RAG 강화)...\")\n            \n        answer_prompt = f\"\"\"당신은 한국 금융감독원의 FSKU 시험 출제위원입니다.\n다음 질문에 대해 제공된 참고 자료를 활용하여 정확하고 완전한 답변을 작성하세요.\n\n{enhanced_context}\n\n### 질문:\n{question}\n\n### 답변 작성 지침:\n1. 제공된 참고 자료의 정보를 정확히 활용하세요\n2. 금융 전문 용어와 수치를 정확히 포함하세요\n3. FSKU 시험 답안 수준의 완성도를 갖추세요\n4. 핵심 내용을 먼저, 부가 설명을 나중에 제시하세요\n5. 답변은 완전한 문장으로 작성하세요\n\n### 모범 답안:\"\"\"\n        \n        answer = self.generate_text(answer_prompt, max_tokens=300)\n        \n        # 답변 검증\n        if not answer or len(answer.strip()) < self.config.QUALITY_CONFIG['min_answer_length']:\n            # 폴백: 간단한 프롬프트로 재시도\n            if self.config.EXPERIMENT_MODE['verbose']:\n                print(\"    ⚠️ 답변 부족, 폴백 시도...\")\n                \n            fallback_prompt = f\"질문: {question}\\n정답:\"\n            answer = self.generate_text(fallback_prompt, max_tokens=200)\n            self.stats['fallback_used'] += 1\n            \n        # 결과 생성\n        result = {\n            'question': question.strip(),\n            'answer': answer.strip(),\n            'context': context[:500],\n            'question_type': question_type,\n            'generation_mode': 'separated_improved',\n            'rag_enhanced': self.rag is not None,\n            'quality_score': self._calculate_quality_score(question, answer),\n            'model': self.config.MODEL_NAME,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        self.stats['successful'] += 1\n        self.stats['mode_stats']['separated'] += 1\n        if answer and len(answer.strip()) > 10:\n            self.stats['with_answer'] += 1\n        else:\n            self.stats['without_answer'] += 1\n            \n        return result\n\n# ChainOfThoughtGenerator 개선\nclass ChainOfThoughtGeneratorV2(ChainOfThoughtGenerator):\n    \"\"\"\n    개선된 CoT 생성기 (RAG 활용 강화)\n    \"\"\"\n    \n    def _initial_generation(self, context: str, question_type: str) -> Optional[Dict]:\n        \"\"\"\n        1단계: 초기 생성 (RAG 활용)\n        \"\"\"\n        # RAG로 컨텍스트 보강\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            # 주제 추출을 위한 간단한 검색\n            query = f\"{question_type} 문제 생성을 위한 {context[:100]}\"\n            retrieved = self.rag.search(query, top_k=3)\n            \n            if retrieved:\n                additional = \"\\n\\n\".join([\n                    f\"[참고] {doc['text'][:300]}\"\n                    for doc in retrieved[:2]\n                ])\n                enhanced_context = f\"{context}\\n\\n### 추가 참고 자료:\\n{additional}\"\n            else:\n                enhanced_context = context\n        else:\n            enhanced_context = context\n            \n        prompt = self.cot_prompts['initial_generation'].format(\n            context=enhanced_context[:1000],\n            question_type=question_type\n        )\n        \n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_initial', 0.7)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        return self._parse_qa(generated)\n    \n    def _improvement_generation(self, question: str, answer: str, feedback: str) -> Optional[Dict]:\n        \"\"\"\n        3단계: 개선 생성 (질문 기반 RAG 재검색)\n        \"\"\"\n        # 생성된 질문으로 RAG 재검색\n        enhanced_context = \"\"\n        if self.rag and self.config.RAG_CONFIG['use_rag']:\n            # 질문으로 더 정확한 문서 검색\n            retrieved = self.rag.search(question, top_k=5)\n            \n            if retrieved:\n                enhanced_context = \"\\n\\n### 질문 관련 참고 자료:\\n\" + \"\\n\".join([\n                    f\"- {doc['text'][:200]}\"\n                    for doc in retrieved[:3]\n                ])\n        \n        prompt = f\"\"\"{self.cot_prompts['improvement']}\n\n{enhanced_context}\"\"\".format(\n            question=question,\n            answer=answer,\n            feedback=feedback\n        )\n        \n        temp = self.config.COT_GENERATION_PARAMS.get('temperature_improvement', 0.5)\n        generated = self.generate_text(prompt, temperature=temp)\n        \n        return self._parse_qa(generated)\n\nprint(\"✅ RAG 활용 개선 완료\\!\")\nprint(\"\\n🚀 개선된 기능:\")\nprint(\"  1. 분리형: 질문 생성 → 질문으로 RAG 재검색 → 답변 생성\")\nprint(\"  2. CoT: 각 단계에서 RAG 활용 강화\")\nprint(\"  3. 답변 생성 시 질문 관련 문서 우선 참조\")\nprint(\"\\n💡 사용법:\")\nprint(\"  - config.GENERATION_MODE = 'separated' 설정 후 실행\")\nprint(\"  - 자동으로 개선된 RAG 활용 적용됨\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ========================================\n# 🧪 CoT 통합 테스트 및 모드 비교\n# ========================================\n\ndef test_cot_generation():\n    \"\"\"\n    CoT 모드 단일 테스트\n    \"\"\"\n    print(\"=\"*80)\n    print(\"🧠 Chain-of-Thought (CoT) 테스트\")\n    print(\"=\"*80)\n    \n    # CoT 설정\n    test_config = ExperimentConfig()\n    test_config.GENERATION_MODE = \"cot\"\n    test_config.COT_CONFIG['use_cot'] = True\n    test_config.EXPERIMENT_MODE['verbose'] = True\n    \n    # RAG 시스템 초기화 (선택적)\n    rag = None\n    if test_config.RAG_CONFIG['use_rag']:\n        print(\"\\n📚 RAG 시스템 초기화...\")\n        rag = RAGSystem(test_config)\n        rag.initialize()\n    \n    # CoT 생성기 생성\n    print(\"\\n🧠 CoT 생성기 초기화...\")\n    cot_gen = ChainOfThoughtGenerator(test_config, rag)\n    cot_gen.load_model()\n    \n    # 테스트 컨텍스트\n    test_context = \"\"\"\n    바젤III 규제는 글로벌 금융위기 이후 은행의 자본 적정성과 유동성 관리를 강화하기 위해 도입되었습니다.\n    주요 내용으로는 보통주자본비율 4.5%, Tier1 자본비율 6%, 총자본비율 8% 이상 유지,\n    자본보전완충자본 2.5% 추가, 경기대응완충자본 0~2.5% 탄력 운영 등이 있습니다.\n    또한 레버리지 비율 3% 이상, 유동성 커버리지 비율(LCR) 100% 이상,\n    순안정자금조달비율(NSFR) 100% 이상을 요구합니다.\n    \"\"\"\n    \n    print(\"\\n🔬 CoT 4단계 프로세스 시작...\")\n    print(\"  1️⃣ 초기 생성\")\n    print(\"  2️⃣ 자가 검증\")\n    print(\"  3️⃣ 개선 생성\")\n    print(\"  4️⃣ 최종 검증\")\n    \n    # CoT 생성 실행\n    result = cot_gen.generate_qa_pair(test_context, \"주관식\")\n    \n    if result:\n        print(\"\\n✅ CoT 생성 성공\\!\")\n        print(f\"\\n📝 생성된 문제:\")\n        print(f\"  {result['question'][:200]}...\")\n        print(f\"\\n💡 생성된 답변:\")\n        print(f\"  {result['answer'][:200]}...\")\n        print(f\"\\n📊 품질 점수: {result.get('quality_score', 0)}/100\")\n        print(f\"🔄 개선 반복 횟수: {result.get('iterations', 0)}회\")\n        \n        # 통계 출력\n        stats = cot_gen.get_stats()\n        print(f\"\\n📈 CoT 통계:\")\n        print(f\"  - 성공률: {stats.get('success_rate', 0):.1f}%\")\n        print(f\"  - 개선률: {stats.get('improvement_rate', 0):.1f}%\")\n        print(f\"  - 평균 품질: {stats.get('avg_quality_score', 0):.1f}\")\n        print(f\"  - 캐시 적중률: {stats.get('cache_hit_rate', 0):.1f}%\")\n    else:\n        print(\"\\n❌ CoT 생성 실패\")\n    \n    return result\n\ndef compare_all_modes():\n    \"\"\"\n    통합형 vs 분리형 vs CoT 전체 비교\n    \"\"\"\n    print(\"=\"*80)\n    print(\"🔬 전체 모드 비교 테스트\")\n    print(\"=\"*80)\n    \n    # 테스트 설정\n    test_contexts = [\n        \"\"\"금융보안원(FSI)은 국내 금융 IT 보안을 총괄하는 전문기관으로,\n        금융권 사이버 보안 강화를 위한 다양한 정책과 기술을 개발합니다.\"\"\",\n        \n        \"\"\"파생상품은 기초자산의 가격 변동에 따라 가치가 결정되는 금융상품으로,\n        선물, 옵션, 스왑 등이 대표적입니다. 위험 헤지와 투기 목적으로 활용됩니다.\"\"\"\n    ]\n    \n    modes = [\"integrated\", \"separated\", \"cot\"]\n    results = {mode: [] for mode in modes}\n    \n    for mode in modes:\n        print(f\"\\n{'='*40}\")\n        print(f\"📋 {mode.upper()} 모드 테스트\")\n        print(f\"{'='*40}\")\n        \n        # 설정 생성\n        config = ExperimentConfig()\n        config.GENERATION_MODE = mode\n        if mode == \"cot\":\n            config.COT_CONFIG['use_cot'] = True\n            config.CURRENT_COT_PRESET = \"balanced\"\n        \n        # 생성기 생성\n        if mode == \"cot\":\n            generator = ChainOfThoughtGenerator(config)\n        else:\n            generator = AnswerGuaranteedGenerator(config)\n        \n        generator.load_model()\n        \n        # 각 컨텍스트에 대해 테스트\n        for i, context in enumerate(test_contexts):\n            print(f\"\\n테스트 {i+1}:\")\n            start_time = time.time()\n            \n            result = generator.generate_qa_pair(context, \"주관식\")\n            elapsed = time.time() - start_time\n            \n            if result:\n                results[mode].append({\n                    'result': result,\n                    'time': elapsed,\n                    'quality': result.get('quality_score', 70)\n                })\n                print(f\"  ✅ 성공 ({elapsed:.2f}초)\")\n                print(f\"  품질: {result.get('quality_score', 70)}/100\")\n            else:\n                print(f\"  ❌ 실패\")\n    \n    # 결과 비교\n    print(f\"\\n{'='*80}\")\n    print(\"📊 비교 결과\")\n    print(f\"{'='*80}\")\n    \n    for mode in modes:\n        if results[mode]:\n            avg_time = np.mean([r['time'] for r in results[mode]])\n            avg_quality = np.mean([r['quality'] for r in results[mode]])\n            success_rate = len(results[mode]) / len(test_contexts) * 100\n            \n            print(f\"\\n{mode.upper()} 모드:\")\n            print(f\"  - 성공률: {success_rate:.0f}%\")\n            print(f\"  - 평균 시간: {avg_time:.2f}초\")\n            print(f\"  - 평균 품질: {avg_quality:.1f}/100\")\n            \n            # 모드별 특징\n            if mode == \"integrated\":\n                print(\"  - 특징: 빠르고 일관성 있는 생성\")\n            elif mode == \"separated\":\n                print(\"  - 특징: 정확한 답변, 느린 속도\")\n            elif mode == \"cot\":\n                print(\"  - 특징: 최고 품질, 4단계 검증\")\n    \n    return results\n\ndef run_cot_experiments():\n    \"\"\"\n    CoT 실험 파라미터 테스트\n    \"\"\"\n    print(\"=\"*80)\n    print(\"🔬 CoT 실험 파라미터 테스트\")\n    print(\"=\"*80)\n    \n    # 테스트할 프리셋들\n    presets = [\"fast\", \"balanced\", \"quality\"]\n    preset_results = {}\n    \n    test_context = \"\"\"\n    중앙은행의 통화정책은 경제 안정화를 위한 핵심 도구입니다.\n    금리 조절, 공개시장조작, 지급준비율 조정 등을 통해\n    물가 안정과 완전 고용을 추구합니다.\n    \"\"\"\n    \n    for preset in presets:\n        print(f\"\\n🧪 프리셋 테스트: {preset}\")\n        print(\"-\"*40)\n        \n        # 설정\n        config = ExperimentConfig()\n        config.GENERATION_MODE = \"cot\"\n        config.CURRENT_COT_PRESET = preset\n        \n        # 프리셋 적용\n        if preset in config.COT_PRESETS:\n            for key, value in config.COT_PRESETS[preset].items():\n                if key in config.COT_CONFIG:\n                    config.COT_CONFIG[key] = value\n        \n        print(f\"  설정:\")\n        print(f\"    - 최대 반복: {config.COT_CONFIG['max_iterations']}\")\n        print(f\"    - 품질 임계값: {config.COT_CONFIG['quality_threshold']}\")\n        print(f\"    - 개선 사용: {config.COT_CONFIG['use_improvement']}\")\n        \n        # 생성기 생성\n        cot_gen = ChainOfThoughtGenerator(config)\n        cot_gen.load_model()\n        \n        # 테스트 실행\n        start_time = time.time()\n        result = cot_gen.generate_qa_pair(test_context, \"서술형\")\n        elapsed = time.time() - start_time\n        \n        if result:\n            preset_results[preset] = {\n                'time': elapsed,\n                'quality': result.get('quality_score', 0),\n                'iterations': result.get('iterations', 0)\n            }\n            \n            print(f\"\\n  결과:\")\n            print(f\"    - 시간: {elapsed:.2f}초\")\n            print(f\"    - 품질: {result.get('quality_score', 0)}/100\")\n            print(f\"    - 반복: {result.get('iterations', 0)}회\")\n    \n    # 프리셋 비교\n    print(f\"\\n{'='*80}\")\n    print(\"📊 프리셋 비교 결과\")\n    print(f\"{'='*80}\")\n    \n    print(f\"\\n{'프리셋':<12} {'시간(초)':<10} {'품질':<10} {'반복':<10}\")\n    print(\"-\"*42)\n    for preset, data in preset_results.items():\n        print(f\"{preset:<12} {data['time']:<10.2f} {data['quality']:<10.1f} {data['iterations']:<10}\")\n    \n    # 권장사항\n    print(f\"\\n💡 권장사항:\")\n    print(\"  - 빠른 프로토타이핑: 'fast' 프리셋\")\n    print(\"  - 일반 사용: 'balanced' 프리셋\")\n    print(\"  - 최고 품질: 'quality' 프리셋\")\n    print(\"  - 연구/논문: 'research' 프리셋 (별도 설정)\")\n    \n    return preset_results\n\n# 실행 메뉴\ndef show_cot_menu():\n    \"\"\"\n    CoT 실험 메뉴\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"🧠 Chain-of-Thought (CoT) 실험 메뉴\")\n    print(\"=\"*80)\n    print(\"\\n실행할 실험을 선택하세요:\")\n    print(\"\\n1. test_cot_generation()\")\n    print(\"   - CoT 단일 테스트\")\n    print(\"\\n2. compare_all_modes()\")\n    print(\"   - 통합형 vs 분리형 vs CoT 비교\")\n    print(\"\\n3. run_cot_experiments()\")\n    print(\"   - CoT 프리셋 실험\")\n    print(\"\\n4. run_bulk_generation()\")\n    print(\"   - 대량 데이터 생성 (기존 함수)\")\n    print(\"\\n예시: test_cot_generation()\")\n\nshow_cot_menu()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 📝 사용 가이드 및 팁\n",
    "\n",
    "### 실험 가능한 설정들\n",
    "\n",
    "1. **생성 모드 변경**\n",
    "```python\n",
    "config.GENERATION_MODE = \"integrated\"  # 또는 \"separated\"\n",
    "```\n",
    "\n",
    "2. **Temperature 조정** (창의성)\n",
    "```python\n",
    "config.GENERATION_PARAMS['temperature'] = 0.3  # 보수적\n",
    "config.GENERATION_PARAMS['temperature'] = 0.8  # 균형\n",
    "config.GENERATION_PARAMS['temperature'] = 1.0  # 창의적\n",
    "```\n",
    "\n",
    "3. **품질 임계값 변경**\n",
    "```python\n",
    "config.QUALITY_CONFIG['quality_threshold'] = 50  # 낮은 기준\n",
    "config.QUALITY_CONFIG['quality_threshold'] = 70  # 중간\n",
    "config.QUALITY_CONFIG['quality_threshold'] = 90  # 높은 기준\n",
    "```\n",
    "\n",
    "4. **모델 변경**\n",
    "```python\n",
    "config.MODEL_NAME = \"upstage/SOLAR-10.7B-v1.0\"  # 더 큰 모델\n",
    "```\n",
    "\n",
    "5. **RAG 활성화/비활성화**\n",
    "```python\n",
    "config.RAG_CONFIG['use_rag'] = False  # RAG 없이\n",
    "```\n",
    "\n",
    "### 성능 최적화 팁\n",
    "\n",
    "1. **속도 우선**\n",
    "   - 통합형 모드 사용\n",
    "   - Temperature 낮게 (0.3~0.5)\n",
    "   - max_new_tokens 줄이기 (200~300)\n",
    "   - RAG 비활성화\n",
    "\n",
    "2. **품질 우선**\n",
    "   - 분리형 모드 사용\n",
    "   - Temperature 중간 (0.7~0.8)\n",
    "   - 품질 임계값 높게 (80+)\n",
    "   - RAG 활성화\n",
    "\n",
    "3. **메모리 절약**\n",
    "   - 양자화 활성화\n",
    "   - 배치 크기 줄이기\n",
    "   - 작은 모델 사용\n",
    "\n",
    "### 문제 해결\n",
    "\n",
    "1. **답변이 생성되지 않을 때**\n",
    "   - 분리형 모드 시도\n",
    "   - Temperature 높이기\n",
    "   - 재시도 횟수 늘리기\n",
    "\n",
    "2. **품질이 낮을 때**\n",
    "   - 프롬프트 스타일을 'expert'로 변경\n",
    "   - 더 큰 모델 사용\n",
    "   - RAG 활성화\n",
    "\n",
    "3. **속도가 느릴 때**\n",
    "   - 통합형 모드 사용\n",
    "   - max_new_tokens 줄이기\n",
    "   - num_beams = 1로 설정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}