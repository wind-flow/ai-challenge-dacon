{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š FSKU_2_ëª¨ë¸í•™ìŠµ\n",
    "\n",
    "## ğŸ“‹ ë…¸íŠ¸ë¶ ê°œìš”\n",
    "\n",
    "### ëª©ì \n",
    "ì´ ë…¸íŠ¸ë¶ì€ FSKU ê¸ˆìœµ AI Challengeë¥¼ ìœ„í•œ ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ì…ë‹ˆë‹¤. `FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynb`ì—ì„œ ìƒì„±ëœ ì¦ê°• ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸ˆìœµ ì „ë¬¸ AI ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì…ë ¥ ë°ì´í„°\n",
    "- **ìœ„ì¹˜**: `data/augmented/` í´ë”\n",
    "- **í˜•ì‹**: JSON íŒŒì¼ (questions_*.json)\n",
    "- **ë‚´ìš©**: ì¦ê°•ëœ ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸-ë‹µë³€ ìŒ\n",
    "\n",
    "### ì¶œë ¥ë¬¼\n",
    "- **í•™ìŠµëœ ëª¨ë¸**: `models/fsku_finetuned_model/` (ì¶”ë¡  ë‹¨ê³„ì—ì„œ ì‚¬ìš©)\n",
    "- **ì²´í¬í¬ì¸íŠ¸**: `models/checkpoints/`\n",
    "- **í•™ìŠµ ë©”íŠ¸ë¦­**: `results/training_metrics.json`\n",
    "\n",
    "### í•µì‹¬ ì œì•½ì‚¬í•­\n",
    "- RTX 4090 24GB ë©”ëª¨ë¦¬ ì œí•œ\n",
    "- ë‹¨ì¼ LLMë§Œ ì‚¬ìš© (ì•™ìƒë¸” ë¶ˆê°€)\n",
    "- ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥\n",
    "- 270ë¶„ ë‚´ 515ë¬¸í•­ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì†ë„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸ ë° ìë™ ì„¤ì¹˜\nimport subprocess\nimport sys\n\ndef install_package(package):\n    \"\"\"íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ ì„¤ì¹˜\"\"\"\n    try:\n        __import__(package.split('>')[0].split('=')[0])\n    except ImportError:\n        print(f\"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n        print(f\"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ!\")\n\n# í•„ìˆ˜ íŒ¨í‚¤ì§€ ëª©ë¡\nrequired_packages = [\n    \"transformers>=4.36.0\",\n    \"peft>=0.7.0\",\n    \"bitsandbytes>=0.41.0\",\n    \"accelerate>=0.25.0\",\n    \"datasets\",\n    \"sentencepiece\",\n    \"protobuf\",\n    \"scipy\",\n    \"scikit-learn\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"psutil\",  # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ìš©\n    \"tensorboard\"  # í•™ìŠµ ëª¨ë‹ˆí„°ë§ìš©\n]\n\nprint(\"ğŸ” í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘...\")\nfor package in required_packages:\n    install_package(package)\nprint(\"\\nâœ… ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport os\nimport json\nimport glob\nimport random\nimport warnings\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# ë”¥ëŸ¬ë‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n    logging as transformers_logging\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom datasets import Dataset as HFDataset\n\n# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ê²½ê³  ë©”ì‹œì§€ ì„¤ì •\nwarnings.filterwarnings('ignore')\ntransformers_logging.set_verbosity_error()\n\n# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œê°í™”ìš©)\nimport platform\n\nif platform.system() == 'Darwin':  # macOS\n    plt.rcParams['font.family'] = 'AppleGothic'\nelif platform.system() == 'Windows':\n    plt.rcParams['font.family'] = 'Malgun Gothic'\nelse:  # Linux\n    plt.rcParams['font.family'] = 'NanumGothic'\nplt.rcParams['axes.unicode_minus'] = False\n\n# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§\nimport psutil\nprint(f\"\\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:\")\nprint(f\"  - CPU ì½”ì–´: {psutil.cpu_count(logical=False)}ê°œ (ë…¼ë¦¬: {psutil.cpu_count()}ê°œ)\")\nprint(f\"  - RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\nprint(f\"  - ì‚¬ìš© ê°€ëŠ¥ RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n\nprint(\"\\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\nprint(f\"ğŸ“Š PyTorch ë²„ì „: {torch.__version__}\")\nprint(f\"ğŸ–¥ï¸ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë“œ ê³ ì • (ì¬í˜„ì„±ì„ ìœ„í•´)\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë³´ì¥\n",
    "    \n",
    "    Args:\n",
    "        seed: ëœë¤ ì‹œë“œ ê°’\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"ğŸ² ëœë¤ ì‹œë“œ ê³ ì • ì™„ë£Œ (seed=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ë¡œë”© í•¨ìˆ˜\ndef load_augmented_data(data_dir: str = \"data/augmented\") -> List[Dict]:\n    \"\"\"\n    ì¦ê°•ëœ ë°ì´í„°ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n    \n    Args:\n        data_dir: ì¦ê°• ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n        \n    Returns:\n        ëª¨ë“  ì§ˆë¬¸-ë‹µë³€ ìŒì´ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸\n    \"\"\"\n    all_data = []\n    \n    # JSON íŒŒì¼ íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  ì¦ê°• ë°ì´í„° íŒŒì¼ ì°¾ê¸°\n    json_files = glob.glob(os.path.join(data_dir, \"questions_*.json\"))\n    \n    if not json_files:\n        print(f\"âš ï¸ ê²½ê³ : {data_dir}ì—ì„œ ì¦ê°• ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n        print(\"ğŸ’¡ ë¨¼ì € FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\")\n        return []\n    \n    print(f\"ğŸ“‚ ë°œê²¬ëœ ë°ì´í„° íŒŒì¼ ìˆ˜: {len(json_files)}ê°œ\")\n    \n    # ê° íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ )\n    failed_files = []\n    for file_path in tqdm(json_files, desc=\"ë°ì´í„° íŒŒì¼ ë¡œë”©\"):\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                # ë°ì´í„° ê²€ì¦\n                valid_data = []\n                for item in data:\n                    if 'question' in item and 'answer' in item:\n                        # ë¹ˆ ë¬¸ìì—´ ì œê±°\n                        if item['question'].strip() and item['answer'].strip():\n                            valid_data.append(item)\n                all_data.extend(valid_data)\n        except json.JSONDecodeError as e:\n            print(f\"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {file_path}\")\n            print(f\"   ì˜¤ë¥˜: {str(e)}\")\n            failed_files.append(file_path)\n        except Exception as e:\n            print(f\"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}\")\n            print(f\"   ì˜¤ë¥˜: {str(e)}\")\n            failed_files.append(file_path)\n    \n    if failed_files:\n        print(f\"\\nâš ï¸ {len(failed_files)}ê°œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨\")\n    \n    print(f\"\\nâœ… ì´ {len(all_data)}ê°œì˜ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n    return all_data\n\n# ë°ì´í„° ë¡œë“œ\nraw_data = load_augmented_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° í†µê³„ ë¶„ì„\ndef analyze_data(data: List[Dict]) -> None:\n    \"\"\"\n    ë¡œë“œëœ ë°ì´í„°ì˜ í†µê³„ ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  ì¶œë ¥\n    \n    Args:\n        data: ë¶„ì„í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n    \"\"\"\n    if not data:\n        print(\"âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n        return\n    \n    # ê¸°ë³¸ í†µê³„\n    print(\"ğŸ“Š ë°ì´í„° í†µê³„ ë¶„ì„\")\n    print(\"=\" * 50)\n    print(f\"ì´ ë°ì´í„° ìˆ˜: {len(data):,}ê°œ\")\n    \n    # ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬\n    question_types = {}\n    answer_lengths = []\n    \n    for item in data:\n        # ì§ˆë¬¸ ìœ í˜• ì¹´ìš´íŠ¸\n        q_type = item.get('question_type', 'unknown')\n        question_types[q_type] = question_types.get(q_type, 0) + 1\n        \n        # ë‹µë³€ ê¸¸ì´ ìˆ˜ì§‘\n        answer = item.get('answer', '')\n        answer_lengths.append(len(answer))\n    \n    # ì§ˆë¬¸ ìœ í˜• ì¶œë ¥\n    print(\"\\nğŸ“ ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:\")\n    for q_type, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True):\n        percentage = (count / len(data)) * 100\n        print(f\"  - {q_type}: {count:,}ê°œ ({percentage:.1f}%)\")\n    \n    # ë‹µë³€ ê¸¸ì´ í†µê³„\n    if answer_lengths:\n        print(f\"\\nğŸ“ ë‹µë³€ ê¸¸ì´ í†µê³„:\")\n        print(f\"  - í‰ê· : {np.mean(answer_lengths):.0f}ì\")\n        print(f\"  - ì¤‘ê°„ê°’: {np.median(answer_lengths):.0f}ì\")\n        print(f\"  - ìµœì†Œ: {np.min(answer_lengths)}ì\")\n        print(f\"  - ìµœëŒ€: {np.max(answer_lengths)}ì\")\n        print(f\"  - í‘œì¤€í¸ì°¨: {np.std(answer_lengths):.0f}ì\")\n        \n        # ì´ìƒì¹˜ ê²€ì¶œ\n        q1 = np.percentile(answer_lengths, 25)\n        q3 = np.percentile(answer_lengths, 75)\n        iqr = q3 - q1\n        outliers = sum(1 for l in answer_lengths if l < q1 - 1.5*iqr or l > q3 + 1.5*iqr)\n        if outliers > 0:\n            print(f\"  - ì´ìƒì¹˜: {outliers}ê°œ ({outliers/len(answer_lengths)*100:.1f}%)\")\n    \n    # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥\n    print(\"\\nğŸ” ìƒ˜í”Œ ë°ì´í„° (ì²« 3ê°œ):\")\n    for i, item in enumerate(data[:3]):\n        print(f\"\\n[ìƒ˜í”Œ {i+1}]\")\n        print(f\"ì§ˆë¬¸: {item.get('question', '')[:100]}...\")\n        print(f\"ë‹µë³€: {item.get('answer', '')[:100]}...\")\n        print(f\"ìœ í˜•: {item.get('question_type', 'unknown')}\")\n\n# ë°ì´í„° ë¶„ì„ ì‹¤í–‰\nif raw_data:\n    analyze_data(raw_data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "def split_data(data: List[Dict], test_size: float = 0.2, seed: int = 42) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• \n",
    "    \n",
    "    Args:\n",
    "        data: ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "        test_size: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2)\n",
    "        seed: ëœë¤ ì‹œë“œ\n",
    "        \n",
    "    Returns:\n",
    "        (í•™ìŠµ ë°ì´í„°, ê²€ì¦ ë°ì´í„°) íŠœí”Œ\n",
    "    \"\"\"\n",
    "    # ë°ì´í„° ì…”í”Œ\n",
    "    random.seed(seed)\n",
    "    shuffled_data = data.copy()\n",
    "    random.shuffle(shuffled_data)\n",
    "    \n",
    "    # ë¶„í•  ì§€ì  ê³„ì‚°\n",
    "    split_idx = int(len(shuffled_data) * (1 - test_size))\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í• \n",
    "    train_data = shuffled_data[:split_idx]\n",
    "    val_data = shuffled_data[split_idx:]\n",
    "    \n",
    "    print(f\"âœ‚ï¸ ë°ì´í„° ë¶„í•  ì™„ë£Œ!\")\n",
    "    print(f\"  - í•™ìŠµ ë°ì´í„°: {len(train_data):,}ê°œ ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"  - ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "if raw_data:\n",
    "    train_data, val_data = split_data(raw_data)\n",
    "else:\n",
    "    train_data, val_data = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ì„ íƒ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡\n",
    "AVAILABLE_MODELS = {\n",
    "    \"exaone\": {\n",
    "        \"name\": \"LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "        \"description\": \"LG AI Researchì˜ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (ì¶”ì²œ)\",\n",
    "        \"size\": \"7.8B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"solar\": {\n",
    "        \"name\": \"upstage/SOLAR-10.7B-v1.0\",\n",
    "        \"description\": \"Upstageì˜ í•œêµ­ì–´ ê°•í™” ëª¨ë¸\",\n",
    "        \"size\": \"10.7B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"qwen\": {\n",
    "        \"name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"description\": \"ë‹¤êµ­ì–´ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ëª¨ë¸\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": False\n",
    "    },\n",
    "    \"llama-ko\": {\n",
    "        \"name\": \"beomi/llama-2-ko-7b\",\n",
    "        \"description\": \"í•œêµ­ì–´ë¡œ íŒŒì¸íŠœë‹ëœ Llama ëª¨ë¸\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:\")\n",
    "print(\"=\" * 60)\n",
    "for key, info in AVAILABLE_MODELS.items():\n",
    "    print(f\"\\n[{key}] {info['name']}\")\n",
    "    print(f\"  - ì„¤ëª…: {info['description']}\")\n",
    "    print(f\"  - í¬ê¸°: {info['size']}\")\n",
    "    print(f\"  - í•œêµ­ì–´ íŠ¹í™”: {'âœ…' if info['korean_specialized'] else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ ì„ íƒ (í™˜ê²½ë³€ìˆ˜ë¡œë„ ì„¤ì • ê°€ëŠ¥)\nimport os\n\n# ì˜µì…˜: \"exaone\", \"solar\", \"qwen\", \"llama-ko\"\nSELECTED_MODEL = os.getenv('FSKU_MODEL', 'exaone')  # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’\n\n# ëª¨ë¸ ì„ íƒ ê²€ì¦\nif SELECTED_MODEL not in AVAILABLE_MODELS:\n    print(f\"âš ï¸ ì˜ëª»ëœ ëª¨ë¸ ì„ íƒ: {SELECTED_MODEL}\")\n    print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(AVAILABLE_MODELS.keys())}\")\n    SELECTED_MODEL = \"exaone\"  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›\n    print(f\"ê¸°ë³¸ ëª¨ë¸ë¡œ ë³€ê²½: {SELECTED_MODEL}\")\n\n# ì„ íƒëœ ëª¨ë¸ ì •ë³´\nmodel_info = AVAILABLE_MODELS[SELECTED_MODEL]\nMODEL_NAME = model_info[\"name\"]\n\nprint(f\"\\nâœ… ì„ íƒëœ ëª¨ë¸: {MODEL_NAME}\")\nprint(f\"   {model_info['description']}\")\n\n# ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì •\nif SELECTED_MODEL == \"exaone\":\n    # EXAONE ëª¨ë¸ì€ íŠ¹ë³„í•œ í† í° ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ\n    print(\"\\nğŸ’¡ EXAONE ëª¨ë¸ íŠ¹ìˆ˜ ì„¤ì • ì ìš©\")\n    USE_SPECIAL_TOKENS = True\nelif SELECTED_MODEL == \"solar\":\n    # SOLAR ëª¨ë¸ì€ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ì— ê°•í•¨\n    print(\"\\nğŸ’¡ SOLAR ëª¨ë¸ íŠ¹ìˆ˜ ì„¤ì • ì ìš©\")\n    USE_SPECIAL_TOKENS = False\nelse:\n    USE_SPECIAL_TOKENS = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QLoRA ì„¤ì • ë° ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# QLoRAë¥¼ ìœ„í•œ 4bit ì–‘ìí™” ì„¤ì •\ndef get_quantization_config():\n    \"\"\"\n    RTX 4090 24GBì— ìµœì í™”ëœ 4bit ì–‘ìí™” ì„¤ì • ë°˜í™˜\n    \n    Returns:\n        BitsAndBytesConfig ê°ì²´\n    \"\"\"\n    return BitsAndBytesConfig(\n        load_in_4bit=True,  # 4bit ì–‘ìí™” ì‚¬ìš©\n        bnb_4bit_compute_dtype=torch.float16,  # ê³„ì‚°ì€ FP16ìœ¼ë¡œ\n        bnb_4bit_use_double_quant=True,  # ì´ì¤‘ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½\n        bnb_4bit_quant_type=\"nf4\"  # NormalFloat4 ì–‘ìí™” (ë” ë‚˜ì€ ì„±ëŠ¥)\n    )\n\n# LoRA ì„¤ì •\ndef get_lora_config():\n    \"\"\"\n    LoRA (Low-Rank Adaptation) ì„¤ì • ë°˜í™˜\n    \n    Returns:\n        LoraConfig ê°ì²´\n    \"\"\"\n    # ëª¨ë¸ë³„ íƒ€ê²Ÿ ëª¨ë“ˆ ì„¤ì • (ë” ì„¸ë°€í•œ ì„¤ì •)\n    if \"qwen\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"solar\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"llama\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"exaone\" in MODEL_NAME.lower():\n        # EXAONE ëª¨ë¸ì€ íŠ¹ë³„í•œ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    else:\n        # ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì—ì„œ ì‘ë™í•˜ëŠ” ê¸°ë³¸ ì„¤ì •\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    \n    return LoraConfig(\n        r=16,  # LoRA rank (8~32 ë²”ìœ„, ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€)\n        lora_alpha=32,  # LoRA scaling parameter (ì¼ë°˜ì ìœ¼ë¡œ r*2)\n        target_modules=target_modules,  # ì ìš©í•  ëª¨ë“ˆ\n        lora_dropout=0.1,  # Dropout ë¹„ìœ¨\n        bias=\"none\",  # Bias í•™ìŠµ ì—¬ë¶€\n        task_type=TaskType.CAUSAL_LM,  # ì–¸ì–´ ëª¨ë¸ë§ íƒœìŠ¤í¬\n    )\n\n# ì„¤ì • ìƒì„±\nquantization_config = get_quantization_config()\nlora_config = get_lora_config()\n\nprint(\"âš™ï¸ QLoRA ì„¤ì • ì™„ë£Œ!\")\nprint(f\"  - ì–‘ìí™”: 4bit (NF4)\")\nprint(f\"  - LoRA rank: {lora_config.r}\")\nprint(f\"  - LoRA alpha: {lora_config.lora_alpha}\")\nprint(f\"  - íƒ€ê²Ÿ ëª¨ë“ˆ: {lora_config.target_modules}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©\nprint(f\"\\nğŸš€ ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_NAME}\")\nprint(\"â³ ì²« ì‹¤í–‰ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (10-20GB)...\")\n\ntry:\n    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,  # ì¼ë¶€ ëª¨ë¸ì€ ì»¤ìŠ¤í…€ ì½”ë“œ í•„ìš”\n        use_fast=True  # Fast tokenizer ì‚¬ìš© (ë” ë¹ ë¦„)\n    )\n    \n    # íŒ¨ë”© í† í° ì„¤ì • (ì—†ëŠ” ê²½ìš°)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"â„¹ï¸ íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\")\n    \n    # GPU ë©”ëª¨ë¦¬ í™•ì¸\n    if torch.cuda.is_available():\n        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n        print(f\"\\nğŸ’¾ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬: {free_memory / 1024**3:.1f} GB\")\n        \n        # ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ê²½ê³ \n        if free_memory < 10 * 1024**3:  # 10GB ë¯¸ë§Œ\n            print(\"âš ï¸ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. batch_sizeë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n    \n    # ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™” ì ìš©)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=quantization_config,\n        device_map=\"auto\",  # GPUì— ìë™ ë°°ì¹˜\n        trust_remote_code=True,\n        torch_dtype=torch.float16,  # FP16 ì‚¬ìš©\n        low_cpu_mem_usage=True  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ\n    )\n    \n    # í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA ì ìš©\n    model = get_peft_model(model, lora_config)\n    \n    # ëª¨ë¸ ì •ë³´ ì¶œë ¥\n    print(\"\\nâœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n    print(f\"ğŸ“Š í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°:\")\n    model.print_trainable_parameters()\n    \nexcept Exception as e:\n    print(f\"\\nâŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}\")\n    print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n    print(\"  1. ì¸í„°ë„· ì—°ê²° í™•ì¸\")\n    print(\"  2. Hugging Face í† í° ì„¤ì • í™•ì¸\")\n    print(\"  3. GPU ë©”ëª¨ë¦¬ í™•ì¸\")\n    raise e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„°ì…‹ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "def format_prompt(question: str, answer: str, is_training: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ëª¨ë¸ í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ë¡œ í¬ë§·íŒ…\n",
    "    \n",
    "    Args:\n",
    "        question: ì§ˆë¬¸ í…ìŠ¤íŠ¸\n",
    "        answer: ë‹µë³€ í…ìŠ¤íŠ¸\n",
    "        is_training: í•™ìŠµìš©ì¸ì§€ ì—¬ë¶€ (í•™ìŠµì‹œì—ëŠ” ë‹µë³€ í¬í•¨)\n",
    "        \n",
    "    Returns:\n",
    "        í¬ë§·ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "    if \"exaone\" in MODEL_NAME.lower():\n",
    "        # EXAONE ëª¨ë¸ìš© í…œí”Œë¦¿\n",
    "        if is_training:\n",
    "            prompt = f\"[|ì‹œìŠ¤í…œ|]ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.[|ì¢…ë£Œ|]\\n[|ì‚¬ìš©ì|]{question}[|ì¢…ë£Œ|]\\n[|AI|]{answer}[|ì¢…ë£Œ|]\"\n",
    "        else:\n",
    "            prompt = f\"[|ì‹œìŠ¤í…œ|]ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.[|ì¢…ë£Œ|]\\n[|ì‚¬ìš©ì|]{question}[|ì¢…ë£Œ|]\\n[|AI|]\"\n",
    "    elif \"solar\" in MODEL_NAME.lower():\n",
    "        # SOLAR ëª¨ë¸ìš© í…œí”Œë¦¿\n",
    "        system_prompt = \"ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµ ì‹œì¥ì— ì •í†µí•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\"\n",
    "        if is_training:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n{answer}\"\n",
    "        else:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n\"\n",
    "    else:\n",
    "        # ê¸°ë³¸ í…œí”Œë¦¿ (Qwen, Llama ë“±)\n",
    "        if is_training:\n",
    "            prompt = f\"ì§ˆë¬¸: {question}\\n\\në‹µë³€: {answer}\"\n",
    "        else:\n",
    "            prompt = f\"ì§ˆë¬¸: {question}\\n\\në‹µë³€: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ í™•ì¸\n",
    "if train_data:\n",
    "    sample = train_data[0]\n",
    "    sample_prompt = format_prompt(sample['question'], sample['answer'])\n",
    "    print(\"ğŸ“ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜ˆì‹œ:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(sample_prompt[:500] + \"...\" if len(sample_prompt) > 500 else sample_prompt)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í† í°í™” í•¨ìˆ˜\ndef tokenize_function(examples: Dict[str, List]) -> Dict[str, List]:\n    \"\"\"\n    ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ í† í°í™”\n    \n    Args:\n        examples: ë°°ì¹˜ ë°ì´í„° (questions, answers í¬í•¨)\n        \n    Returns:\n        í† í°í™”ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬\n    \"\"\"\n    # í”„ë¡¬í”„íŠ¸ ìƒì„± (ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)\n    prompts = []\n    for question, answer in zip(examples['question'], examples['answer']):\n        # None ê°’ ì²´í¬\n        if question is None or answer is None:\n            continue\n        # ë¬¸ìì—´ ë³€í™˜ ë° ê³µë°± ì œê±°\n        question = str(question).strip()\n        answer = str(answer).strip()\n        if question and answer:\n            prompt = format_prompt(question, answer, is_training=True)\n            prompts.append(prompt)\n    \n    # ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬\n    if not prompts:\n        raise ValueError(\"ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n    \n    # í† í°í™”\n    model_inputs = tokenizer(\n        prompts,\n        max_length=2048,  # ìµœëŒ€ í† í° ê¸¸ì´\n        padding=\"max_length\",  # ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©\n        truncation=True,  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸°\n        return_tensors=\"pt\"\n    )\n    \n    # ë ˆì´ë¸” ì„¤ì • (input_idsì™€ ë™ì¼í•˜ê²Œ, íŒ¨ë”© ë¶€ë¶„ì€ -100ìœ¼ë¡œ)\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n    \n    # íŒ¨ë”© í† í°ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸ (-100ìœ¼ë¡œ ì„¤ì •)\n    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n    \n    return model_inputs\n\n# ë°ì´í„°ë¥¼ HuggingFace Datasetìœ¼ë¡œ ë³€í™˜\ndef prepare_datasets(train_data: List[Dict], val_data: List[Dict]):\n    \"\"\"\n    í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¥¼ HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  í† í°í™”\n    \n    Args:\n        train_data: í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n        val_data: ê²€ì¦ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n        \n    Returns:\n        (í† í°í™”ëœ í•™ìŠµ ë°ì´í„°ì…‹, í† í°í™”ëœ ê²€ì¦ ë°ì´í„°ì…‹)\n    \"\"\"\n    # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ì‰¬ìš´ ì²˜ë¦¬ë¥¼ ìœ„í•´)\n    train_df = pd.DataFrame(train_data)\n    val_df = pd.DataFrame(val_data)\n    \n    # HuggingFace Datasetìœ¼ë¡œ ë³€í™˜\n    train_dataset = HFDataset.from_pandas(train_df)\n    val_dataset = HFDataset.from_pandas(val_df)\n    \n    # í† í°í™” ì ìš©\n    print(\"ğŸ”„ í•™ìŠµ ë°ì´í„° í† í°í™” ì¤‘...\")\n    tokenized_train = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=train_dataset.column_names\n    )\n    \n    print(\"ğŸ”„ ê²€ì¦ ë°ì´í„° í† í°í™” ì¤‘...\")\n    tokenized_val = val_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=val_dataset.column_names\n    )\n    \n    print(\"\\nâœ… í† í°í™” ì™„ë£Œ!\")\n    print(f\"  - í•™ìŠµ ë°ì´í„°: {len(tokenized_train)}ê°œ\")\n    print(f\"  - ê²€ì¦ ë°ì´í„°: {len(tokenized_val)}ê°œ\")\n    \n    return tokenized_train, tokenized_val\n\n# ë°ì´í„°ì…‹ ì¤€ë¹„\nif train_data and val_data:\n    tokenized_train_dataset, tokenized_val_dataset = prepare_datasets(train_data, val_data)\nelse:\n    print(\"âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n    tokenized_train_dataset, tokenized_val_dataset = None, None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í† í° ê¸¸ì´ ë¶„í¬ ì‹œê°í™”\ndef visualize_token_distribution(dataset, title=\"Token Length Distribution\"):\n    \"\"\"\n    ë°ì´í„°ì…‹ì˜ í† í° ê¸¸ì´ ë¶„í¬ë¥¼ ì‹œê°í™”\n    \n    Args:\n        dataset: í† í°í™”ëœ ë°ì´í„°ì…‹\n        title: ê·¸ë˜í”„ ì œëª©\n    \"\"\"\n    if dataset is None:\n        return\n    \n    # ì‹¤ì œ í† í° ê¸¸ì´ ê³„ì‚° (íŒ¨ë”© ì œì™¸) - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ\n    lengths = []\n    sample_size = min(1000, len(dataset))  # ìµœëŒ€ 1000ê°œ ìƒ˜í”Œë§Œ ë¶„ì„\n    indices = np.random.choice(len(dataset), sample_size, replace=False)\n    \n    for idx in indices:\n        item = dataset[int(idx)]\n        # attention_maskê°€ 1ì¸ ë¶€ë¶„ë§Œ ì‹¤ì œ í† í°\n        actual_length = sum(item['attention_mask'])\n        lengths.append(actual_length)\n    \n    print(f\"\\nğŸ“Š ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ (ì „ì²´ {len(dataset)}ê°œ ì¤‘)\")\n    \n    # í†µê³„ ê³„ì‚°\n    avg_length = np.mean(lengths)\n    median_length = np.median(lengths)\n    max_length = np.max(lengths)\n    \n    # ì‹œê°í™”\n    plt.figure(figsize=(10, 6))\n    plt.hist(lengths, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    plt.axvline(avg_length, color='red', linestyle='--', label=f'Average: {avg_length:.0f}')\n    plt.axvline(median_length, color='green', linestyle='--', label=f'Median: {median_length:.0f}')\n    plt.xlabel('Token Length')\n    plt.ylabel('Frequency')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    print(f\"ğŸ“Š í† í° ê¸¸ì´ í†µê³„:\")\n    print(f\"  - í‰ê· : {avg_length:.0f} í† í°\")\n    print(f\"  - ì¤‘ê°„ê°’: {median_length:.0f} í† í°\")\n    print(f\"  - ìµœëŒ€: {max_length} í† í°\")\n    print(f\"  - 2048 í† í° ì´ˆê³¼: {sum(1 for l in lengths if l >= 2048)}ê°œ ({sum(1 for l in lengths if l >= 2048)/len(lengths)*100:.1f}%)\")\n\n# í•™ìŠµ ë°ì´í„° í† í° ë¶„í¬ í™•ì¸\nif tokenized_train_dataset:\n    visualize_token_distribution(tokenized_train_dataset, \"Training Data Token Distribution\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\ndef get_training_args(output_dir: str = \"./models/checkpoints\"):\n    \"\"\"\n    RTX 4090 24GBì— ìµœì í™”ëœ í•™ìŠµ ì„¤ì • ë°˜í™˜\n    \n    Args:\n        output_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬\n        \n    Returns:\n        TrainingArguments ê°ì²´\n    \"\"\"\n    # ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ ê³„ì‚°\n    if tokenized_train_dataset:\n        steps_per_epoch = len(tokenized_train_dataset) // (4 * 4)  # batch_size=4, gradient_accumulation=4\n        total_steps = steps_per_epoch * 3  # num_epochs=3\n    else:\n        total_steps = 1000  # ê¸°ë³¸ê°’\n    \n    # ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •\n    if torch.cuda.is_available():\n        free_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        if free_gb < 12:\n            batch_size = 2\n            print(f\"âš ï¸ GPU ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ batch_sizeë¥¼ {batch_size}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.\")\n        else:\n            batch_size = 4\n    \n    return TrainingArguments(\n        # ê¸°ë³¸ ì„¤ì •\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        \n        # í•™ìŠµ ì„¤ì •\n        num_train_epochs=3,  # ì—í­ ìˆ˜ (3-5 ê¶Œì¥)\n        per_device_train_batch_size=batch_size,  # ë™ì  ë°°ì¹˜ í¬ê¸°\n        per_device_eval_batch_size=batch_size,\n        gradient_accumulation_steps=4,  # ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = batch_size * 4\n        gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½\n        \n        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n        learning_rate=2e-4,  # LoRAìš© í•™ìŠµë¥  (ì¼ë°˜ì ìœ¼ë¡œ 1e-4 ~ 5e-4)\n        weight_decay=0.01,  # ê°€ì¤‘ì¹˜ ê°ì‡ \n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,  # Gradient clipping\n        \n        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n        lr_scheduler_type=\"cosine\",  # Cosine ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©\n        warmup_steps=int(total_steps * 0.1),  # ì „ì²´ì˜ 10%ë¥¼ warmup\n        \n        # ë¡œê¹… ë° ì €ì¥\n        logging_steps=10,\n        logging_first_step=True,\n        save_strategy=\"steps\",\n        save_steps=100,\n        save_total_limit=3,  # ìµœëŒ€ 3ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€\n        \n        # í‰ê°€ ì„¤ì •\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        load_best_model_at_end=True,\n        \n        # ê¸°íƒ€ ì„¤ì •\n        fp16=True,  # Mixed precision training\n        fp16_opt_level=\"O1\",  # ì•ˆì •ì ì¸ mixed precision\n        dataloader_num_workers=min(4, psutil.cpu_count()),  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •\n        remove_unused_columns=False,\n        push_to_hub=False,  # Hugging Face Hubì— í‘¸ì‹œí•˜ì§€ ì•ŠìŒ\n        report_to=[\"tensorboard\"],  # TensorBoard ë¡œê¹… í™œì„±í™”\n        logging_dir=\"./logs\",  # TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬\n        seed=42,  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ\n        dataloader_pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ\n        ddp_find_unused_parameters=False,  # DDP ìµœì í™”\n    )\n\n# í•™ìŠµ ì„¤ì • ìƒì„±\ntraining_args = get_training_args()\n\nprint(\"âš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ!\")\nprint(f\"  - ì—í­ ìˆ˜: {training_args.num_train_epochs}\")\nprint(f\"  - ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size} Ã— {training_args.gradient_accumulation_steps} = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - í•™ìŠµë¥ : {training_args.learning_rate}\")\nprint(f\"  - Warmup ìŠ¤í…: {training_args.warmup_steps}\")\nprint(f\"  - ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©: {training_args.save_steps} ìŠ¤í…\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # 3ë²ˆì˜ í‰ê°€ì—ì„œ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "    early_stopping_threshold=0.001  # ìµœì†Œ ê°œì„  í­\n",
    ")\n",
    "\n",
    "print(\"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"  - Patience: {early_stopping_callback.early_stopping_patience}\")\n",
    "print(f\"  - Threshold: {early_stopping_callback.early_stopping_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì»¤ìŠ¤í…€ Trainer í´ë˜ìŠ¤ (ì„ íƒì‚¬í•­)\nclass FSKUTrainer(Trainer):\n    \"\"\"\n    FSKU í”„ë¡œì íŠ¸ìš© ì»¤ìŠ¤í…€ Trainer\n    í•™ìŠµ ì¤‘ ì¶”ê°€ ë¡œê¹…ì´ë‚˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_history = []\n        \n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"\n        ì†ì‹¤ ê³„ì‚° (í•„ìš”ì‹œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥)\n        \"\"\"\n        # ê¸°ë³¸ ì†ì‹¤ ê³„ì‚°\n        outputs = model(**inputs)\n        loss = outputs.loss\n        \n        # ì†ì‹¤ ê¸°ë¡\n        self.loss_history.append(loss.item())\n        \n        # Perplexity ê³„ì‚° (ì–¸ì–´ ëª¨ë¸ì˜ ì¤‘ìš” ì§€í‘œ)\n        if len(self.loss_history) % 100 == 0:\n            avg_loss = np.mean(self.loss_history[-100:])\n            perplexity = np.exp(avg_loss)\n            print(f\"\\nğŸ“Š ìµœê·¼ 100 ìŠ¤í… í‰ê·  Perplexity: {perplexity:.2f}\")\n        \n        return (loss, outputs) if return_outputs else loss\n    \n    def log(self, logs):\n        \"\"\"ë¡œê¹… ì»¤ìŠ¤í„°ë§ˆì´ì§•\"\"\"\n        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ê°€\n        if torch.cuda.is_available():\n            logs[\"gpu_memory_gb\"] = torch.cuda.memory_allocated() / 1024**3\n        super().log(logs)\n\n# Trainer ì´ˆê¸°í™”\nif tokenized_train_dataset and tokenized_val_dataset:\n    trainer = FSKUTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_dataset,\n        eval_dataset=tokenized_val_dataset,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False,  # Causal LMì´ë¯€ë¡œ MLM ë¹„í™œì„±í™”\n            pad_to_multiple_of=8  # íš¨ìœ¨ì„±ì„ ìœ„í•´ 8ì˜ ë°°ìˆ˜ë¡œ íŒ¨ë”©\n        ),\n        callbacks=[early_stopping_callback],\n    )\n    \n    print(\"\\nâœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ!\")\n    print(f\"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í… ìˆ˜: {trainer.args.max_steps if trainer.args.max_steps > 0 else len(tokenized_train_dataset) // (trainer.args.per_device_train_batch_size * trainer.args.gradient_accumulation_steps) * trainer.args.num_train_epochs}\")\nelse:\n    trainer = None\n    print(\"âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ Trainerë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜\ndef clear_gpu_memory():\n    \"\"\"\n    GPU ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•˜ì—¬ OOM ë°©ì§€\n    \"\"\"\n    import gc\n    \n    # ê¸°ì¡´ ë³€ìˆ˜ë“¤ ì •ë¦¬\n    for obj in gc.get_objects():\n        if torch.is_tensor(obj):\n            del obj\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    if torch.cuda.is_available():\n        # ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        free = torch.cuda.get_device_properties(0).total_memory / 1024**3 - reserved\n        \n        print(f\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")\n        print(f\"   í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f} GB\")\n        print(f\"   ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {reserved:.2f} GB\")\n        print(f\"   ì‚¬ìš© ê°€ëŠ¥: {free:.2f} GB\")\n        \n        # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ \n        if free < 5:\n            print(\"âš ï¸ GPU ë©”ëª¨ë¦¬ê°€ 5GB ë¯¸ë§Œì…ë‹ˆë‹¤. í•™ìŠµ ì¤‘ OOM ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\")\n\n# í•™ìŠµ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬\nclear_gpu_memory()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\nif trainer is not None:\n    print(\"\\nğŸš€ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ 1-4ì‹œê°„\")\n    print(\"ğŸ’¡ íŒ: í•™ìŠµ ì¤‘ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ batch_sizeë¥¼ ì¤„ì´ì„¸ìš”.\\n\")\n    \n    # í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n    start_time = datetime.now()\n    \n    try:\n        # í•™ìŠµ ì‹¤í–‰\n        train_result = trainer.train()\n        \n        # í•™ìŠµ ì™„ë£Œ\n        end_time = datetime.now()\n        training_time = end_time - start_time\n        \n        print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")\n        print(f\"â±ï¸ ì´ í•™ìŠµ ì‹œê°„: {training_time}\")\n        print(f\"ğŸ“Š ìµœì¢… í•™ìŠµ ì†ì‹¤: {train_result.training_loss:.4f}\")\n        \n        # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥ (ë” ìƒì„¸í•œ ì •ë³´)\n        metrics = {\n            \"training_loss\": float(train_result.training_loss),\n            \"training_time\": str(training_time),\n            \"training_time_seconds\": training_time.total_seconds(),\n            \"model_name\": MODEL_NAME,\n            \"model_type\": SELECTED_MODEL,\n            \"total_steps\": train_result.global_step,\n            \"epochs\": training_args.num_train_epochs,\n            \"train_samples\": len(train_data),\n            \"val_samples\": len(val_data),\n            \"batch_size\": training_args.per_device_train_batch_size,\n            \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n            \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n            \"learning_rate\": training_args.learning_rate,\n            \"lora_rank\": lora_config.r,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        # ìµœì¢… ê²€ì¦ ì†ì‹¤ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)\n        if hasattr(trainer.state, 'best_metric'):\n            metrics[\"best_eval_loss\"] = float(trainer.state.best_metric)\n        \n        # ë©”íŠ¸ë¦­ ì €ì¥\n        os.makedirs(\"results\", exist_ok=True)\n        with open(\"results/training_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(metrics, f, indent=2, ensure_ascii=False)\n        \n        print(\"\\nğŸ“Š í•™ìŠµ ë©”íŠ¸ë¦­ì´ results/training_metrics.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n        \n    except Exception as e:\n        print(f\"\\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n        print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n        print(\"  1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: batch_sizeë¥¼ 2ë¡œ ì¤„ì´ê¸°\")\n        print(\"  2. CUDA ì˜¤ë¥˜: GPU ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸\")\n        print(\"  3. ê·¸ ì™¸: ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸ í›„ êµ¬ê¸€ë§\")\n        raise e\nelse:\n    print(\"âš ï¸ Trainerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•™ìŠµ ê³¡ì„  ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë¡œê·¸ ì‹œê°í™”\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ ë³€í™”ë¥¼ ì‹œê°í™”\n",
    "    \n",
    "    Args:\n",
    "        trainer: í•™ìŠµì´ ì™„ë£Œëœ Trainer ê°ì²´\n",
    "    \"\"\"\n",
    "    if trainer is None or not hasattr(trainer.state, 'log_history'):\n",
    "        print(\"âš ï¸ í•™ìŠµ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        return\n",
    "    \n",
    "    # ë¡œê·¸ì—ì„œ ì†ì‹¤ ê°’ ì¶”ì¶œ\n",
    "    log_history = trainer.state.log_history\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in log_history:\n",
    "        if 'loss' in log:\n",
    "            train_loss.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_loss)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_loss.append(log['eval_loss'])\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # í•™ìŠµ ì†ì‹¤\n",
    "    if train_loss:\n",
    "        ax1.plot(steps[:len(train_loss)], train_loss, 'b-', label='Training Loss')\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss Over Time')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # ê²€ì¦ ì†ì‹¤\n",
    "    if eval_loss:\n",
    "        eval_steps = [i * training_args.eval_steps for i in range(1, len(eval_loss) + 1)]\n",
    "        ax2.plot(eval_steps, eval_loss, 'r-', marker='o', label='Validation Loss')\n",
    "        ax2.set_xlabel('Steps')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Validation Loss Over Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìµœì¢… ì†ì‹¤ ì¶œë ¥\n",
    "    if train_loss:\n",
    "        print(f\"\\nğŸ“Š ìµœì¢… í•™ìŠµ ì†ì‹¤: {train_loss[-1]:.4f}\")\n",
    "    if eval_loss:\n",
    "        print(f\"ğŸ“Š ìµœì¢… ê²€ì¦ ì†ì‹¤: {eval_loss[-1]:.4f}\")\n",
    "        print(f\"ğŸ“Š ìµœê³  ê²€ì¦ ì†ì‹¤: {min(eval_loss):.4f}\")\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°\n",
    "if trainer and hasattr(trainer, 'state'):\n",
    "    plot_training_history(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ìµœì¢… ëª¨ë¸ ì €ì¥\ndef save_final_model(trainer, output_dir: str = \"models/fsku_finetuned_model\"):\n    \"\"\"\n    í•™ìŠµëœ ëª¨ë¸ì„ ì¶”ë¡ ìš©ìœ¼ë¡œ ì €ì¥\n    \n    Args:\n        trainer: í•™ìŠµì´ ì™„ë£Œëœ Trainer ê°ì²´\n        output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n    \"\"\"\n    if trainer is None:\n        print(\"âš ï¸ ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n        return\n    \n    print(f\"\\nğŸ’¾ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤: {output_dir}\")\n    \n    # ë””ë ‰í† ë¦¬ ìƒì„±\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (LoRA ì–´ëŒ‘í„°ë§Œ)\n    trainer.save_model(output_dir)\n    \n    # í† í¬ë‚˜ì´ì €ë„ ì €ì¥\n    trainer.tokenizer.save_pretrained(output_dir)\n    \n    # ì„¤ì • ì •ë³´ ì €ì¥ (ë” ìƒì„¸í•œ ì •ë³´)\n    config_info = {\n        \"base_model\": MODEL_NAME,\n        \"model_type\": SELECTED_MODEL,\n        \"training_completed\": datetime.now().isoformat(),\n        \"lora_config\": {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"target_modules\": lora_config.target_modules\n        },\n        \"training_args\": {\n            \"num_train_epochs\": training_args.num_train_epochs,\n            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n            \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n            \"learning_rate\": training_args.learning_rate,\n            \"warmup_steps\": training_args.warmup_steps,\n            \"fp16\": training_args.fp16\n        },\n        \"dataset_info\": {\n            \"train_samples\": len(train_data) if 'train_data' in globals() else 0,\n            \"val_samples\": len(val_data) if 'val_data' in globals() else 0\n        },\n        \"final_loss\": float(trainer.state.log_history[-1].get('loss', 0)) if hasattr(trainer, 'state') and trainer.state.log_history else 'N/A',\n        \"quantization\": \"4bit\",\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    }\n    \n    with open(os.path.join(output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(config_info, f, indent=2, ensure_ascii=False)\n    \n    print(\"\\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n    print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}\")\n    print(\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n    print(\"  - adapter_model.safetensors (LoRA ê°€ì¤‘ì¹˜)\")\n    print(\"  - adapter_config.json (LoRA ì„¤ì •)\")\n    print(\"  - tokenizer íŒŒì¼ë“¤\")\n    print(\"  - training_config.json (í•™ìŠµ ì •ë³´)\")\n    print(\"\\nğŸ’¡ ì¶”ë¡ ì‹œ ì´ ê²½ë¡œë¥¼ FSKU_3_ì¶”ë¡ .ipynbì—ì„œ ì‚¬ìš©í•˜ì„¸ìš”!\")\n    \n    return output_dir\n\n# ëª¨ë¸ ì €ì¥ ì‹¤í–‰\nif trainer:\n    saved_model_path = save_final_model(trainer)\nelse:\n    print(\"âš ï¸ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ì–´ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ í‰ê°€ ë° ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒ˜í”Œ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "def generate_sample(model, tokenizer, prompt: str, max_length: int = 512):\n",
    "    \"\"\"\n",
    "    í•™ìŠµëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "    \n",
    "    Args:\n",
    "        model: í•™ìŠµëœ ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸\n",
    "        max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´\n",
    "        \n",
    "    Returns:\n",
    "        ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ\n",
    "    model.eval()\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ í† í°í™”\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
    "if trainer and val_data:\n",
    "    print(\"\\nğŸ§ª í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì„ íƒ\n",
    "    test_samples = random.sample(val_data, min(3, len(val_data)))\n",
    "    \n",
    "    for i, sample in enumerate(test_samples):\n",
    "        print(f\"\\n[í…ŒìŠ¤íŠ¸ {i+1}]\")\n",
    "        \n",
    "        # ì§ˆë¬¸ë§Œìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        test_prompt = format_prompt(sample['question'], \"\", is_training=False)\n",
    "        \n",
    "        print(f\"ì§ˆë¬¸: {sample['question']}\")\n",
    "        print(f\"\\nì •ë‹µ: {sample['answer'][:200]}...\" if len(sample['answer']) > 200 else f\"\\nì •ë‹µ: {sample['answer']}\")\n",
    "        \n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        generated = generate_sample(model, tokenizer, test_prompt)\n",
    "        print(f\"\\nëª¨ë¸ ìƒì„±: {generated[:200]}...\" if len(generated) > 200 else f\"\\nëª¨ë¸ ìƒì„±: {generated}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì´ë‚˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ìµœì¢… ìš”ì•½\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ğŸ“Š í•™ìŠµ ì™„ë£Œ ìš”ì•½\")\nprint(\"=\" * 60)\n\nif trainer:\n    print(f\"\\nâœ… ëª¨ë¸: {MODEL_NAME}\")\n    print(f\"âœ… ëª¨ë¸ íƒ€ì…: {SELECTED_MODEL}\")\n    print(f\"âœ… í•™ìŠµ ë°ì´í„°: {len(train_data):,}ê°œ\")\n    print(f\"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ\")\n    print(f\"âœ… í•™ìŠµ ì—í­: {training_args.num_train_epochs}\")\n    print(f\"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: models/fsku_finetuned_model/\")\n    \n    # ì„±ëŠ¥ ìš”ì•½\n    if hasattr(trainer, 'state') and trainer.state.log_history:\n        final_train_loss = trainer.state.log_history[-1].get('loss', 'N/A')\n        if isinstance(final_train_loss, float):\n            print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:\")\n            print(f\"  - ìµœì¢… í•™ìŠµ ì†ì‹¤: {final_train_loss:.4f}\")\n            print(f\"  - ìµœì¢… Perplexity: {np.exp(final_train_loss):.2f}\")\n    \n    print(f\"\\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:\")\n    print(f\"   1. FSKU_3_ì¶”ë¡ .ipynb íŒŒì¼ ì‹¤í–‰\")\n    print(f\"   2. ëª¨ë¸ ê²½ë¡œë¡œ 'models/fsku_finetuned_model' ì‚¬ìš©\")\n    print(f\"   3. test.csv íŒŒì¼ë¡œ ì¶”ë¡  ìˆ˜í–‰\")\n    \n    print(f\"\\nğŸ’¡ ì¶”ê°€ íŒ:\")\n    print(f\"   - TensorBoard ë¡œê·¸ í™•ì¸: tensorboard --logdir ./logs\")\n    print(f\"   - ëª¨ë¸ í¬ê¸° í™•ì¸: du -sh models/fsku_finetuned_model/\")\n    print(f\"   - GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§: nvidia-smi -l 1\")\nelse:\n    print(\"\\nâŒ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n    print(\"ğŸ’¡ FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\")\n    print(\"\\nğŸ” ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n    print(\"   [ ] data/augmented/ í´ë”ì— JSON íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸\")\n    print(\"   [ ] GPUê°€ ì œëŒ€ë¡œ ì¸ì‹ë˜ëŠ”ì§€ í™•ì¸\")\n    print(\"   [ ] í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\")\n\nprint(\"\\n\" + \"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}