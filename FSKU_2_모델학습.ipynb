{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 FSKU_2_모델학습\n",
    "\n",
    "## 📋 노트북 개요\n",
    "\n",
    "### 목적\n",
    "이 노트북은 FSKU 금융 AI Challenge를 위한 모델 학습 단계입니다. `FSKU_1_데이터증강_RAG.ipynb`에서 생성된 증강 데이터를 사용하여 금융 전문 AI 모델을 학습합니다.\n",
    "\n",
    "### 입력 데이터\n",
    "- **위치**: `data/augmented/` 폴더\n",
    "- **형식**: JSON 파일 (questions_*.json)\n",
    "- **내용**: 증강된 금융 관련 질문-답변 쌍\n",
    "\n",
    "### 출력물\n",
    "- **학습된 모델**: `models/fsku_finetuned_model/` (추론 단계에서 사용)\n",
    "- **체크포인트**: `models/checkpoints/`\n",
    "- **학습 메트릭**: `results/training_metrics.json`\n",
    "\n",
    "### 핵심 제약사항\n",
    "- RTX 4090 24GB 메모리 제한\n",
    "- 단일 LLM만 사용 (앙상블 불가)\n",
    "- 오프라인 환경에서 실행 가능\n",
    "- 270분 내 515문항 처리 가능한 속도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 필수 라이브러리 설치 확인 및 자동 설치\nimport subprocess\nimport sys\n\ndef install_package(package):\n    \"\"\"패키지가 설치되어 있지 않으면 자동으로 설치\"\"\"\n    try:\n        __import__(package.split('>')[0].split('=')[0])\n    except ImportError:\n        print(f\"📦 {package} 설치 중...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n        print(f\"✅ {package} 설치 완료!\")\n\n# 필수 패키지 목록\nrequired_packages = [\n    \"transformers>=4.36.0\",\n    \"peft>=0.7.0\",\n    \"bitsandbytes>=0.41.0\",\n    \"accelerate>=0.25.0\",\n    \"datasets\",\n    \"sentencepiece\",\n    \"protobuf\",\n    \"scipy\",\n    \"scikit-learn\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"psutil\",  # 시스템 모니터링용\n    \"tensorboard\"  # 학습 모니터링용\n]\n\nprint(\"🔍 필수 패키지 확인 중...\")\nfor package in required_packages:\n    install_package(package)\nprint(\"\\n✅ 모든 필수 패키지가 준비되었습니다!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 기본 라이브러리 임포트\nimport os\nimport json\nimport glob\nimport random\nimport warnings\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# 딥러닝 관련 라이브러리\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Hugging Face 라이브러리\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n    logging as transformers_logging\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom datasets import Dataset as HFDataset\n\n# 시각화 라이브러리\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 경고 메시지 설정\nwarnings.filterwarnings('ignore')\ntransformers_logging.set_verbosity_error()\n\n# 한글 폰트 설정 (시각화용)\nimport platform\n\nif platform.system() == 'Darwin':  # macOS\n    plt.rcParams['font.family'] = 'AppleGothic'\nelif platform.system() == 'Windows':\n    plt.rcParams['font.family'] = 'Malgun Gothic'\nelse:  # Linux\n    plt.rcParams['font.family'] = 'NanumGothic'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 시스템 리소스 모니터링\nimport psutil\nprint(f\"\\n💻 시스템 정보:\")\nprint(f\"  - CPU 코어: {psutil.cpu_count(logical=False)}개 (논리: {psutil.cpu_count()}개)\")\nprint(f\"  - RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\nprint(f\"  - 사용 가능 RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n\nprint(\"\\n✅ 라이브러리 임포트 완료!\")\nprint(f\"📊 PyTorch 버전: {torch.__version__}\")\nprint(f\"🖥️ CUDA 사용 가능: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"💾 GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 (재현성을 위해)\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    모든 랜덤 시드를 고정하여 재현 가능한 결과를 보장\n",
    "    \n",
    "    Args:\n",
    "        seed: 랜덤 시드 값\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"🎲 랜덤 시드 고정 완료 (seed=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 로딩 함수\ndef load_augmented_data(data_dir: str = \"data/augmented\") -> List[Dict]:\n    \"\"\"\n    증강된 데이터를 모두 로드하여 하나의 리스트로 반환\n    \n    Args:\n        data_dir: 증강 데이터가 저장된 디렉토리 경로\n        \n    Returns:\n        모든 질문-답변 쌍이 담긴 리스트\n    \"\"\"\n    all_data = []\n    \n    # JSON 파일 패턴으로 모든 증강 데이터 파일 찾기\n    json_files = glob.glob(os.path.join(data_dir, \"questions_*.json\"))\n    \n    if not json_files:\n        print(f\"⚠️ 경고: {data_dir}에서 증강 데이터를 찾을 수 없습니다!\")\n        print(\"💡 먼저 FSKU_1_데이터증강_RAG.ipynb를 실행하여 데이터를 생성하세요.\")\n        return []\n    \n    print(f\"📂 발견된 데이터 파일 수: {len(json_files)}개\")\n    \n    # 각 파일에서 데이터 로드 (병렬 처리 개선)\n    failed_files = []\n    for file_path in tqdm(json_files, desc=\"데이터 파일 로딩\"):\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                # 데이터 검증\n                valid_data = []\n                for item in data:\n                    if 'question' in item and 'answer' in item:\n                        # 빈 문자열 제거\n                        if item['question'].strip() and item['answer'].strip():\n                            valid_data.append(item)\n                all_data.extend(valid_data)\n        except json.JSONDecodeError as e:\n            print(f\"❌ JSON 파싱 오류: {file_path}\")\n            print(f\"   오류: {str(e)}\")\n            failed_files.append(file_path)\n        except Exception as e:\n            print(f\"❌ 파일 로드 실패: {file_path}\")\n            print(f\"   오류: {str(e)}\")\n            failed_files.append(file_path)\n    \n    if failed_files:\n        print(f\"\\n⚠️ {len(failed_files)}개 파일 로드 실패\")\n    \n    print(f\"\\n✅ 총 {len(all_data)}개의 학습 데이터 로드 완료!\")\n    return all_data\n\n# 데이터 로드\nraw_data = load_augmented_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 통계 분석\ndef analyze_data(data: List[Dict]) -> None:\n    \"\"\"\n    로드된 데이터의 통계 정보를 분석하고 출력\n    \n    Args:\n        data: 분석할 데이터 리스트\n    \"\"\"\n    if not data:\n        print(\"❌ 분석할 데이터가 없습니다!\")\n        return\n    \n    # 기본 통계\n    print(\"📊 데이터 통계 분석\")\n    print(\"=\" * 50)\n    print(f\"총 데이터 수: {len(data):,}개\")\n    \n    # 질문 유형별 분포\n    question_types = {}\n    answer_lengths = []\n    \n    for item in data:\n        # 질문 유형 카운트\n        q_type = item.get('question_type', 'unknown')\n        question_types[q_type] = question_types.get(q_type, 0) + 1\n        \n        # 답변 길이 수집\n        answer = item.get('answer', '')\n        answer_lengths.append(len(answer))\n    \n    # 질문 유형 출력\n    print(\"\\n📝 질문 유형별 분포:\")\n    for q_type, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True):\n        percentage = (count / len(data)) * 100\n        print(f\"  - {q_type}: {count:,}개 ({percentage:.1f}%)\")\n    \n    # 답변 길이 통계\n    if answer_lengths:\n        print(f\"\\n📏 답변 길이 통계:\")\n        print(f\"  - 평균: {np.mean(answer_lengths):.0f}자\")\n        print(f\"  - 중간값: {np.median(answer_lengths):.0f}자\")\n        print(f\"  - 최소: {np.min(answer_lengths)}자\")\n        print(f\"  - 최대: {np.max(answer_lengths)}자\")\n        print(f\"  - 표준편차: {np.std(answer_lengths):.0f}자\")\n        \n        # 이상치 검출\n        q1 = np.percentile(answer_lengths, 25)\n        q3 = np.percentile(answer_lengths, 75)\n        iqr = q3 - q1\n        outliers = sum(1 for l in answer_lengths if l < q1 - 1.5*iqr or l > q3 + 1.5*iqr)\n        if outliers > 0:\n            print(f\"  - 이상치: {outliers}개 ({outliers/len(answer_lengths)*100:.1f}%)\")\n    \n    # 샘플 데이터 출력\n    print(\"\\n🔍 샘플 데이터 (첫 3개):\")\n    for i, item in enumerate(data[:3]):\n        print(f\"\\n[샘플 {i+1}]\")\n        print(f\"질문: {item.get('question', '')[:100]}...\")\n        print(f\"답변: {item.get('answer', '')[:100]}...\")\n        print(f\"유형: {item.get('question_type', 'unknown')}\")\n\n# 데이터 분석 실행\nif raw_data:\n    analyze_data(raw_data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/검증 데이터 분할\n",
    "def split_data(data: List[Dict], test_size: float = 0.2, seed: int = 42) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    데이터를 학습용과 검증용으로 분할\n",
    "    \n",
    "    Args:\n",
    "        data: 전체 데이터 리스트\n",
    "        test_size: 검증 데이터 비율 (기본값: 0.2)\n",
    "        seed: 랜덤 시드\n",
    "        \n",
    "    Returns:\n",
    "        (학습 데이터, 검증 데이터) 튜플\n",
    "    \"\"\"\n",
    "    # 데이터 셔플\n",
    "    random.seed(seed)\n",
    "    shuffled_data = data.copy()\n",
    "    random.shuffle(shuffled_data)\n",
    "    \n",
    "    # 분할 지점 계산\n",
    "    split_idx = int(len(shuffled_data) * (1 - test_size))\n",
    "    \n",
    "    # 데이터 분할\n",
    "    train_data = shuffled_data[:split_idx]\n",
    "    val_data = shuffled_data[split_idx:]\n",
    "    \n",
    "    print(f\"✂️ 데이터 분할 완료!\")\n",
    "    print(f\"  - 학습 데이터: {len(train_data):,}개 ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"  - 검증 데이터: {len(val_data):,}개 ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# 데이터 분할\n",
    "if raw_data:\n",
    "    train_data, val_data = split_data(raw_data)\n",
    "else:\n",
    "    train_data, val_data = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 선택 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 모델 목록\n",
    "AVAILABLE_MODELS = {\n",
    "    \"exaone\": {\n",
    "        \"name\": \"LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "        \"description\": \"LG AI Research의 한국어 특화 모델 (추천)\",\n",
    "        \"size\": \"7.8B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"solar\": {\n",
    "        \"name\": \"upstage/SOLAR-10.7B-v1.0\",\n",
    "        \"description\": \"Upstage의 한국어 강화 모델\",\n",
    "        \"size\": \"10.7B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"qwen\": {\n",
    "        \"name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"description\": \"다국어 성능이 우수한 모델\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": False\n",
    "    },\n",
    "    \"llama-ko\": {\n",
    "        \"name\": \"beomi/llama-2-ko-7b\",\n",
    "        \"description\": \"한국어로 파인튜닝된 Llama 모델\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🤖 사용 가능한 모델 목록:\")\n",
    "print(\"=\" * 60)\n",
    "for key, info in AVAILABLE_MODELS.items():\n",
    "    print(f\"\\n[{key}] {info['name']}\")\n",
    "    print(f\"  - 설명: {info['description']}\")\n",
    "    print(f\"  - 크기: {info['size']}\")\n",
    "    print(f\"  - 한국어 특화: {'✅' if info['korean_specialized'] else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델 선택 (환경변수로도 설정 가능)\nimport os\n\n# 옵션: \"exaone\", \"solar\", \"qwen\", \"llama-ko\"\nSELECTED_MODEL = os.getenv('FSKU_MODEL', 'exaone')  # 환경변수 또는 기본값\n\n# 모델 선택 검증\nif SELECTED_MODEL not in AVAILABLE_MODELS:\n    print(f\"⚠️ 잘못된 모델 선택: {SELECTED_MODEL}\")\n    print(f\"사용 가능한 모델: {list(AVAILABLE_MODELS.keys())}\")\n    SELECTED_MODEL = \"exaone\"  # 기본값으로 복원\n    print(f\"기본 모델로 변경: {SELECTED_MODEL}\")\n\n# 선택된 모델 정보\nmodel_info = AVAILABLE_MODELS[SELECTED_MODEL]\nMODEL_NAME = model_info[\"name\"]\n\nprint(f\"\\n✅ 선택된 모델: {MODEL_NAME}\")\nprint(f\"   {model_info['description']}\")\n\n# 모델별 특수 설정\nif SELECTED_MODEL == \"exaone\":\n    # EXAONE 모델은 특별한 토큰 처리가 필요할 수 있음\n    print(\"\\n💡 EXAONE 모델 특수 설정 적용\")\n    USE_SPECIAL_TOKENS = True\nelif SELECTED_MODEL == \"solar\":\n    # SOLAR 모델은 긴 컨텍스트 처리에 강함\n    print(\"\\n💡 SOLAR 모델 특수 설정 적용\")\n    USE_SPECIAL_TOKENS = False\nelse:\n    USE_SPECIAL_TOKENS = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QLoRA 설정 및 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# QLoRA를 위한 4bit 양자화 설정\ndef get_quantization_config():\n    \"\"\"\n    RTX 4090 24GB에 최적화된 4bit 양자화 설정 반환\n    \n    Returns:\n        BitsAndBytesConfig 객체\n    \"\"\"\n    return BitsAndBytesConfig(\n        load_in_4bit=True,  # 4bit 양자화 사용\n        bnb_4bit_compute_dtype=torch.float16,  # 계산은 FP16으로\n        bnb_4bit_use_double_quant=True,  # 이중 양자화로 메모리 추가 절약\n        bnb_4bit_quant_type=\"nf4\"  # NormalFloat4 양자화 (더 나은 성능)\n    )\n\n# LoRA 설정\ndef get_lora_config():\n    \"\"\"\n    LoRA (Low-Rank Adaptation) 설정 반환\n    \n    Returns:\n        LoraConfig 객체\n    \"\"\"\n    # 모델별 타겟 모듈 설정 (더 세밀한 설정)\n    if \"qwen\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"solar\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"llama\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"exaone\" in MODEL_NAME.lower():\n        # EXAONE 모델은 특별한 구조를 가질 수 있음\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    else:\n        # 대부분의 모델에서 작동하는 기본 설정\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    \n    return LoraConfig(\n        r=16,  # LoRA rank (8~32 범위, 높을수록 표현력 증가)\n        lora_alpha=32,  # LoRA scaling parameter (일반적으로 r*2)\n        target_modules=target_modules,  # 적용할 모듈\n        lora_dropout=0.1,  # Dropout 비율\n        bias=\"none\",  # Bias 학습 여부\n        task_type=TaskType.CAUSAL_LM,  # 언어 모델링 태스크\n    )\n\n# 설정 생성\nquantization_config = get_quantization_config()\nlora_config = get_lora_config()\n\nprint(\"⚙️ QLoRA 설정 완료!\")\nprint(f\"  - 양자화: 4bit (NF4)\")\nprint(f\"  - LoRA rank: {lora_config.r}\")\nprint(f\"  - LoRA alpha: {lora_config.lora_alpha}\")\nprint(f\"  - 타겟 모듈: {lora_config.target_modules}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델과 토크나이저 로딩\nprint(f\"\\n🚀 모델 로딩 시작: {MODEL_NAME}\")\nprint(\"⏳ 첫 실행시 모델 다운로드로 시간이 걸릴 수 있습니다 (10-20GB)...\")\n\ntry:\n    # 토크나이저 로드\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,  # 일부 모델은 커스텀 코드 필요\n        use_fast=True  # Fast tokenizer 사용 (더 빠름)\n    )\n    \n    # 패딩 토큰 설정 (없는 경우)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"ℹ️ 패딩 토큰을 EOS 토큰으로 설정했습니다.\")\n    \n    # GPU 메모리 확인\n    if torch.cuda.is_available():\n        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n        print(f\"\\n💾 사용 가능한 GPU 메모리: {free_memory / 1024**3:.1f} GB\")\n        \n        # 메모리가 부족한 경우 경고\n        if free_memory < 10 * 1024**3:  # 10GB 미만\n            print(\"⚠️ GPU 메모리가 부족할 수 있습니다. batch_size를 줄이는 것을 권장합니다.\")\n    \n    # 모델 로드 (4bit 양자화 적용)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=quantization_config,\n        device_map=\"auto\",  # GPU에 자동 배치\n        trust_remote_code=True,\n        torch_dtype=torch.float16,  # FP16 사용\n        low_cpu_mem_usage=True  # CPU 메모리 사용량 감소\n    )\n    \n    # 학습을 위한 모델 준비\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA 적용\n    model = get_peft_model(model, lora_config)\n    \n    # 모델 정보 출력\n    print(\"\\n✅ 모델 로딩 완료!\")\n    print(f\"📊 학습 가능한 파라미터:\")\n    model.print_trainable_parameters()\n    \nexcept Exception as e:\n    print(f\"\\n❌ 모델 로딩 실패: {str(e)}\")\n    print(\"💡 해결 방법:\")\n    print(\"  1. 인터넷 연결 확인\")\n    print(\"  2. Hugging Face 토큰 설정 확인\")\n    print(\"  3. GPU 메모리 확인\")\n    raise e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터셋 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "def format_prompt(question: str, answer: str, is_training: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    질문과 답변을 모델 학습용 프롬프트로 포맷팅\n",
    "    \n",
    "    Args:\n",
    "        question: 질문 텍스트\n",
    "        answer: 답변 텍스트\n",
    "        is_training: 학습용인지 여부 (학습시에는 답변 포함)\n",
    "        \n",
    "    Returns:\n",
    "        포맷된 프롬프트 문자열\n",
    "    \"\"\"\n",
    "    # 모델별 프롬프트 템플릿\n",
    "    if \"exaone\" in MODEL_NAME.lower():\n",
    "        # EXAONE 모델용 템플릿\n",
    "        if is_training:\n",
    "            prompt = f\"[|시스템|]당신은 금융 전문 AI 어시스턴트입니다. 정확하고 전문적인 답변을 제공하세요.[|종료|]\\n[|사용자|]{question}[|종료|]\\n[|AI|]{answer}[|종료|]\"\n",
    "        else:\n",
    "            prompt = f\"[|시스템|]당신은 금융 전문 AI 어시스턴트입니다. 정확하고 전문적인 답변을 제공하세요.[|종료|]\\n[|사용자|]{question}[|종료|]\\n[|AI|]\"\n",
    "    elif \"solar\" in MODEL_NAME.lower():\n",
    "        # SOLAR 모델용 템플릿\n",
    "        system_prompt = \"당신은 한국 금융 시장에 정통한 전문가입니다. 질문에 대해 정확하고 상세한 답변을 제공하세요.\"\n",
    "        if is_training:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n{answer}\"\n",
    "        else:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n\"\n",
    "    else:\n",
    "        # 기본 템플릿 (Qwen, Llama 등)\n",
    "        if is_training:\n",
    "            prompt = f\"질문: {question}\\n\\n답변: {answer}\"\n",
    "        else:\n",
    "            prompt = f\"질문: {question}\\n\\n답변: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 샘플 프롬프트 확인\n",
    "if train_data:\n",
    "    sample = train_data[0]\n",
    "    sample_prompt = format_prompt(sample['question'], sample['answer'])\n",
    "    print(\"📝 프롬프트 템플릿 예시:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(sample_prompt[:500] + \"...\" if len(sample_prompt) > 500 else sample_prompt)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 토큰화 함수\ndef tokenize_function(examples: Dict[str, List]) -> Dict[str, List]:\n    \"\"\"\n    배치 단위로 데이터를 토큰화\n    \n    Args:\n        examples: 배치 데이터 (questions, answers 포함)\n        \n    Returns:\n        토큰화된 데이터 딕셔너리\n    \"\"\"\n    # 프롬프트 생성 (에러 처리 추가)\n    prompts = []\n    for question, answer in zip(examples['question'], examples['answer']):\n        # None 값 체크\n        if question is None or answer is None:\n            continue\n        # 문자열 변환 및 공백 제거\n        question = str(question).strip()\n        answer = str(answer).strip()\n        if question and answer:\n            prompt = format_prompt(question, answer, is_training=True)\n            prompts.append(prompt)\n    \n    # 유효한 프롬프트가 없는 경우 처리\n    if not prompts:\n        raise ValueError(\"유효한 프롬프트가 없습니다!\")\n    \n    # 토큰화\n    model_inputs = tokenizer(\n        prompts,\n        max_length=2048,  # 최대 토큰 길이\n        padding=\"max_length\",  # 최대 길이까지 패딩\n        truncation=True,  # 긴 텍스트는 자르기\n        return_tensors=\"pt\"\n    )\n    \n    # 레이블 설정 (input_ids와 동일하게, 패딩 부분은 -100으로)\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n    \n    # 패딩 토큰은 손실 계산에서 제외 (-100으로 설정)\n    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n    \n    return model_inputs\n\n# 데이터를 HuggingFace Dataset으로 변환\ndef prepare_datasets(train_data: List[Dict], val_data: List[Dict]):\n    \"\"\"\n    학습/검증 데이터를 HuggingFace Dataset 형식으로 변환하고 토큰화\n    \n    Args:\n        train_data: 학습 데이터 리스트\n        val_data: 검증 데이터 리스트\n        \n    Returns:\n        (토큰화된 학습 데이터셋, 토큰화된 검증 데이터셋)\n    \"\"\"\n    # 리스트를 DataFrame으로 변환 (쉬운 처리를 위해)\n    train_df = pd.DataFrame(train_data)\n    val_df = pd.DataFrame(val_data)\n    \n    # HuggingFace Dataset으로 변환\n    train_dataset = HFDataset.from_pandas(train_df)\n    val_dataset = HFDataset.from_pandas(val_df)\n    \n    # 토큰화 적용\n    print(\"🔄 학습 데이터 토큰화 중...\")\n    tokenized_train = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=train_dataset.column_names\n    )\n    \n    print(\"🔄 검증 데이터 토큰화 중...\")\n    tokenized_val = val_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=val_dataset.column_names\n    )\n    \n    print(\"\\n✅ 토큰화 완료!\")\n    print(f\"  - 학습 데이터: {len(tokenized_train)}개\")\n    print(f\"  - 검증 데이터: {len(tokenized_val)}개\")\n    \n    return tokenized_train, tokenized_val\n\n# 데이터셋 준비\nif train_data and val_data:\n    tokenized_train_dataset, tokenized_val_dataset = prepare_datasets(train_data, val_data)\nelse:\n    print(\"⚠️ 학습 데이터가 없습니다! FSKU_1_데이터증강_RAG.ipynb를 먼저 실행하세요.\")\n    tokenized_train_dataset, tokenized_val_dataset = None, None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 토큰 길이 분포 시각화\ndef visualize_token_distribution(dataset, title=\"Token Length Distribution\"):\n    \"\"\"\n    데이터셋의 토큰 길이 분포를 시각화\n    \n    Args:\n        dataset: 토큰화된 데이터셋\n        title: 그래프 제목\n    \"\"\"\n    if dataset is None:\n        return\n    \n    # 실제 토큰 길이 계산 (패딩 제외) - 메모리 효율적으로\n    lengths = []\n    sample_size = min(1000, len(dataset))  # 최대 1000개 샘플만 분석\n    indices = np.random.choice(len(dataset), sample_size, replace=False)\n    \n    for idx in indices:\n        item = dataset[int(idx)]\n        # attention_mask가 1인 부분만 실제 토큰\n        actual_length = sum(item['attention_mask'])\n        lengths.append(actual_length)\n    \n    print(f\"\\n📊 샘플 크기: {sample_size}개 (전체 {len(dataset)}개 중)\")\n    \n    # 통계 계산\n    avg_length = np.mean(lengths)\n    median_length = np.median(lengths)\n    max_length = np.max(lengths)\n    \n    # 시각화\n    plt.figure(figsize=(10, 6))\n    plt.hist(lengths, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    plt.axvline(avg_length, color='red', linestyle='--', label=f'Average: {avg_length:.0f}')\n    plt.axvline(median_length, color='green', linestyle='--', label=f'Median: {median_length:.0f}')\n    plt.xlabel('Token Length')\n    plt.ylabel('Frequency')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    print(f\"📊 토큰 길이 통계:\")\n    print(f\"  - 평균: {avg_length:.0f} 토큰\")\n    print(f\"  - 중간값: {median_length:.0f} 토큰\")\n    print(f\"  - 최대: {max_length} 토큰\")\n    print(f\"  - 2048 토큰 초과: {sum(1 for l in lengths if l >= 2048)}개 ({sum(1 for l in lengths if l >= 2048)/len(lengths)*100:.1f}%)\")\n\n# 학습 데이터 토큰 분포 확인\nif tokenized_train_dataset:\n    visualize_token_distribution(tokenized_train_dataset, \"Training Data Token Distribution\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 학습 하이퍼파라미터 설정\ndef get_training_args(output_dir: str = \"./models/checkpoints\"):\n    \"\"\"\n    RTX 4090 24GB에 최적화된 학습 설정 반환\n    \n    Args:\n        output_dir: 체크포인트 저장 디렉토리\n        \n    Returns:\n        TrainingArguments 객체\n    \"\"\"\n    # 전체 학습 스텝 수 계산\n    if tokenized_train_dataset:\n        steps_per_epoch = len(tokenized_train_dataset) // (4 * 4)  # batch_size=4, gradient_accumulation=4\n        total_steps = steps_per_epoch * 3  # num_epochs=3\n    else:\n        total_steps = 1000  # 기본값\n    \n    # 메모리 기반 배치 크기 자동 조정\n    if torch.cuda.is_available():\n        free_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        if free_gb < 12:\n            batch_size = 2\n            print(f\"⚠️ GPU 메모리 제한으로 batch_size를 {batch_size}로 조정합니다.\")\n        else:\n            batch_size = 4\n    \n    return TrainingArguments(\n        # 기본 설정\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        \n        # 학습 설정\n        num_train_epochs=3,  # 에폭 수 (3-5 권장)\n        per_device_train_batch_size=batch_size,  # 동적 배치 크기\n        per_device_eval_batch_size=batch_size,\n        gradient_accumulation_steps=4,  # 실제 배치 크기 = batch_size * 4\n        gradient_checkpointing=True,  # 메모리 절약\n        \n        # 옵티마이저 설정\n        learning_rate=2e-4,  # LoRA용 학습률 (일반적으로 1e-4 ~ 5e-4)\n        weight_decay=0.01,  # 가중치 감쇠\n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,  # Gradient clipping\n        \n        # 학습률 스케줄러\n        lr_scheduler_type=\"cosine\",  # Cosine 스케줄러 사용\n        warmup_steps=int(total_steps * 0.1),  # 전체의 10%를 warmup\n        \n        # 로깅 및 저장\n        logging_steps=10,\n        logging_first_step=True,\n        save_strategy=\"steps\",\n        save_steps=100,\n        save_total_limit=3,  # 최대 3개의 체크포인트만 유지\n        \n        # 평가 설정\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        load_best_model_at_end=True,\n        \n        # 기타 설정\n        fp16=True,  # Mixed precision training\n        fp16_opt_level=\"O1\",  # 안정적인 mixed precision\n        dataloader_num_workers=min(4, psutil.cpu_count()),  # CPU 코어 수에 맞게 조정\n        remove_unused_columns=False,\n        push_to_hub=False,  # Hugging Face Hub에 푸시하지 않음\n        report_to=[\"tensorboard\"],  # TensorBoard 로깅 활성화\n        logging_dir=\"./logs\",  # TensorBoard 로그 디렉토리\n        seed=42,  # 재현성을 위한 시드\n        dataloader_pin_memory=True,  # GPU 전송 속도 향상\n        ddp_find_unused_parameters=False,  # DDP 최적화\n    )\n\n# 학습 설정 생성\ntraining_args = get_training_args()\n\nprint(\"⚙️ 학습 설정 완료!\")\nprint(f\"  - 에폭 수: {training_args.num_train_epochs}\")\nprint(f\"  - 배치 크기: {training_args.per_device_train_batch_size} × {training_args.gradient_accumulation_steps} = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - 학습률: {training_args.learning_rate}\")\nprint(f\"  - Warmup 스텝: {training_args.warmup_steps}\")\nprint(f\"  - 체크포인트 저장 간격: {training_args.save_steps} 스텝\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기 종료 콜백 설정\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # 3번의 평가에서 개선이 없으면 종료\n",
    "    early_stopping_threshold=0.001  # 최소 개선 폭\n",
    ")\n",
    "\n",
    "print(\"🛑 조기 종료 설정 완료!\")\n",
    "print(f\"  - Patience: {early_stopping_callback.early_stopping_patience}\")\n",
    "print(f\"  - Threshold: {early_stopping_callback.early_stopping_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 커스텀 Trainer 클래스 (선택사항)\nclass FSKUTrainer(Trainer):\n    \"\"\"\n    FSKU 프로젝트용 커스텀 Trainer\n    학습 중 추가 로깅이나 커스터마이징이 필요한 경우 사용\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_history = []\n        \n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"\n        손실 계산 (필요시 커스터마이징 가능)\n        \"\"\"\n        # 기본 손실 계산\n        outputs = model(**inputs)\n        loss = outputs.loss\n        \n        # 손실 기록\n        self.loss_history.append(loss.item())\n        \n        # Perplexity 계산 (언어 모델의 중요 지표)\n        if len(self.loss_history) % 100 == 0:\n            avg_loss = np.mean(self.loss_history[-100:])\n            perplexity = np.exp(avg_loss)\n            print(f\"\\n📊 최근 100 스텝 평균 Perplexity: {perplexity:.2f}\")\n        \n        return (loss, outputs) if return_outputs else loss\n    \n    def log(self, logs):\n        \"\"\"로깅 커스터마이징\"\"\"\n        # GPU 메모리 사용량 추가\n        if torch.cuda.is_available():\n            logs[\"gpu_memory_gb\"] = torch.cuda.memory_allocated() / 1024**3\n        super().log(logs)\n\n# Trainer 초기화\nif tokenized_train_dataset and tokenized_val_dataset:\n    trainer = FSKUTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_dataset,\n        eval_dataset=tokenized_val_dataset,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False,  # Causal LM이므로 MLM 비활성화\n            pad_to_multiple_of=8  # 효율성을 위해 8의 배수로 패딩\n        ),\n        callbacks=[early_stopping_callback],\n    )\n    \n    print(\"\\n✅ Trainer 초기화 완료!\")\n    print(f\"📊 총 학습 스텝 수: {trainer.args.max_steps if trainer.args.max_steps > 0 else len(tokenized_train_dataset) // (trainer.args.per_device_train_batch_size * trainer.args.gradient_accumulation_steps) * trainer.args.num_train_epochs}\")\nelse:\n    trainer = None\n    print(\"⚠️ 학습 데이터가 없어 Trainer를 초기화할 수 없습니다!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU 메모리 정리 함수\ndef clear_gpu_memory():\n    \"\"\"\n    GPU 메모리를 정리하여 OOM 방지\n    \"\"\"\n    import gc\n    \n    # 기존 변수들 정리\n    for obj in gc.get_objects():\n        if torch.is_tensor(obj):\n            del obj\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    if torch.cuda.is_available():\n        # 메모리 사용 통계\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        free = torch.cuda.get_device_properties(0).total_memory / 1024**3 - reserved\n        \n        print(f\"🧹 GPU 메모리 정리 완료!\")\n        print(f\"   할당된 메모리: {allocated:.2f} GB\")\n        print(f\"   예약된 메모리: {reserved:.2f} GB\")\n        print(f\"   사용 가능: {free:.2f} GB\")\n        \n        # 메모리 부족 경고\n        if free < 5:\n            print(\"⚠️ GPU 메모리가 5GB 미만입니다. 학습 중 OOM 발생 가능성이 있습니다.\")\n\n# 학습 전 메모리 정리\nclear_gpu_memory()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델 학습 실행\nif trainer is not None:\n    print(\"\\n🚀 모델 학습을 시작합니다!\")\n    print(\"⏱️ 예상 소요 시간: 데이터 크기에 따라 1-4시간\")\n    print(\"💡 팁: 학습 중 GPU 메모리가 부족하면 batch_size를 줄이세요.\\n\")\n    \n    # 학습 시작 시간 기록\n    start_time = datetime.now()\n    \n    try:\n        # 학습 실행\n        train_result = trainer.train()\n        \n        # 학습 완료\n        end_time = datetime.now()\n        training_time = end_time - start_time\n        \n        print(f\"\\n✅ 학습 완료!\")\n        print(f\"⏱️ 총 학습 시간: {training_time}\")\n        print(f\"📊 최종 학습 손실: {train_result.training_loss:.4f}\")\n        \n        # 학습 메트릭 저장 (더 상세한 정보)\n        metrics = {\n            \"training_loss\": float(train_result.training_loss),\n            \"training_time\": str(training_time),\n            \"training_time_seconds\": training_time.total_seconds(),\n            \"model_name\": MODEL_NAME,\n            \"model_type\": SELECTED_MODEL,\n            \"total_steps\": train_result.global_step,\n            \"epochs\": training_args.num_train_epochs,\n            \"train_samples\": len(train_data),\n            \"val_samples\": len(val_data),\n            \"batch_size\": training_args.per_device_train_batch_size,\n            \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n            \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n            \"learning_rate\": training_args.learning_rate,\n            \"lora_rank\": lora_config.r,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        # 최종 검증 손실 추가 (있는 경우)\n        if hasattr(trainer.state, 'best_metric'):\n            metrics[\"best_eval_loss\"] = float(trainer.state.best_metric)\n        \n        # 메트릭 저장\n        os.makedirs(\"results\", exist_ok=True)\n        with open(\"results/training_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(metrics, f, indent=2, ensure_ascii=False)\n        \n        print(\"\\n📊 학습 메트릭이 results/training_metrics.json에 저장되었습니다.\")\n        \n    except Exception as e:\n        print(f\"\\n❌ 학습 중 오류 발생: {str(e)}\")\n        print(\"\\n💡 해결 방법:\")\n        print(\"  1. GPU 메모리 부족: batch_size를 2로 줄이기\")\n        print(\"  2. CUDA 오류: GPU 드라이버 업데이트\")\n        print(\"  3. 그 외: 에러 메시지 확인 후 구글링\")\n        raise e\nelse:\n    print(\"⚠️ Trainer가 초기화되지 않아 학습을 시작할 수 없습니다!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 곡선 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 로그 시각화\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    학습 과정의 손실 변화를 시각화\n",
    "    \n",
    "    Args:\n",
    "        trainer: 학습이 완료된 Trainer 객체\n",
    "    \"\"\"\n",
    "    if trainer is None or not hasattr(trainer.state, 'log_history'):\n",
    "        print(\"⚠️ 학습 로그가 없습니다!\")\n",
    "        return\n",
    "    \n",
    "    # 로그에서 손실 값 추출\n",
    "    log_history = trainer.state.log_history\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in log_history:\n",
    "        if 'loss' in log:\n",
    "            train_loss.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_loss)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_loss.append(log['eval_loss'])\n",
    "    \n",
    "    # 시각화\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 학습 손실\n",
    "    if train_loss:\n",
    "        ax1.plot(steps[:len(train_loss)], train_loss, 'b-', label='Training Loss')\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss Over Time')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # 검증 손실\n",
    "    if eval_loss:\n",
    "        eval_steps = [i * training_args.eval_steps for i in range(1, len(eval_loss) + 1)]\n",
    "        ax2.plot(eval_steps, eval_loss, 'r-', marker='o', label='Validation Loss')\n",
    "        ax2.set_xlabel('Steps')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Validation Loss Over Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 최종 손실 출력\n",
    "    if train_loss:\n",
    "        print(f\"\\n📊 최종 학습 손실: {train_loss[-1]:.4f}\")\n",
    "    if eval_loss:\n",
    "        print(f\"📊 최종 검증 손실: {eval_loss[-1]:.4f}\")\n",
    "        print(f\"📊 최고 검증 손실: {min(eval_loss):.4f}\")\n",
    "\n",
    "# 학습 곡선 그리기\n",
    "if trainer and hasattr(trainer, 'state'):\n",
    "    plot_training_history(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 최종 모델 저장\ndef save_final_model(trainer, output_dir: str = \"models/fsku_finetuned_model\"):\n    \"\"\"\n    학습된 모델을 추론용으로 저장\n    \n    Args:\n        trainer: 학습이 완료된 Trainer 객체\n        output_dir: 모델 저장 경로\n    \"\"\"\n    if trainer is None:\n        print(\"⚠️ 저장할 모델이 없습니다!\")\n        return\n    \n    print(f\"\\n💾 모델을 저장합니다: {output_dir}\")\n    \n    # 디렉토리 생성\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # 최고 성능 모델 저장 (LoRA 어댑터만)\n    trainer.save_model(output_dir)\n    \n    # 토크나이저도 저장\n    trainer.tokenizer.save_pretrained(output_dir)\n    \n    # 설정 정보 저장 (더 상세한 정보)\n    config_info = {\n        \"base_model\": MODEL_NAME,\n        \"model_type\": SELECTED_MODEL,\n        \"training_completed\": datetime.now().isoformat(),\n        \"lora_config\": {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"target_modules\": lora_config.target_modules\n        },\n        \"training_args\": {\n            \"num_train_epochs\": training_args.num_train_epochs,\n            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n            \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n            \"learning_rate\": training_args.learning_rate,\n            \"warmup_steps\": training_args.warmup_steps,\n            \"fp16\": training_args.fp16\n        },\n        \"dataset_info\": {\n            \"train_samples\": len(train_data) if 'train_data' in globals() else 0,\n            \"val_samples\": len(val_data) if 'val_data' in globals() else 0\n        },\n        \"final_loss\": float(trainer.state.log_history[-1].get('loss', 0)) if hasattr(trainer, 'state') and trainer.state.log_history else 'N/A',\n        \"quantization\": \"4bit\",\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    }\n    \n    with open(os.path.join(output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(config_info, f, indent=2, ensure_ascii=False)\n    \n    print(\"\\n✅ 모델 저장 완료!\")\n    print(f\"📁 저장 위치: {output_dir}\")\n    print(\"\\n📝 저장된 파일:\")\n    print(\"  - adapter_model.safetensors (LoRA 가중치)\")\n    print(\"  - adapter_config.json (LoRA 설정)\")\n    print(\"  - tokenizer 파일들\")\n    print(\"  - training_config.json (학습 정보)\")\n    print(\"\\n💡 추론시 이 경로를 FSKU_3_추론.ipynb에서 사용하세요!\")\n    \n    return output_dir\n\n# 모델 저장 실행\nif trainer:\n    saved_model_path = save_final_model(trainer)\nelse:\n    print(\"⚠️ 학습된 모델이 없어 저장할 수 없습니다!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 모델 평가 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델로 샘플 생성 테스트\n",
    "def generate_sample(model, tokenizer, prompt: str, max_length: int = 512):\n",
    "    \"\"\"\n",
    "    학습된 모델로 텍스트 생성 테스트\n",
    "    \n",
    "    Args:\n",
    "        model: 학습된 모델\n",
    "        tokenizer: 토크나이저\n",
    "        prompt: 입력 프롬프트\n",
    "        max_length: 최대 생성 길이\n",
    "        \n",
    "    Returns:\n",
    "        생성된 텍스트\n",
    "    \"\"\"\n",
    "    # 모델을 평가 모드로\n",
    "    model.eval()\n",
    "    \n",
    "    # 프롬프트 토큰화\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 디코딩\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 입력 프롬프트 제거\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 샘플 테스트\n",
    "if trainer and val_data:\n",
    "    print(\"\\n🧪 학습된 모델 테스트\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 검증 데이터에서 샘플 선택\n",
    "    test_samples = random.sample(val_data, min(3, len(val_data)))\n",
    "    \n",
    "    for i, sample in enumerate(test_samples):\n",
    "        print(f\"\\n[테스트 {i+1}]\")\n",
    "        \n",
    "        # 질문만으로 프롬프트 생성\n",
    "        test_prompt = format_prompt(sample['question'], \"\", is_training=False)\n",
    "        \n",
    "        print(f\"질문: {sample['question']}\")\n",
    "        print(f\"\\n정답: {sample['answer'][:200]}...\" if len(sample['answer']) > 200 else f\"\\n정답: {sample['answer']}\")\n",
    "        \n",
    "        # 모델 생성\n",
    "        generated = generate_sample(model, tokenizer, test_prompt)\n",
    "        print(f\"\\n모델 생성: {generated[:200]}...\" if len(generated) > 200 else f\"\\n모델 생성: {generated}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"⚠️ 테스트할 모델이나 데이터가 없습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 최종 요약\nprint(\"\\n\" + \"=\" * 60)\nprint(\"📊 학습 완료 요약\")\nprint(\"=\" * 60)\n\nif trainer:\n    print(f\"\\n✅ 모델: {MODEL_NAME}\")\n    print(f\"✅ 모델 타입: {SELECTED_MODEL}\")\n    print(f\"✅ 학습 데이터: {len(train_data):,}개\")\n    print(f\"✅ 검증 데이터: {len(val_data):,}개\")\n    print(f\"✅ 학습 에폭: {training_args.num_train_epochs}\")\n    print(f\"✅ 최종 모델 저장: models/fsku_finetuned_model/\")\n    \n    # 성능 요약\n    if hasattr(trainer, 'state') and trainer.state.log_history:\n        final_train_loss = trainer.state.log_history[-1].get('loss', 'N/A')\n        if isinstance(final_train_loss, float):\n            print(f\"\\n📈 성능 지표:\")\n            print(f\"  - 최종 학습 손실: {final_train_loss:.4f}\")\n            print(f\"  - 최종 Perplexity: {np.exp(final_train_loss):.2f}\")\n    \n    print(f\"\\n📝 다음 단계:\")\n    print(f\"   1. FSKU_3_추론.ipynb 파일 실행\")\n    print(f\"   2. 모델 경로로 'models/fsku_finetuned_model' 사용\")\n    print(f\"   3. test.csv 파일로 추론 수행\")\n    \n    print(f\"\\n💡 추가 팁:\")\n    print(f\"   - TensorBoard 로그 확인: tensorboard --logdir ./logs\")\n    print(f\"   - 모델 크기 확인: du -sh models/fsku_finetuned_model/\")\n    print(f\"   - GPU 메모리 모니터링: nvidia-smi -l 1\")\nelse:\n    print(\"\\n❌ 학습이 완료되지 않았습니다!\")\n    print(\"💡 FSKU_1_데이터증강_RAG.ipynb를 먼저 실행하여 데이터를 생성하세요.\")\n    print(\"\\n🔍 체크리스트:\")\n    print(\"   [ ] data/augmented/ 폴더에 JSON 파일이 있는지 확인\")\n    print(\"   [ ] GPU가 제대로 인식되는지 확인\")\n    print(\"   [ ] 필요한 패키지가 모두 설치되었는지 확인\")\n\nprint(\"\\n\" + \"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}