#!/usr/bin/env python3
"""
Colabìš© ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ê³ ì„±ëŠ¥ GPUì—ì„œ ìµœê³  í’ˆì§ˆì˜ ë°ì´í„°ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
"""

# Colab í™˜ê²½ ì²´í¬
import sys
try:
    import google.colab
    IN_COLAB = True
    print("âœ… Google Colab í™˜ê²½ ê°ì§€")
except:
    IN_COLAB = False
    print("âš ï¸ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
if IN_COLAB:
    print("\nğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...")
    !pip install -q transformers accelerate sentencepiece
    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    !pip install -q datasets tqdm

import torch
import json
import time
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# GPU ì •ë³´ ì¶œë ¥
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"\nğŸš€ GPU ê°ì§€: {gpu_name} ({gpu_memory:.1f}GB)")
else:
    print("\nâš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

class ColabDataGenerator:
    """Colabì—ì„œ ëŒ€ëŸ‰ ë°ì´í„° ìƒì„±"""
    
    def __init__(self, model_name="Qwen/Qwen2.5-32B-Instruct"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: Qwen 32B)
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)"""
        print(f"\nğŸ”„ ëª¨ë¸ ë¡œë”©: {self.model_name}")
        
        # í† í¬ë‚˜ì´ì €
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # í° ëª¨ë¸ì€ 4-bit ì–‘ìí™”
        if "32B" in self.model_name or "14B" in self.model_name:
            from transformers import BitsAndBytesConfig
            
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            print("âœ… 4-bit ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            # ì‘ì€ ëª¨ë¸ì€ FP16
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            print("âœ… FP16 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def generate_questions(self, num_questions=10000, batch_size=4):
        """
        ëŒ€ëŸ‰ ë¬¸ì œ ìƒì„±
        
        Args:
            num_questions: ìƒì„±í•  ë¬¸ì œ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ìƒì„±ëœ ë¬¸ì œ ë¦¬ìŠ¤íŠ¸
        """
        if self.model is None:
            self.load_model()
        
        questions = []
        concepts = self._get_finance_concepts()
        
        print(f"\nğŸ“Š {num_questions}ê°œ ë¬¸ì œ ìƒì„± ì‹œì‘...")
        print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        with tqdm(total=num_questions) as pbar:
            while len(questions) < num_questions:
                # ë°°ì¹˜ ìƒì„±
                batch_prompts = []
                for _ in range(min(batch_size, num_questions - len(questions))):
                    concept = concepts[len(questions) % len(concepts)]
                    prompt = self._create_prompt(concept)
                    batch_prompts.append(prompt)
                
                # ë°°ì¹˜ ì²˜ë¦¬
                try:
                    generated = self._generate_batch(batch_prompts)
                    
                    for i, text in enumerate(generated):
                        parsed = self._parse_question(text)
                        if parsed and self._check_quality(parsed):
                            questions.append({
                                'id': f"COLAB_{len(questions)+1:05d}",
                                'concept': concepts[len(questions) % len(concepts)],
                                'question': parsed['question'],
                                'choices': parsed.get('choices', []),
                                'answer': parsed['answer'],
                                'explanation': parsed.get('explanation', ''),
                                'quality_score': self._calculate_quality(parsed),
                                'timestamp': datetime.now().isoformat()
                            })
                            pbar.update(1)
                
                except Exception as e:
                    print(f"\nâš ï¸ ìƒì„± ì˜¤ë¥˜: {e}")
                    continue
        
        return questions
    
    def _create_prompt(self, concept):
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°œë…ì— ëŒ€í•œ ê³ í’ˆì§ˆ FSKU ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ê¸ˆìœµ ê°œë…: {concept}

ìš”êµ¬ì‚¬í•­:
1. í•œêµ­ ê¸ˆìœµ ì‹¤ë¬´ì™€ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©
2. ëª…í™•í•˜ê³  ëª¨í˜¸í•˜ì§€ ì•Šì€ ë¬¸ì œ
3. 4ì§€ì„ ë‹¤ ê°ê´€ì‹ ë˜ëŠ” ì„œìˆ í˜•
4. ì‹¤ì œ ê¸ˆìœµ ì „ë¬¸ê°€ê°€ ì•Œì•„ì•¼ í•  ë‚´ìš©

[ë¬¸ì œ]
(ì—¬ê¸°ì— ë¬¸ì œ ì‘ì„±)

[ì„ íƒì§€] (ê°ê´€ì‹ì¸ ê²½ìš°)
1) 
2) 
3) 
4) 

[ì •ë‹µ]
(ì •ë‹µ ë²ˆí˜¸ ë˜ëŠ” ì„œìˆ í˜• ë‹µë³€)

[í•´ì„¤]
(ì™œ ì´ê²ƒì´ ì •ë‹µì¸ì§€ ì„¤ëª…)"""

    def _generate_batch(self, prompts):
        """ë°°ì¹˜ ìƒì„±"""
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.8,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        generated_texts = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            for prompt in prompts:
                if prompt in text:
                    text = text.replace(prompt, "").strip()
                    break
            generated_texts.append(text)
        
        return generated_texts
    
    def _parse_question(self, text):
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        import re
        
        result = {
            'question': '',
            'choices': [],
            'answer': '',
            'explanation': ''
        }
        
        # ë¬¸ì œ ì¶”ì¶œ
        question_match = re.search(r'\[ë¬¸ì œ\](.*?)(?:\[|$)', text, re.DOTALL)
        if question_match:
            result['question'] = question_match.group(1).strip()
        
        # ì„ íƒì§€ ì¶”ì¶œ
        choices_match = re.search(r'\[ì„ íƒì§€\](.*?)(?:\[|$)', text, re.DOTALL)
        if choices_match:
            choices_text = choices_match.group(1).strip()
            choice_pattern = r'([1-4])\)\s*(.+?)(?=(?:[1-4]\)|$))'
            choices = re.findall(choice_pattern, choices_text, re.DOTALL)
            result['choices'] = [f"{num}) {text.strip()}" for num, text in choices]
        
        # ì •ë‹µ ì¶”ì¶œ
        answer_match = re.search(r'\[ì •ë‹µ\](.*?)(?:\[|$)', text, re.DOTALL)
        if answer_match:
            result['answer'] = answer_match.group(1).strip()
        
        # í•´ì„¤ ì¶”ì¶œ
        explanation_match = re.search(r'\[í•´ì„¤\](.*?)(?:\[|$)', text, re.DOTALL)
        if explanation_match:
            result['explanation'] = explanation_match.group(1).strip()
        
        return result if result['question'] and result['answer'] else None
    
    def _check_quality(self, parsed):
        """í’ˆì§ˆ ê²€ì¦"""
        # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        if len(parsed['question']) < 20:
            return False
        
        if parsed['choices'] and len(parsed['choices']) < 4:
            return False
        
        if not parsed['answer']:
            return False
        
        return True
    
    def _calculate_quality(self, parsed):
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 70  # ê¸°ë³¸ ì ìˆ˜
        
        # ë¬¸ì œ ê¸¸ì´
        if len(parsed['question']) > 100:
            score += 10
        
        # í•´ì„¤ ìœ ë¬´
        if parsed['explanation'] and len(parsed['explanation']) > 50:
            score += 10
        
        # ì„ íƒì§€ í’ˆì§ˆ
        if parsed['choices'] and all(len(c) > 20 for c in parsed['choices']):
            score += 10
        
        return min(score, 100)
    
    def _get_finance_concepts(self):
        """ê¸ˆìœµ ê°œë… ë¦¬ìŠ¤íŠ¸"""
        return [
            "ì „ìê¸ˆìœµê±°ë˜", "ê¸ˆìœµë³´ì•ˆ", "ê°œì¸ì •ë³´ë³´í˜¸", "ì•”í˜¸ê¸°ìˆ ",
            "ë¸”ë¡ì²´ì¸", "í•€í…Œí¬", "ì˜¤í”ˆë±…í‚¹", "ë§ˆì´ë°ì´í„°",
            "ì¸ì¦ì„œ", "ë°”ì´ì˜¤ì¸ì¦", "ì´ìƒê±°ë˜íƒì§€", "ìê¸ˆì„¸íƒë°©ì§€",
            "ì‹ ìš©í‰ê°€", "ë¦¬ìŠ¤í¬ê´€ë¦¬", "ë‚´ë¶€í†µì œ", "ì»´í”Œë¼ì´ì–¸ìŠ¤",
            "ê¸ˆìœµì‚¬ê³ ", "ë³´ì•ˆì·¨ì•½ì ", "ì¹¨í•´ì‚¬ê³ ëŒ€ì‘", "ì¬í•´ë³µêµ¬",
            "í´ë¼ìš°ë“œë³´ì•ˆ", "APIë³´ì•ˆ", "ëª¨ë°”ì¼ë³´ì•ˆ", "AIë³´ì•ˆ"
        ]
    
    def save_results(self, questions, output_path="generated_data_colab.jsonl"):
        """ê²°ê³¼ ì €ì¥"""
        # í’ˆì§ˆìˆœ ì •ë ¬
        questions.sort(key=lambda x: x['quality_score'], reverse=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for q in questions:
                f.write(json.dumps(q, ensure_ascii=False) + '\n')
        
        print(f"\nâœ… {len(questions)}ê°œ ë¬¸ì œ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # í†µê³„ ì¶œë ¥
        avg_quality = sum(q['quality_score'] for q in questions) / len(questions)
        print(f"ğŸ“Š í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.1f}")
        print(f"ğŸ“Š 90ì  ì´ìƒ: {sum(1 for q in questions if q['quality_score'] >= 90)}ê°œ")
        print(f"ğŸ“Š 80ì  ì´ìƒ: {sum(1 for q in questions if q['quality_score'] >= 80)}ê°œ")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸš€ Colab ëŒ€ëŸ‰ ë°ì´í„° ìƒì„±")
    print("="*60)
    
    # ëª¨ë¸ ì„ íƒ
    print("\nëª¨ë¸ ì„ íƒ:")
    print("1. Qwen2.5-32B (ìµœê³  í’ˆì§ˆ) â­")
    print("2. Qwen2.5-14B (ê· í˜•)")
    print("3. EXAONE-3.0-7.8B (í•œêµ­ íŠ¹í™”)")
    print("4. SOLAR-10.7B (ê²€ì¦ëœ ì„±ëŠ¥)")
    
    choice = input("\nì„ íƒ [1]: ").strip() or "1"
    
    model_map = {
        "1": "Qwen/Qwen2.5-32B-Instruct",
        "2": "Qwen/Qwen2.5-14B-Instruct", 
        "3": "LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct",
        "4": "upstage/SOLAR-10.7B-v1.0"
    }
    
    model_name = model_map.get(choice, model_map["1"])
    
    # ìƒì„± ê°œìˆ˜
    num_questions = int(input("ìƒì„±í•  ë¬¸ì œ ìˆ˜ [10000]: ").strip() or "10000")
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = ColabDataGenerator(model_name)
    
    # ìƒì„± ì‹œì‘
    start_time = time.time()
    questions = generator.generate_questions(num_questions)
    elapsed = time.time() - start_time
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"colab_data_{timestamp}.jsonl"
    generator.save_results(questions, output_file)
    
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
    print(f"âš¡ í‰ê·  ìƒì„± ì†ë„: {len(questions)/elapsed:.1f} ë¬¸ì œ/ì´ˆ")
    
    # Google Drive ì €ì¥ (Colabì¸ ê²½ìš°)
    if IN_COLAB:
        from google.colab import drive
        drive.mount('/content/drive')
        
        save_path = f"/content/drive/MyDrive/FSKU/{output_file}"
        Path("/content/drive/MyDrive/FSKU").mkdir(exist_ok=True)
        
        import shutil
        shutil.copy(output_file, save_path)
        print(f"\nğŸ’¾ Google Drive ì €ì¥ ì™„ë£Œ: {save_path}")

if __name__ == "__main__":
    main()