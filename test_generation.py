#!/usr/bin/env python3
"""
ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
EXAONE ë˜ëŠ” Qwen ëª¨ë¸ë¡œ ì§ˆë¬¸-ë‹µë³€ ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime

# src ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent / "src"))

from generate_data.main import DataGenerator
from rag.retriever import DocumentRetriever


def test_generation_with_prompts():
    """ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    
    print("="*80)
    print("ğŸ§ª EXAONE ëª¨ë¸ì„ ì´ìš©í•œ ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤
    prompt_templates = [
        'prompts/diverse_types.txt',
        'prompts/high_quality.txt', 
        'prompts/training_data.txt'
    ]
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì • (Qwen ë˜ëŠ” SOLAR ì‚¬ìš©)
    base_config = {
        'model_name': 'Qwen/Qwen2.5-7B-Instruct',  # EXAONE ëŒ€ì‹  Qwen ì‚¬ìš©
        'use_rag': True,
        'use_quantization': False,  # Macì´ë¯€ë¡œ ë¹„í™œì„±í™”
        'temperature': 0.7,
        'top_p': 0.9
    }
    
    all_results = []
    
    for template_path in prompt_templates:
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¤‘: {template_path}")
        print("-"*60)
        
        config = base_config.copy()
        config['prompt_template'] = template_path
        
        try:
            # ìƒì„±ê¸° ì´ˆê¸°í™”
            generator = DataGenerator(config)
            
            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
            generator.load_model()
            
            # RAG í…ŒìŠ¤íŠ¸
            if generator.retriever:
                print("\nğŸ“š RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
                test_query = "ê¸ˆìœµë³´ì•ˆ"
                context = generator.retriever.search(test_query, top_k=2)
                if context:
                    print(f"âœ… RAG ì‘ë™ í™•ì¸ (ê²€ìƒ‰ì–´: {test_query})")
                    print(f"   ê²€ìƒ‰ ê²°ê³¼ ê¸¸ì´: {len(context)} ê¸€ì")
                else:
                    print("âš ï¸ RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            # í…ŒìŠ¤íŠ¸ ë¬¸ì œ ìƒì„± (3ê°œë§Œ)
            print("\nğŸ¯ ë¬¸ì œ ìƒì„± ì¤‘...")
            start_time = time.time()
            
            # ê¸ˆìœµ ê´€ë ¨ ì£¼ì œë“¤
            finance_topics = ["ìê¸ˆì„¸íƒë°©ì§€", "ê°œì¸ì •ë³´ë³´í˜¸", "ë‚´ë¶€í†µì œ", "ë¦¬ìŠ¤í¬ê´€ë¦¬", "íˆ¬ììë³´í˜¸"]
            
            generated_items = []
            for i, topic in enumerate(finance_topics[:3], 1):
                print(f"\n[{i}/3] ì£¼ì œ: {topic}")
                
                # RAGë¡œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
                context = ""
                if generator.retriever:
                    context = generator.retriever.search(topic, top_k=2)
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = generator.prompt_template.format(
                    concept=topic,
                    context=context if context else "ê¸ˆìœµë³´ì•ˆ ê´€ë ¨ ì¼ë°˜ ì§€ì‹"
                )
                
                # LLMìœ¼ë¡œ ìƒì„±
                try:
                    generated_text = generator._generate_with_llm(prompt, temperature=0.7)
                    
                    result = {
                        'template': Path(template_path).stem,
                        'topic': topic,
                        'context_used': bool(context),
                        'generated': generated_text[:500] + "..." if len(generated_text) > 500 else generated_text,
                        'full_text': generated_text,
                        'length': len(generated_text)
                    }
                    
                    generated_items.append(result)
                    all_results.append(result)
                    
                    # ê°„ë‹¨í•œ ì¶œë ¥
                    print(f"âœ… ìƒì„± ì™„ë£Œ ({len(generated_text)} ê¸€ì)")
                    
                except Exception as e:
                    print(f"âŒ ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            elapsed = time.time() - start_time
            print(f"\nâ±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            generator.cleanup()
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    # ê²°ê³¼ ì €ì¥
    output_file = Path("test_results") / f"generation_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'test_time': datetime.now().isoformat(),
            'model': base_config['model_name'],
            'templates_tested': prompt_templates,
            'results': all_results
        }, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*80)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    for template in prompt_templates:
        template_name = Path(template).stem
        template_results = [r for r in all_results if r['template'] == template_name]
        
        if template_results:
            print(f"\nğŸ“ {template_name}:")
            avg_length = sum(r['length'] for r in template_results) / len(template_results)
            print(f"   - ìƒì„± ê°œìˆ˜: {len(template_results)}")
            print(f"   - í‰ê·  ê¸¸ì´: {avg_length:.0f} ê¸€ì")
            print(f"   - RAG ì‚¬ìš©ë¥ : {sum(1 for r in template_results if r['context_used']) / len(template_results) * 100:.0f}%")
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_file}")
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # ìƒì„±ëœ ë‚´ìš© ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“„ ìƒì„± ìƒ˜í”Œ (ì²« ë²ˆì§¸ ê²°ê³¼)")
    print("="*80)
    
    if all_results:
        first_result = all_results[0]
        print(f"í…œí”Œë¦¿: {first_result['template']}")
        print(f"ì£¼ì œ: {first_result['topic']}")
        print(f"RAG ì‚¬ìš©: {'ì˜ˆ' if first_result['context_used'] else 'ì•„ë‹ˆì˜¤'}")
        print(f"\nìƒì„±ëœ ë‚´ìš©:\n{'-'*60}")
        print(first_result['full_text'][:1000])
        if len(first_result['full_text']) > 1000:
            print("\n... (ì´í•˜ ìƒëµ)")


def test_simple_generation():
    """ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í™•ì¸ìš©)"""
    
    print("\nğŸš€ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("-"*60)
    
    # Qwen ì‚¬ìš© (ì‘ê³  ë¹ ë¦„)
    config = {
        'model_name': 'Qwen/Qwen2.5-1.5B-Instruct',  # ì‘ì€ ëª¨ë¸
        'use_rag': True,
        'use_quantization': False,
        'prompt_template': 'prompts/training_data.txt'
    }
    
    try:
        generator = DataGenerator(config)
        generator.load_model()
        
        # í•œ ê°œë§Œ í…ŒìŠ¤íŠ¸
        topic = "ê¸ˆìœµë³´ì•ˆ"
        
        # RAG í…ŒìŠ¤íŠ¸
        context = ""
        if generator.retriever:
            # ëœë¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
            context = generator.retriever.get_random_chunks(n=2)
            if context:
                print(f"âœ… RAG ëœë¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
        
        # í”„ë¡¬í”„íŠ¸
        prompt = generator.prompt_template.format(
            concept=topic,
            context=context if context else "ê¸ˆìœµ ê´€ë ¨ ì¼ë°˜ ì§€ì‹"
        )
        
        print("\nìƒì„± ì¤‘...")
        result = generator._generate_with_llm(prompt, temperature=0.7)
        
        print("\n" + "="*60)
        print("ìƒì„± ê²°ê³¼:")
        print("="*60)
        print(result)
        
        generator.cleanup()
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸")
    parser.add_argument('--simple', action='store_true', help='ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    parser.add_argument('--model', type=str, help='ì‚¬ìš©í•  ëª¨ë¸ (exaone/qwen)')
    
    args = parser.parse_args()
    
    if args.simple:
        test_simple_generation()
    else:
        if args.model and args.model.lower() == 'qwen':
            # Qwenìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ config ìˆ˜ì •
            print("Qwen ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
        test_generation_with_prompts()