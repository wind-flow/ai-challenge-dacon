#!/usr/bin/env python3
"""
ëª¨ë¸ ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ëª¨ë¸ ìžë™ ì„ íƒ

EXAONEì´ gatedì¸ ê²½ìš° ìžë™ìœ¼ë¡œ ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©
"""

from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoader:
    """ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ë¡œë”"""
    
    # ëª¨ë¸ ìš°ì„ ìˆœìœ„ (ë†’ì€ ìˆœ)
    MODEL_PRIORITY = [
        {
            'name': 'LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct',
            'fallback': 'upstage/SOLAR-10.7B-v1.0',
            'type': 'korean-specialized'
        },
        {
            'name': 'upstage/SOLAR-10.7B-v1.0',
            'fallback': 'Qwen/Qwen2.5-7B-Instruct',
            'type': 'korean-verified'
        },
        {
            'name': 'Qwen/Qwen2.5-14B-Instruct',
            'fallback': 'Qwen/Qwen2.5-7B-Instruct',
            'type': 'large-scale'
        },
        {
            'name': 'Qwen/Qwen2.5-7B-Instruct',
            'fallback': 'NCSOFT/Llama-VARCO-8B-Instruct',
            'type': 'balanced'
        }
    ]
    
    @classmethod
    def load_model(cls, model_name=None, device="auto", quantization=None):
        """
        ëª¨ë¸ ë¡œë“œ (ì‹¤íŒ¨ ì‹œ ìžë™ ëŒ€ì²´)
        
        Args:
            model_name: ìš”ì²­í•œ ëª¨ë¸ëª…
            device: ë””ë°”ì´ìŠ¤ ì„¤ì •
            quantization: ì–‘ìží™” ì„¤ì • (4bit, 8bit, None)
            
        Returns:
            model, tokenizer, actual_model_name
        """
        # ìš”ì²­ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ìš°ì„ ìˆœìœ„
        if not model_name:
            model_name = cls.MODEL_PRIORITY[0]['name']
        
        # ìš°ì„ ìˆœìœ„ì—ì„œ í•´ë‹¹ ëª¨ë¸ ì°¾ê¸°
        model_info = None
        for info in cls.MODEL_PRIORITY:
            if info['name'] == model_name:
                model_info = info
                break
        
        if not model_info:
            # ìš°ì„ ìˆœìœ„ì— ì—†ëŠ” ëª¨ë¸ì€ ê·¸ëŒ€ë¡œ ì‹œë„
            model_info = {'name': model_name, 'fallback': cls.MODEL_PRIORITY[0]['name']}
        
        # ë¡œë“œ ì‹œë„
        model, tokenizer = cls._try_load(model_info['name'], device, quantization)
        
        if model is not None:
            logger.info(f"âœ… {model_info['name']} ë¡œë“œ ì„±ê³µ!")
            return model, tokenizer, model_info['name']
        
        # ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ëª¨ë¸
        logger.warning(f"âš ï¸ {model_info['name']} ë¡œë“œ ì‹¤íŒ¨. ëŒ€ì²´ ëª¨ë¸ ì‹œë„...")
        
        fallback_name = model_info['fallback']
        model, tokenizer = cls._try_load(fallback_name, device, quantization)
        
        if model is not None:
            logger.info(f"âœ… ëŒ€ì²´ ëª¨ë¸ {fallback_name} ë¡œë“œ ì„±ê³µ!")
            return model, tokenizer, fallback_name
        
        # ëª¨ë“  ëª¨ë¸ ìˆœì°¨ ì‹œë„
        logger.warning("âš ï¸ ëª¨ë“  ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì‹œë„ ì¤‘...")
        
        for info in cls.MODEL_PRIORITY:
            if info['name'] != model_name and info['name'] != fallback_name:
                model, tokenizer = cls._try_load(info['name'], device, quantization)
                if model is not None:
                    logger.info(f"âœ… {info['name']} ë¡œë“œ ì„±ê³µ!")
                    return model, tokenizer, info['name']
        
        raise RuntimeError("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    @classmethod
    def _try_load(cls, model_name, device, quantization):
        """ëª¨ë¸ ë¡œë“œ ì‹œë„"""
        try:
            logger.info(f"ðŸ”„ {model_name} ë¡œë“œ ì‹œë„ ì¤‘...")
            
            # í† í¬ë‚˜ì´ì €
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # ì–‘ìží™” ì„¤ì •
            model_kwargs = {
                'trust_remote_code': True,
                'device_map': device
            }
            
            if quantization == '4bit':
                from transformers import BitsAndBytesConfig
                model_kwargs['quantization_config'] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            elif quantization == '8bit':
                from transformers import BitsAndBytesConfig
                model_kwargs['quantization_config'] = BitsAndBytesConfig(
                    load_in_8bit=True
                )
            else:
                model_kwargs['torch_dtype'] = torch.float16
            
            # ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                **model_kwargs
            )
            
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
    
    @classmethod
    def get_recommended_settings(cls, model_name, gpu_memory_gb):
        """
        GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ê¶Œìž¥ ì„¤ì •
        
        Args:
            model_name: ëª¨ë¸ëª…
            gpu_memory_gb: GPU ë©”ëª¨ë¦¬ (GB)
            
        Returns:
            dict: ê¶Œìž¥ ì„¤ì •
        """
        settings = {
            'batch_size': 1,
            'max_length': 2048,
            'quantization': None,
            'gradient_accumulation': 1
        }
        
        # ëª¨ë¸ í¬ê¸° ì¶”ì •
        if "32B" in model_name:
            model_size = 32
        elif "14B" in model_name:
            model_size = 14
        elif "10.7B" in model_name or "10B" in model_name:
            model_size = 10.7
        elif "8B" in model_name:
            model_size = 8
        elif "7B" in model_name:
            model_size = 7
        elif "6B" in model_name:
            model_size = 6
        else:
            model_size = 7  # ê¸°ë³¸ê°’
        
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ì„¤ì •
        if gpu_memory_gb >= 40:  # A100
            if model_size <= 14:
                settings['batch_size'] = 8
                settings['quantization'] = None
            else:
                settings['batch_size'] = 4
                settings['quantization'] = '8bit'
                
        elif gpu_memory_gb >= 24:  # RTX 4090
            if model_size <= 7:
                settings['batch_size'] = 4
                settings['quantization'] = None
            elif model_size <= 14:
                settings['batch_size'] = 2
                settings['quantization'] = '8bit'
            else:
                settings['batch_size'] = 1
                settings['quantization'] = '4bit'
                
        elif gpu_memory_gb >= 16:  # V100
            if model_size <= 7:
                settings['batch_size'] = 2
                settings['quantization'] = '8bit'
            else:
                settings['batch_size'] = 1
                settings['quantization'] = '4bit'
                
        else:  # ìž‘ì€ GPU
            settings['batch_size'] = 1
            settings['quantization'] = '4bit'
            settings['max_length'] = 1024
        
        # Gradient accumulation ê³„ì‚°
        target_batch = 16
        settings['gradient_accumulation'] = max(1, target_batch // settings['batch_size'])
        
        return settings


def test_fallback():
    """ëŒ€ì²´ ë¡œì§ í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("ðŸ” ëª¨ë¸ ëŒ€ì²´ ë¡œì§ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"\nGPU: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
    else:
        gpu_memory = 0
        print("\nGPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # EXAONE ë¡œë“œ ì‹œë„ (ì‹¤íŒ¨ ì˜ˆìƒ)
    print("\n1ï¸âƒ£ EXAONE ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    try:
        model, tokenizer, actual_name = ModelLoader.load_model(
            "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
            quantization='4bit' if gpu_memory < 24 else None
        )
        print(f"âœ… ìµœì¢… ë¡œë“œëœ ëª¨ë¸: {actual_name}")
        
        # ê¶Œìž¥ ì„¤ì •
        settings = ModelLoader.get_recommended_settings(actual_name, gpu_memory)
        print(f"\nðŸ“Š ê¶Œìž¥ ì„¤ì •:")
        print(f"- ë°°ì¹˜ í¬ê¸°: {settings['batch_size']}")
        print(f"- ìµœëŒ€ ê¸¸ì´: {settings['max_length']}")
        print(f"- ì–‘ìží™”: {settings['quantization'] or 'None'}")
        print(f"- Gradient Accumulation: {settings['gradient_accumulation']}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    test_fallback()