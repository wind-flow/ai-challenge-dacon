#!/usr/bin/env python3
"""
ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ìƒì„±ëœ ë°ì´í„°ì˜ í’ˆì§ˆê³¼ ê·œì¹™ ì¤€ìˆ˜ë¥¼ ê²€ì¦
"""

import sys
import os
import json
import argparse
import logging
from pathlib import Path
from datetime import datetime
import pandas as pd
from typing import Dict, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.data_augmentation import DataQualityChecker


def setup_logging(log_level: str = 'INFO'):
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


def load_data(file_path: str) -> List[Dict]:
    """ë°ì´í„° ë¡œë“œ"""
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    data = []
    
    if file_path.suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    
    elif file_path.suffix == '.jsonl':
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    
    elif file_path.suffix == '.csv':
        df = pd.read_csv(file_path)
        data = df.to_dict('records')
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}")
    
    return data


def check_competition_rules(data: List[Dict]) -> Dict:
    """ëŒ€íšŒ ê·œì¹™ ì¤€ìˆ˜ ì—¬ë¶€ í™•ì¸"""
    violations = {
        'test_data_usage': [],
        'api_usage': [],
        'commercial_license': [],
        'format_issues': [],
        'single_model_violation': []
    }
    
    for i, item in enumerate(data):
        # test.csv ì‚¬ìš© ì—¬ë¶€ ì²´í¬
        if 'source' in item and 'test.csv' in str(item.get('source', '')):
            violations['test_data_usage'].append(i)
        
        # API ì‚¬ìš© ì—¬ë¶€ ì²´í¬ (ê¸ˆì§€ëœ í‚¤ì›Œë“œ)
        forbidden_apis = ['openai', 'gpt', 'claude', 'gemini', 'api_key']
        content = json.dumps(item).lower()
        for api in forbidden_apis:
            if api in content:
                violations['api_usage'].append(i)
                break
        
        # ë¼ì´ì„ ìŠ¤ ì²´í¬
        if 'license' in item:
            license_info = item['license']
            if isinstance(license_info, dict) and license_info.get('commercial_use', False):
                violations['commercial_license'].append(i)
        
        # í˜•ì‹ ì²´í¬
        if 'type' not in item or 'question' not in item:
            violations['format_issues'].append(i)
    
    return violations


def analyze_data_quality(data: List[Dict]) -> Dict:
    """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
    analysis = {
        'total': len(data),
        'types': {},
        'sources': {},
        'avg_question_length': 0,
        'avg_answer_length': 0,
        'missing_fields': {},
        'duplicates': 0
    }
    
    # íƒ€ì…ë³„ ë¶„í¬
    for item in data:
        item_type = item.get('type', 'unknown')
        analysis['types'][item_type] = analysis['types'].get(item_type, 0) + 1
    
    # ì†ŒìŠ¤ë³„ ë¶„í¬
    for item in data:
        source = item.get('source_document', 'unknown')
        analysis['sources'][source] = analysis['sources'].get(source, 0) + 1
    
    # í‰ê·  ê¸¸ì´
    question_lengths = [len(item.get('question', '')) for item in data]
    answer_lengths = [len(str(item.get('answer', ''))) for item in data if 'answer' in item]
    
    if question_lengths:
        analysis['avg_question_length'] = sum(question_lengths) / len(question_lengths)
    
    if answer_lengths:
        analysis['avg_answer_length'] = sum(answer_lengths) / len(answer_lengths)
    
    # í•„ë“œ ëˆ„ë½ ì²´í¬
    required_fields = ['type', 'question', 'answer']
    for field in required_fields:
        missing = sum(1 for item in data if field not in item)
        if missing > 0:
            analysis['missing_fields'][field] = missing
    
    # ì¤‘ë³µ ì²´í¬ (ê°„ë‹¨í•œ ë°©ë²•)
    questions = [item.get('question', '') for item in data]
    analysis['duplicates'] = len(questions) - len(set(questions))
    
    return analysis


def validate_fsku_format(data: List[Dict]) -> Dict:
    """FSKU í˜•ì‹ ê²€ì¦"""
    issues = {
        'invalid_mc_options': [],
        'invalid_answer_index': [],
        'missing_keywords': [],
        'invalid_question_format': []
    }
    
    for i, item in enumerate(data):
        item_type = item.get('type', '')
        
        # ê°ê´€ì‹ ê²€ì¦
        if 'multiple_choice' in item_type:
            # ì„ íƒì§€ ê°œìˆ˜ ì²´í¬
            expected_options = int(item_type.split('_')[-1]) if '_' in item_type else 4
            actual_options = len(item.get('options', []))
            
            if actual_options != expected_options:
                issues['invalid_mc_options'].append({
                    'index': i,
                    'expected': expected_options,
                    'actual': actual_options
                })
            
            # ì •ë‹µ ì¸ë±ìŠ¤ ì²´í¬
            answer_idx = item.get('answer', 0)
            if answer_idx < 1 or answer_idx > actual_options:
                issues['invalid_answer_index'].append({
                    'index': i,
                    'answer_idx': answer_idx,
                    'options_count': actual_options
                })
        
        # ì£¼ê´€ì‹ ê²€ì¦
        elif item_type == 'essay':
            if 'keywords' not in item or not item['keywords']:
                issues['missing_keywords'].append(i)
        
        # ì§ˆë¬¸ í˜•ì‹ ì²´í¬
        question = item.get('question', '')
        if not question or len(question) < 10:
            issues['invalid_question_format'].append(i)
    
    return issues


def generate_report(data: List[Dict], output_file: str = None):
    """ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
    print("\n" + "=" * 60)
    print("ë°ì´í„° ê²€ì¦ ë³´ê³ ì„œ")
    print("=" * 60)
    
    # ëŒ€íšŒ ê·œì¹™ ê²€ì¦
    print("\n[ëŒ€íšŒ ê·œì¹™ ì¤€ìˆ˜ ì—¬ë¶€]")
    violations = check_competition_rules(data)
    
    for rule, indices in violations.items():
        if indices:
            print(f"âš ï¸  {rule}: {len(indices)}ê±´ ìœ„ë°˜")
        else:
            print(f"âœ… {rule}: ìœ„ë°˜ ì—†ìŒ")
    
    # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
    print("\n[ë°ì´í„° í’ˆì§ˆ ë¶„ì„]")
    quality = analyze_data_quality(data)
    
    print(f"ì´ ë°ì´í„° ìˆ˜: {quality['total']}")
    print(f"íƒ€ì…ë³„ ë¶„í¬:")
    for t, count in quality['types'].items():
        print(f"  - {t}: {count}ê°œ ({count/quality['total']*100:.1f}%)")
    
    print(f"í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {quality['avg_question_length']:.1f}ì")
    print(f"í‰ê·  ë‹µë³€ ê¸¸ì´: {quality['avg_answer_length']:.1f}ì")
    print(f"ì¤‘ë³µ ë¬¸ì œ: {quality['duplicates']}ê°œ")
    
    if quality['missing_fields']:
        print("í•„ë“œ ëˆ„ë½:")
        for field, count in quality['missing_fields'].items():
            print(f"  - {field}: {count}ê°œ")
    
    # FSKU í˜•ì‹ ê²€ì¦
    print("\n[FSKU í˜•ì‹ ê²€ì¦]")
    format_issues = validate_fsku_format(data)
    
    for issue_type, issues in format_issues.items():
        if issues:
            print(f"âš ï¸  {issue_type}: {len(issues)}ê±´")
        else:
            print(f"âœ… {issue_type}: ë¬¸ì œ ì—†ìŒ")
    
    # ì¢…í•© í‰ê°€
    print("\n[ì¢…í•© í‰ê°€]")
    
    total_violations = sum(len(v) for v in violations.values())
    total_format_issues = sum(len(v) for v in format_issues.values())
    
    if total_violations == 0 and total_format_issues == 0:
        print("âœ… ëª¨ë“  ê²€ì¦ í†µê³¼!")
    else:
        print(f"âš ï¸  ìˆ˜ì • í•„ìš”: ê·œì¹™ ìœ„ë°˜ {total_violations}ê±´, í˜•ì‹ ë¬¸ì œ {total_format_issues}ê±´")
    
    # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
    if output_file:
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_data': quality['total'],
            'violations': violations,
            'quality_analysis': quality,
            'format_issues': format_issues
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ ë³´ê³ ì„œ ì €ì¥: {output_file}")
    
    print("=" * 60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ìƒì„±ëœ ë°ì´í„° ê²€ì¦')
    
    parser.add_argument(
        'input_file',
        type=str,
        help='ê²€ì¦í•  ë°ì´í„° íŒŒì¼ (JSON/JSONL/CSV)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        help='ê²€ì¦ ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼'
    )
    
    parser.add_argument(
        '--log_level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='ë¡œê·¸ ë ˆë²¨'
    )
    
    parser.add_argument(
        '--check_quality',
        action='store_true',
        help='í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰'
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(args.log_level)
    
    try:
        # ë°ì´í„° ë¡œë“œ
        logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {args.input_file}")
        data = load_data(args.input_file)
        logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ í•­ëª©")
        
        # í’ˆì§ˆ ê²€ì¦
        if args.check_quality:
            logger.info("í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
            checker = DataQualityChecker()
            validated, stats = checker.validate_batch(data)
            
            print(f"\ní’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
            print(f"- ê²€ì‚¬: {stats['total_checked']}ê°œ")
            print(f"- í†µê³¼: {stats['passed']}ê°œ")
            print(f"- ì‹¤íŒ¨: {stats['failed']}ê°œ")
            print(f"- ì¤‘ë³µ ì œê±°: {stats['duplicates_removed']}ê°œ")
        
        # ë³´ê³ ì„œ ìƒì„±
        generate_report(data, args.output)
        
        return 0
        
    except Exception as e:
        logger.error(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())