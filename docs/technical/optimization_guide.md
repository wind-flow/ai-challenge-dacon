# ìµœì í™” ê°€ì´ë“œ

## ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ

### QLoRA ì„¤ì •
```python
from transformers import BitsAndBytesConfig
import torch

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
```

### Gradient Checkpointing
```python
model.gradient_checkpointing_enable()
model.config.use_cache = False  # í•™ìŠµ ì‹œì—ë§Œ
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
```python
def adaptive_batch_size(model, initial_batch=8):
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •"""
    while initial_batch > 1:
        try:
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            test_input = torch.randint(0, 32000, (initial_batch, 512))
            _ = model(test_input)
            torch.cuda.empty_cache()
            return initial_batch
        except torch.cuda.OutOfMemoryError:
            initial_batch //= 2
            torch.cuda.empty_cache()
    return 1
```

## â±ï¸ ì¶”ë¡  ì†ë„ ìµœì í™”

### ì‹œê°„ ê´€ë¦¬ (270ë¶„ ê¸°ì¤€)
- ëª¨ë¸ ë¡œë”©: ~5ë¶„
- ì¶”ë¡ : ~260ë¶„ (ì•½ 30ì´ˆ/ë¬¸í•­)
- í›„ì²˜ë¦¬ ë° ì €ì¥: ~5ë¶„

### vLLM í™œìš©
```python
from vllm import LLM, SamplingParams

# vLLMìœ¼ë¡œ ë¹ ë¥¸ ì¶”ë¡ 
llm = LLM(
    model="LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct",
    tensor_parallel_size=1,
    dtype="half",  # FP16
    max_model_len=2048
)

sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512
)
```

### ë°°ì¹˜ ì¶”ë¡  ìµœì í™”
```python
def batch_inference(model, questions, batch_size=4):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ë¡ í•˜ì—¬ ì†ë„ í–¥ìƒ"""
    results = []
    
    for i in range(0, len(questions), batch_size):
        batch = questions[i:i+batch_size]
        
        # íŒ¨ë”© ì²˜ë¦¬
        inputs = tokenizer(
            batch,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False,  # ê²°ì •ì  ì¶œë ¥
                num_beams=1  # ë¹” ì„œì¹˜ ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
            )
        
        results.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))
        
    return results
```

## ğŸ”„ ìºì‹± ì „ëµ

### KV ìºì‹œ í™œìš©
```python
class CachedInference:
    def __init__(self, model):
        self.model = model
        self.cache = {}
    
    def get_embedding(self, text):
        if text not in self.cache:
            self.cache[text] = self.model.encode(text)
        return self.cache[text]
```

### ìœ ì‚¬ ì§ˆë¬¸ ê·¸ë£¹í™”
```python
def group_similar_questions(questions):
    """ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ê·¸ë£¹í™”í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©"""
    groups = {
        'multiple_choice_4': [],
        'multiple_choice_5': [],
        'subjective': []
    }
    
    for q in questions:
        if "1 " in q and "4 " in q:
            groups['multiple_choice_4'].append(q)
        elif "1 " in q and "5 " in q:
            groups['multiple_choice_5'].append(q)
        else:
            groups['subjective'].append(q)
    
    return groups
```

## ğŸ“Š í”„ë¡œíŒŒì¼ë§ ë° ëª¨ë‹ˆí„°ë§

### ì‹œê°„ ì¸¡ì •
```python
import time
from contextlib import contextmanager

@contextmanager
def timer(name):
    start = time.time()
    yield
    end = time.time()
    print(f"{name}: {end - start:.2f}ì´ˆ")

# ì‚¬ìš© ì˜ˆì‹œ
with timer("ëª¨ë¸ ë¡œë”©"):
    model = load_model()

with timer("515ë¬¸í•­ ì¶”ë¡ "):
    results = batch_inference(model, questions)
```

### GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```python
def print_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
    import torch
    if torch.cuda.is_available():
        print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
```

## ğŸ¯ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¶”ë¡  ì „ í™•ì¸ì‚¬í•­
- [ ] ëª¨ë¸ 4bit ì–‘ìí™” ì ìš©
- [ ] ë°°ì¹˜ í¬ê¸° ìµœì í™” (OOM ë°©ì§€)
- [ ] ë¶ˆí•„ìš”í•œ ë¡œê¹… ì œê±°
- [ ] torch.no_grad() ì ìš©
- [ ] CUDA ìºì‹œ ì •ë¦¬

### ì„±ëŠ¥ ëª©í‘œ
- [ ] ë‹¨ì¼ ë¬¸í•­ ì¶”ë¡ : < 30ì´ˆ
- [ ] ì „ì²´ 515ë¬¸í•­: < 260ë¶„
- [ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: < 22GB
- [ ] ì •í™•ë„ ì†ì‹¤: < 2%

## ğŸ’¡ ì¶”ê°€ ìµœì í™” íŒ

1. **Flash Attention 2 ì‚¬ìš©**
```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation="flash_attention_2"
)
```

2. **Mixed Precision Training**
```python
from torch.cuda.amp import autocast

with autocast():
    outputs = model(**inputs)
```

3. **ì»´íŒŒì¼ ìµœì í™”** (PyTorch 2.0+)
```python
import torch._dynamo
torch._dynamo.config.suppress_errors = True
model = torch.compile(model)
```