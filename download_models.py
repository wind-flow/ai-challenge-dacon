#!/usr/bin/env python3
"""
FSKU í”„ë¡œì íŠ¸ìš© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” ëª¨ë¸ë“¤ì„ ì‚¬ì „ì— ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•
"""

import os
import sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def print_gpu_info():
    """GPU ì •ë³´ ì¶œë ¥"""
    print("\nğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  - PyTorch ë²„ì „: {torch.__version__}")
    print(f"  - CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"  - GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print()

def download_model(model_name: str, use_4bit: bool = False):
    """
    ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    
    Args:
        model_name: ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ì´ë¦„
        use_4bit: 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
    """
    print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}")
    print("  (ì²« ë‹¤ìš´ë¡œë“œëŠ” ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤...)")
    
    try:
        # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        print("  - í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„¤ì •
        if use_4bit and torch.cuda.is_available():
            from transformers import BitsAndBytesConfig
            
            print("  - 4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
        else:
            print("  - ì „ì²´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
        
        # ìºì‹œ ê²½ë¡œ í™•ì¸
        cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        model_dir = list(cache_dir.glob(f"models--{model_name.replace('/', '--')}*"))
        
        if model_dir:
            size_gb = sum(f.stat().st_size for f in model_dir[0].rglob("*") if f.is_file()) / (1024**3)
            print(f"  âœ… ì™„ë£Œ! (í¬ê¸°: {size_gb:.1f}GB)")
        else:
            print(f"  âœ… ì™„ë£Œ!")
            
        return True
        
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {str(e)}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     FSKU ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸        â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # GPU ì •ë³´ í™•ì¸
    print_gpu_info()
    
    # ì¶”ì²œ ëª¨ë¸ ëª©ë¡
    models = [
        ("beomi/SOLAR-10.7B-v1.0", "SOLAR 10.7B (ì¶”ì²œ)", True),
        ("LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct", "LG EXAONE 7.8B", True),
        ("Qwen/Qwen2.5-7B-Instruct", "Qwen 2.5 7B (ë‹¤êµ­ì–´)", True),
        ("beomi/llama-2-ko-7b", "Llama2 í•œêµ­ì–´ 7B", False),
    ]
    
    print("ğŸ“‹ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸:")
    for i, (model_name, desc, _) in enumerate(models, 1):
        print(f"  {i}. {desc}")
        print(f"     ëª¨ë¸ëª…: {model_name}")
    print(f"  {len(models)+1}. ì „ì²´ ë‹¤ìš´ë¡œë“œ")
    print(f"  0. ì·¨ì†Œ")
    
    # ì‚¬ìš©ì ì„ íƒ
    choice = input("\nì„ íƒ (ë²ˆí˜¸ ì…ë ¥): ").strip()
    
    if choice == "0":
        print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    # 4ë¹„íŠ¸ ì–‘ìí™” ì˜µì…˜
    use_4bit = False
    if torch.cuda.is_available():
        use_4bit_input = input("\n4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©? (ë©”ëª¨ë¦¬ ì ˆì•½) [y/N]: ").strip().lower()
        use_4bit = use_4bit_input == 'y'
        if use_4bit:
            print("â¡ï¸ 4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë“œ í™œì„±í™”")
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    if choice == str(len(models)+1):
        # ì „ì²´ ë‹¤ìš´ë¡œë“œ
        print("\nì „ì²´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        success_count = 0
        for model_name, desc, recommended in models:
            if recommended or not use_4bit:  # ì¶”ì²œ ëª¨ë¸ì´ê±°ë‚˜ ì–‘ìí™” ë¯¸ì‚¬ìš©ì‹œ
                if download_model(model_name, use_4bit):
                    success_count += 1
        
        print(f"\nâœ… ì™„ë£Œ! {success_count}/{len(models)}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¨")
    
    elif choice.isdigit() and 1 <= int(choice) <= len(models):
        # ì„ íƒí•œ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
        idx = int(choice) - 1
        model_name, desc, _ = models[idx]
        download_model(model_name, use_4bit)
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    # ì™„ë£Œ ë©”ì‹œì§€
    print("\n" + "="*50)
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. python main.py ì‹¤í–‰")
    print("  2. 'ë°ì´í„° ìƒì„±' ì„ íƒ")
    print("  3. ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ì‚¬ìš©")
    print("="*50)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)