{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š FSKU_2_ëª¨ë¸í•™ìŠµ\n",
    "\n",
    "## ğŸ“‹ ë…¸íŠ¸ë¶ ê°œìš”\n",
    "\n",
    "### ëª©ì \n",
    "ì´ ë…¸íŠ¸ë¶ì€ FSKU ê¸ˆìœµ AI Challengeë¥¼ ìœ„í•œ ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ì…ë‹ˆë‹¤. `FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynb`ì—ì„œ ìƒì„±ëœ ì¦ê°• ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸ˆìœµ ì „ë¬¸ AI ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì…ë ¥ ë°ì´í„°\n",
    "- **ìœ„ì¹˜**: `data/augmented/` í´ë”\n",
    "- **í˜•ì‹**: JSON íŒŒì¼ (questions_*.json)\n",
    "- **ë‚´ìš©**: ì¦ê°•ëœ ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸-ë‹µë³€ ìŒ\n",
    "\n",
    "### ì¶œë ¥ë¬¼\n",
    "- **í•™ìŠµëœ ëª¨ë¸**: `models/fsku_finetuned_model/` (ì¶”ë¡  ë‹¨ê³„ì—ì„œ ì‚¬ìš©)\n",
    "- **ì²´í¬í¬ì¸íŠ¸**: `models/checkpoints/`\n",
    "- **í•™ìŠµ ë©”íŠ¸ë¦­**: `results/training_metrics.json`\n",
    "\n",
    "### í•µì‹¬ ì œì•½ì‚¬í•­\n",
    "- RTX 4090 24GB ë©”ëª¨ë¦¬ ì œí•œ\n",
    "- ë‹¨ì¼ LLMë§Œ ì‚¬ìš© (ì•™ìƒë¸” ë¶ˆê°€)\n",
    "- ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥\n",
    "- 270ë¶„ ë‚´ 515ë¬¸í•­ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì†ë„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸ ë° ìë™ ì„¤ì¹˜\nimport subprocess\nimport sys\nimport importlib.util\n\ndef install_package(package):\n    \"\"\"\n    íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ ì„¤ì¹˜\n    \n    Args:\n        package: ì„¤ì¹˜í•  íŒ¨í‚¤ì§€ ì´ë¦„ (ë²„ì „ í¬í•¨ ê°€ëŠ¥)\n    \"\"\"\n    package_name = package.split('>')[0].split('=')[0].split('[')[0]\n    \n    # ë” íš¨ìœ¨ì ì¸ íŒ¨í‚¤ì§€ í™•ì¸ ë°©ë²•\n    if importlib.util.find_spec(package_name) is None:\n        print(f\"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...\")\n        try:\n            subprocess.check_call(\n                [sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"],\n                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ\n            )\n            print(f\"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ!\")\n        except subprocess.TimeoutExpired:\n            print(f\"âš ï¸ {package} ì„¤ì¹˜ ì‹œê°„ ì´ˆê³¼ - ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”\")\n        except subprocess.CalledProcessError as e:\n            print(f\"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n\n# í•„ìˆ˜ íŒ¨í‚¤ì§€ ëª©ë¡\nrequired_packages = [\n    \"transformers>=4.36.0\",\n    \"peft>=0.7.0\",\n    \"bitsandbytes>=0.41.0\",\n    \"accelerate>=0.25.0\",\n    \"datasets\",\n    \"sentencepiece\",\n    \"protobuf\",\n    \"scipy\",\n    \"scikit-learn\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"psutil\",  # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ìš©\n    \"tensorboard\",  # í•™ìŠµ ëª¨ë‹ˆí„°ë§ìš©\n    \"tqdm>=4.65.0\"  # ì§„í–‰ í‘œì‹œì¤„\n]\n\nprint(\"ğŸ” í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘...\")\nfor package in required_packages:\n    install_package(package)\nprint(\"\\nâœ… ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport os\nimport json\nimport glob\nimport random\nimport warnings\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# ë”¥ëŸ¬ë‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n    logging as transformers_logging\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom datasets import Dataset as HFDataset\n\n# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ê²½ê³  ë©”ì‹œì§€ ì„¤ì •\nwarnings.filterwarnings('ignore')\ntransformers_logging.set_verbosity_error()\n\n# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œê°í™”ìš©)\nimport platform\n\nif platform.system() == 'Darwin':  # macOS\n    plt.rcParams['font.family'] = 'AppleGothic'\nelif platform.system() == 'Windows':\n    plt.rcParams['font.family'] = 'Malgun Gothic'\nelse:  # Linux\n    plt.rcParams['font.family'] = 'NanumGothic'\nplt.rcParams['axes.unicode_minus'] = False\n\n# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§\nimport psutil\nprint(f\"\\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:\")\nprint(f\"  - CPU ì½”ì–´: {psutil.cpu_count(logical=False)}ê°œ (ë…¼ë¦¬: {psutil.cpu_count()}ê°œ)\")\nprint(f\"  - RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\nprint(f\"  - ì‚¬ìš© ê°€ëŠ¥ RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n\nprint(\"\\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\nprint(f\"ğŸ“Š PyTorch ë²„ì „: {torch.__version__}\")\nprint(f\"ğŸ–¥ï¸ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë“œ ê³ ì • (ì¬í˜„ì„±ì„ ìœ„í•´)\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë³´ì¥\n",
    "    \n",
    "    Args:\n",
    "        seed: ëœë¤ ì‹œë“œ ê°’\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"ğŸ² ëœë¤ ì‹œë“œ ê³ ì • ì™„ë£Œ (seed=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ )\ndef load_augmented_data(\n    data_dir: str = \"data/augmented\", \n    max_samples: Optional[int] = None,\n    min_answer_length: int = 10,\n    max_answer_length: int = 4096\n) -> List[Dict]:\n    \"\"\"\n    ì¦ê°•ëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ë°˜í™˜\n    \n    Args:\n        data_dir: ì¦ê°• ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n        max_samples: ìµœëŒ€ ë¡œë“œí•  ìƒ˜í”Œ ìˆ˜ (ë©”ëª¨ë¦¬ ì œí•œì‹œ ì‚¬ìš©)\n        min_answer_length: ìµœì†Œ ë‹µë³€ ê¸¸ì´\n        max_answer_length: ìµœëŒ€ ë‹µë³€ ê¸¸ì´\n        \n    Returns:\n        ëª¨ë“  ì§ˆë¬¸-ë‹µë³€ ìŒì´ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸\n    \"\"\"\n    all_data = []\n    \n    # JSON íŒŒì¼ íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  ì¦ê°• ë°ì´í„° íŒŒì¼ ì°¾ê¸°\n    json_files = sorted(glob.glob(os.path.join(data_dir, \"questions_*.json\")))\n    \n    if not json_files:\n        print(f\"âš ï¸ ê²½ê³ : {data_dir}ì—ì„œ ì¦ê°• ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n        print(\"ğŸ’¡ ë¨¼ì € FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\")\n        return []\n    \n    print(f\"ğŸ“‚ ë°œê²¬ëœ ë°ì´í„° íŒŒì¼ ìˆ˜: {len(json_files)}ê°œ\")\n    \n    # ë°ì´í„° í’ˆì§ˆ í†µê³„\n    total_items = 0\n    valid_items = 0\n    duplicate_items = 0\n    seen_questions = set()\n    quality_stats = {\n        'too_short': 0,\n        'too_long': 0,\n        'invalid_format': 0,\n        'empty_fields': 0\n    }\n    \n    # ê° íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬)\n    failed_files = []\n    with tqdm(total=len(json_files), desc=\"ë°ì´í„° íŒŒì¼ ë¡œë”©\") as pbar:\n        for file_path in json_files:\n            if max_samples and len(all_data) >= max_samples:\n                print(f\"\\nâ„¹ï¸ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜({max_samples})ì— ë„ë‹¬í•˜ì—¬ ë¡œë”© ì¤‘ë‹¨\")\n                break\n                \n            try:\n                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë‹¨ìœ„ ë¡œë”©\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    total_items += len(data)\n                    \n                    # ë°ì´í„° ê²€ì¦ ë° ì¤‘ë³µ ì œê±°\n                    for item in data:\n                        if max_samples and len(all_data) >= max_samples:\n                            break\n                            \n                        # í•„ìˆ˜ í•„ë“œ ê²€ì¦\n                        if not all(k in item for k in ['question', 'answer']):\n                            quality_stats['invalid_format'] += 1\n                            continue\n                            \n                        # ë¹ˆ ë¬¸ìì—´ ë° ê¸¸ì´ ê²€ì¦\n                        question = item['question'].strip()\n                        answer = item['answer'].strip()\n                        \n                        if not question or not answer:\n                            quality_stats['empty_fields'] += 1\n                            continue\n                        \n                        # ë‹µë³€ ê¸¸ì´ ê²€ì¦\n                        if len(answer) < min_answer_length:\n                            quality_stats['too_short'] += 1\n                            continue\n                        \n                        if len(answer) > max_answer_length:\n                            quality_stats['too_long'] += 1\n                            answer = answer[:max_answer_length] + \"...\"\n                            \n                        # ì¤‘ë³µ ê²€ì‚¬ (í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ )\n                        question_hash = hash(question)\n                        if question_hash in seen_questions:\n                            duplicate_items += 1\n                            continue\n                            \n                        seen_questions.add(question_hash)\n                        valid_items += 1\n                        \n                        # ë°ì´í„° ì •ê·œí™”\n                        all_data.append({\n                            'question': question,\n                            'answer': answer,\n                            'question_type': item.get('question_type', 'general'),\n                            'source_file': os.path.basename(file_path),\n                            'answer_length': len(answer)  # í†µê³„ìš©\n                        })\n                        \n            except json.JSONDecodeError as e:\n                print(f\"\\nâŒ JSON íŒŒì‹± ì˜¤ë¥˜: {os.path.basename(file_path)}\")\n                failed_files.append(file_path)\n            except MemoryError:\n                print(f\"\\nâš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±! í˜„ì¬ê¹Œì§€ ë¡œë“œëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n                break\n            except Exception as e:\n                print(f\"\\nâŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(file_path)} - {str(e)}\")\n                failed_files.append(file_path)\n            \n            pbar.update(1)\n    \n    # ë¡œë”© í†µê³„ ì¶œë ¥ (ë” ìƒì„¸í•œ í’ˆì§ˆ í†µê³„)\n    print(f\"\\nğŸ“Š ë°ì´í„° ë¡œë”© í†µê³„:\")\n    print(f\"  - ì „ì²´ í•­ëª©: {total_items:,}ê°œ\")\n    print(f\"  - ìœ íš¨ í•­ëª©: {valid_items:,}ê°œ ({valid_items/max(total_items,1)*100:.1f}%)\")\n    print(f\"  - ì¤‘ë³µ ì œê±°: {duplicate_items:,}ê°œ\")\n    print(f\"  - í’ˆì§ˆ í•„í„°ë§:\")\n    print(f\"    â€¢ ë„ˆë¬´ ì§§ìŒ (<{min_answer_length}ì): {quality_stats['too_short']:,}ê°œ\")\n    print(f\"    â€¢ ë„ˆë¬´ ê¹€ (>{max_answer_length}ì): {quality_stats['too_long']:,}ê°œ\")\n    print(f\"    â€¢ í˜•ì‹ ì˜¤ë¥˜: {quality_stats['invalid_format']:,}ê°œ\")\n    print(f\"    â€¢ ë¹ˆ í•„ë“œ: {quality_stats['empty_fields']:,}ê°œ\")\n    print(f\"  - ìµœì¢… ë¡œë“œ: {len(all_data):,}ê°œ\")\n    \n    if failed_files:\n        print(f\"  - ì‹¤íŒ¨ íŒŒì¼: {len(failed_files)}ê°œ\")\n    \n    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\n    estimated_memory = sys.getsizeof(all_data) / (1024**2)\n    print(f\"  - ì˜ˆìƒ ë©”ëª¨ë¦¬: {estimated_memory:.1f} MB\")\n    \n    # ë°ì´í„° í’ˆì§ˆ ê²½ê³ \n    if len(all_data) < 1000:\n        print(\"\\nâš ï¸ ê²½ê³ : í•™ìŠµ ë°ì´í„°ê°€ 1,000ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„° ìƒì„±ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n    \n    return all_data\n\n# ë°ì´í„° ë¡œë“œ\nraw_data = load_augmented_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° í†µê³„ ë¶„ì„\ndef analyze_data(data: List[Dict]) -> None:\n    \"\"\"\n    ë¡œë“œëœ ë°ì´í„°ì˜ í†µê³„ ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  ì¶œë ¥\n    \n    Args:\n        data: ë¶„ì„í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n    \"\"\"\n    if not data:\n        print(\"âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n        return\n    \n    # ê¸°ë³¸ í†µê³„\n    print(\"ğŸ“Š ë°ì´í„° í†µê³„ ë¶„ì„\")\n    print(\"=\" * 50)\n    print(f\"ì´ ë°ì´í„° ìˆ˜: {len(data):,}ê°œ\")\n    \n    # ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬\n    question_types = {}\n    answer_lengths = []\n    \n    for item in data:\n        # ì§ˆë¬¸ ìœ í˜• ì¹´ìš´íŠ¸\n        q_type = item.get('question_type', 'unknown')\n        question_types[q_type] = question_types.get(q_type, 0) + 1\n        \n        # ë‹µë³€ ê¸¸ì´ ìˆ˜ì§‘\n        answer = item.get('answer', '')\n        answer_lengths.append(len(answer))\n    \n    # ì§ˆë¬¸ ìœ í˜• ì¶œë ¥\n    print(\"\\nğŸ“ ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:\")\n    for q_type, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True):\n        percentage = (count / len(data)) * 100\n        print(f\"  - {q_type}: {count:,}ê°œ ({percentage:.1f}%)\")\n    \n    # ë‹µë³€ ê¸¸ì´ í†µê³„\n    if answer_lengths:\n        print(f\"\\nğŸ“ ë‹µë³€ ê¸¸ì´ í†µê³„:\")\n        print(f\"  - í‰ê· : {np.mean(answer_lengths):.0f}ì\")\n        print(f\"  - ì¤‘ê°„ê°’: {np.median(answer_lengths):.0f}ì\")\n        print(f\"  - ìµœì†Œ: {np.min(answer_lengths)}ì\")\n        print(f\"  - ìµœëŒ€: {np.max(answer_lengths)}ì\")\n        print(f\"  - í‘œì¤€í¸ì°¨: {np.std(answer_lengths):.0f}ì\")\n        \n        # ì´ìƒì¹˜ ê²€ì¶œ\n        q1 = np.percentile(answer_lengths, 25)\n        q3 = np.percentile(answer_lengths, 75)\n        iqr = q3 - q1\n        outliers = sum(1 for l in answer_lengths if l < q1 - 1.5*iqr or l > q3 + 1.5*iqr)\n        if outliers > 0:\n            print(f\"  - ì´ìƒì¹˜: {outliers}ê°œ ({outliers/len(answer_lengths)*100:.1f}%)\")\n    \n    # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥\n    print(\"\\nğŸ” ìƒ˜í”Œ ë°ì´í„° (ì²« 3ê°œ):\")\n    for i, item in enumerate(data[:3]):\n        print(f\"\\n[ìƒ˜í”Œ {i+1}]\")\n        print(f\"ì§ˆë¬¸: {item.get('question', '')[:100]}...\")\n        print(f\"ë‹µë³€: {item.get('answer', '')[:100]}...\")\n        print(f\"ìœ í˜•: {item.get('question_type', 'unknown')}\")\n\n# ë°ì´í„° ë¶„ì„ ì‹¤í–‰\nif raw_data:\n    analyze_data(raw_data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  (ì¸µí™” ìƒ˜í”Œë§ ì§€ì›)\ndef split_data(\n    data: List[Dict], \n    test_size: float = 0.2, \n    seed: int = 42,\n    stratify_by: Optional[str] = 'question_type'\n) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í•  (ì¸µí™” ìƒ˜í”Œë§ ì§€ì›)\n    \n    Args:\n        data: ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n        test_size: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2)\n        seed: ëœë¤ ì‹œë“œ\n        stratify_by: ì¸µí™” ìƒ˜í”Œë§ ê¸°ì¤€ í•„ë“œ (Noneì´ë©´ ë‹¨ìˆœ ëœë¤ ë¶„í• )\n        \n    Returns:\n        (í•™ìŠµ ë°ì´í„°, ê²€ì¦ ë°ì´í„°) íŠœí”Œ\n    \"\"\"\n    if not data:\n        return [], []\n    \n    random.seed(seed)\n    \n    # ì¸µí™” ìƒ˜í”Œë§ ì‹œë„\n    if stratify_by and stratify_by in data[0]:\n        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”\n        grouped_data = {}\n        for item in data:\n            key = item.get(stratify_by, 'unknown')\n            if key not in grouped_data:\n                grouped_data[key] = []\n            grouped_data[key].append(item)\n        \n        train_data = []\n        val_data = []\n        \n        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¹„ìœ¨ì— ë§ê²Œ ë¶„í• \n        for category, items in grouped_data.items():\n            random.shuffle(items)\n            split_idx = int(len(items) * (1 - test_size))\n            train_data.extend(items[:split_idx])\n            val_data.extend(items[split_idx:])\n        \n        # ìµœì¢… ì…”í”Œ\n        random.shuffle(train_data)\n        random.shuffle(val_data)\n        \n        print(f\"âœ‚ï¸ ì¸µí™” ìƒ˜í”Œë§ ë¶„í•  ì™„ë£Œ! (ê¸°ì¤€: {stratify_by})\")\n    else:\n        # ë‹¨ìˆœ ëœë¤ ë¶„í• \n        shuffled_data = data.copy()\n        random.shuffle(shuffled_data)\n        \n        # ë¶„í•  ì§€ì  ê³„ì‚°\n        split_idx = int(len(shuffled_data) * (1 - test_size))\n        \n        # ë°ì´í„° ë¶„í• \n        train_data = shuffled_data[:split_idx]\n        val_data = shuffled_data[split_idx:]\n        \n        print(f\"âœ‚ï¸ ëœë¤ ë¶„í•  ì™„ë£Œ!\")\n    \n    print(f\"  - í•™ìŠµ ë°ì´í„°: {len(train_data):,}ê°œ ({len(train_data)/len(data)*100:.1f}%)\")\n    print(f\"  - ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ ({len(val_data)/len(data)*100:.1f}%)\")\n    \n    # ì¹´í…Œê³ ë¦¬ ë¶„í¬ í™•ì¸\n    if stratify_by:\n        train_categories = {}\n        val_categories = {}\n        \n        for item in train_data:\n            cat = item.get(stratify_by, 'unknown')\n            train_categories[cat] = train_categories.get(cat, 0) + 1\n            \n        for item in val_data:\n            cat = item.get(stratify_by, 'unknown')\n            val_categories[cat] = val_categories.get(cat, 0) + 1\n        \n        print(f\"\\nğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n        all_categories = set(train_categories.keys()) | set(val_categories.keys())\n        for cat in sorted(all_categories):\n            train_cnt = train_categories.get(cat, 0)\n            val_cnt = val_categories.get(cat, 0)\n            total_cnt = train_cnt + val_cnt\n            if total_cnt > 0:\n                print(f\"  - {cat}: í•™ìŠµ {train_cnt}ê°œ ({train_cnt/total_cnt*100:.1f}%), ê²€ì¦ {val_cnt}ê°œ ({val_cnt/total_cnt*100:.1f}%)\")\n    \n    return train_data, val_data\n\n# ë°ì´í„° ë¶„í• \nif raw_data:\n    train_data, val_data = split_data(raw_data, stratify_by='question_type')\nelse:\n    train_data, val_data = [], []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ì„ íƒ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡\n",
    "AVAILABLE_MODELS = {\n",
    "    \"exaone\": {\n",
    "        \"name\": \"LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "        \"description\": \"LG AI Researchì˜ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (ì¶”ì²œ)\",\n",
    "        \"size\": \"7.8B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"solar\": {\n",
    "        \"name\": \"upstage/SOLAR-10.7B-v1.0\",\n",
    "        \"description\": \"Upstageì˜ í•œêµ­ì–´ ê°•í™” ëª¨ë¸\",\n",
    "        \"size\": \"10.7B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"qwen\": {\n",
    "        \"name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"description\": \"ë‹¤êµ­ì–´ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ëª¨ë¸\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": False\n",
    "    },\n",
    "    \"llama-ko\": {\n",
    "        \"name\": \"beomi/llama-2-ko-7b\",\n",
    "        \"description\": \"í•œêµ­ì–´ë¡œ íŒŒì¸íŠœë‹ëœ Llama ëª¨ë¸\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:\")\n",
    "print(\"=\" * 60)\n",
    "for key, info in AVAILABLE_MODELS.items():\n",
    "    print(f\"\\n[{key}] {info['name']}\")\n",
    "    print(f\"  - ì„¤ëª…: {info['description']}\")\n",
    "    print(f\"  - í¬ê¸°: {info['size']}\")\n",
    "    print(f\"  - í•œêµ­ì–´ íŠ¹í™”: {'âœ…' if info['korean_specialized'] else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ ì„ íƒ (í™˜ê²½ë³€ìˆ˜ë¡œë„ ì„¤ì • ê°€ëŠ¥)\nimport os\n\n# ì˜µì…˜: \"exaone\", \"solar\", \"qwen\", \"llama-ko\"\nSELECTED_MODEL = os.getenv('FSKU_MODEL', 'exaone')  # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’\n\n# ëª¨ë¸ ì„ íƒ ê²€ì¦\nif SELECTED_MODEL not in AVAILABLE_MODELS:\n    print(f\"âš ï¸ ì˜ëª»ëœ ëª¨ë¸ ì„ íƒ: {SELECTED_MODEL}\")\n    print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(AVAILABLE_MODELS.keys())}\")\n    SELECTED_MODEL = \"exaone\"  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›\n    print(f\"ê¸°ë³¸ ëª¨ë¸ë¡œ ë³€ê²½: {SELECTED_MODEL}\")\n\n# ì„ íƒëœ ëª¨ë¸ ì •ë³´\nmodel_info = AVAILABLE_MODELS[SELECTED_MODEL]\nMODEL_NAME = model_info[\"name\"]\n\nprint(f\"\\nâœ… ì„ íƒëœ ëª¨ë¸: {MODEL_NAME}\")\nprint(f\"   {model_info['description']}\")\n\n# ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì •\nif SELECTED_MODEL == \"exaone\":\n    # EXAONE ëª¨ë¸ì€ íŠ¹ë³„í•œ í† í° ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ\n    print(\"\\nğŸ’¡ EXAONE ëª¨ë¸ íŠ¹ìˆ˜ ì„¤ì • ì ìš©\")\n    USE_SPECIAL_TOKENS = True\nelif SELECTED_MODEL == \"solar\":\n    # SOLAR ëª¨ë¸ì€ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ì— ê°•í•¨\n    print(\"\\nğŸ’¡ SOLAR ëª¨ë¸ íŠ¹ìˆ˜ ì„¤ì • ì ìš©\")\n    USE_SPECIAL_TOKENS = False\nelse:\n    USE_SPECIAL_TOKENS = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QLoRA ì„¤ì • ë° ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# QLoRAë¥¼ ìœ„í•œ 4bit ì–‘ìí™” ì„¤ì •\ndef get_quantization_config():\n    \"\"\"\n    RTX 4090 24GBì— ìµœì í™”ëœ 4bit ì–‘ìí™” ì„¤ì • ë°˜í™˜\n    \n    Returns:\n        BitsAndBytesConfig ê°ì²´\n    \"\"\"\n    return BitsAndBytesConfig(\n        load_in_4bit=True,  # 4bit ì–‘ìí™” ì‚¬ìš©\n        bnb_4bit_compute_dtype=torch.float16,  # ê³„ì‚°ì€ FP16ìœ¼ë¡œ\n        bnb_4bit_use_double_quant=True,  # ì´ì¤‘ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½\n        bnb_4bit_quant_type=\"nf4\"  # NormalFloat4 ì–‘ìí™” (ë” ë‚˜ì€ ì„±ëŠ¥)\n    )\n\n# LoRA ì„¤ì •\ndef get_lora_config():\n    \"\"\"\n    LoRA (Low-Rank Adaptation) ì„¤ì • ë°˜í™˜\n    \n    Returns:\n        LoraConfig ê°ì²´\n    \"\"\"\n    # ëª¨ë¸ë³„ íƒ€ê²Ÿ ëª¨ë“ˆ ì„¤ì • (ë” ì„¸ë°€í•œ ì„¤ì •)\n    if \"qwen\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"solar\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"llama\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"exaone\" in MODEL_NAME.lower():\n        # EXAONE ëª¨ë¸ì€ íŠ¹ë³„í•œ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    else:\n        # ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì—ì„œ ì‘ë™í•˜ëŠ” ê¸°ë³¸ ì„¤ì •\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    \n    return LoraConfig(\n        r=16,  # LoRA rank (8~32 ë²”ìœ„, ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€)\n        lora_alpha=32,  # LoRA scaling parameter (ì¼ë°˜ì ìœ¼ë¡œ r*2)\n        target_modules=target_modules,  # ì ìš©í•  ëª¨ë“ˆ\n        lora_dropout=0.1,  # Dropout ë¹„ìœ¨\n        bias=\"none\",  # Bias í•™ìŠµ ì—¬ë¶€\n        task_type=TaskType.CAUSAL_LM,  # ì–¸ì–´ ëª¨ë¸ë§ íƒœìŠ¤í¬\n    )\n\n# ì„¤ì • ìƒì„±\nquantization_config = get_quantization_config()\nlora_config = get_lora_config()\n\nprint(\"âš™ï¸ QLoRA ì„¤ì • ì™„ë£Œ!\")\nprint(f\"  - ì–‘ìí™”: 4bit (NF4)\")\nprint(f\"  - LoRA rank: {lora_config.r}\")\nprint(f\"  - LoRA alpha: {lora_config.lora_alpha}\")\nprint(f\"  - íƒ€ê²Ÿ ëª¨ë“ˆ: {lora_config.target_modules}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©\nprint(f\"\\nğŸš€ ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_NAME}\")\nprint(\"â³ ì²« ì‹¤í–‰ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (10-20GB)...\")\n\ntry:\n    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,  # ì¼ë¶€ ëª¨ë¸ì€ ì»¤ìŠ¤í…€ ì½”ë“œ í•„ìš”\n        use_fast=True  # Fast tokenizer ì‚¬ìš© (ë” ë¹ ë¦„)\n    )\n    \n    # íŒ¨ë”© í† í° ì„¤ì • (ì—†ëŠ” ê²½ìš°)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"â„¹ï¸ íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\")\n    \n    # GPU ë©”ëª¨ë¦¬ í™•ì¸\n    if torch.cuda.is_available():\n        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n        print(f\"\\nğŸ’¾ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬: {free_memory / 1024**3:.1f} GB\")\n        \n        # ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ê²½ê³ \n        if free_memory < 10 * 1024**3:  # 10GB ë¯¸ë§Œ\n            print(\"âš ï¸ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. batch_sizeë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n    \n    # ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™” ì ìš©)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=quantization_config,\n        device_map=\"auto\",  # GPUì— ìë™ ë°°ì¹˜\n        trust_remote_code=True,\n        torch_dtype=torch.float16,  # FP16 ì‚¬ìš©\n        low_cpu_mem_usage=True  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ\n    )\n    \n    # í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA ì ìš©\n    model = get_peft_model(model, lora_config)\n    \n    # ëª¨ë¸ ì •ë³´ ì¶œë ¥\n    print(\"\\nâœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n    print(f\"ğŸ“Š í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°:\")\n    model.print_trainable_parameters()\n    \nexcept Exception as e:\n    print(f\"\\nâŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}\")\n    print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n    print(\"  1. ì¸í„°ë„· ì—°ê²° í™•ì¸\")\n    print(\"  2. Hugging Face í† í° ì„¤ì • í™•ì¸\")\n    print(\"  3. GPU ë©”ëª¨ë¦¬ í™•ì¸\")\n    raise e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„°ì…‹ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "def format_prompt(question: str, answer: str, is_training: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ëª¨ë¸ í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ë¡œ í¬ë§·íŒ…\n",
    "    \n",
    "    Args:\n",
    "        question: ì§ˆë¬¸ í…ìŠ¤íŠ¸\n",
    "        answer: ë‹µë³€ í…ìŠ¤íŠ¸\n",
    "        is_training: í•™ìŠµìš©ì¸ì§€ ì—¬ë¶€ (í•™ìŠµì‹œì—ëŠ” ë‹µë³€ í¬í•¨)\n",
    "        \n",
    "    Returns:\n",
    "        í¬ë§·ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "    if \"exaone\" in MODEL_NAME.lower():\n",
    "        # EXAONE ëª¨ë¸ìš© í…œí”Œë¦¿\n",
    "        if is_training:\n",
    "            prompt = f\"[|ì‹œìŠ¤í…œ|]ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.[|ì¢…ë£Œ|]\\n[|ì‚¬ìš©ì|]{question}[|ì¢…ë£Œ|]\\n[|AI|]{answer}[|ì¢…ë£Œ|]\"\n",
    "        else:\n",
    "            prompt = f\"[|ì‹œìŠ¤í…œ|]ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.[|ì¢…ë£Œ|]\\n[|ì‚¬ìš©ì|]{question}[|ì¢…ë£Œ|]\\n[|AI|]\"\n",
    "    elif \"solar\" in MODEL_NAME.lower():\n",
    "        # SOLAR ëª¨ë¸ìš© í…œí”Œë¦¿\n",
    "        system_prompt = \"ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµ ì‹œì¥ì— ì •í†µí•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\"\n",
    "        if is_training:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n{answer}\"\n",
    "        else:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n\"\n",
    "    else:\n",
    "        # ê¸°ë³¸ í…œí”Œë¦¿ (Qwen, Llama ë“±)\n",
    "        if is_training:\n",
    "            prompt = f\"ì§ˆë¬¸: {question}\\n\\në‹µë³€: {answer}\"\n",
    "        else:\n",
    "            prompt = f\"ì§ˆë¬¸: {question}\\n\\në‹µë³€: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ í™•ì¸\n",
    "if train_data:\n",
    "    sample = train_data[0]\n",
    "    sample_prompt = format_prompt(sample['question'], sample['answer'])\n",
    "    print(\"ğŸ“ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜ˆì‹œ:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(sample_prompt[:500] + \"...\" if len(sample_prompt) > 500 else sample_prompt)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í† í°í™” í•¨ìˆ˜\ndef tokenize_function(examples: Dict[str, List]) -> Dict[str, List]:\n    \"\"\"\n    ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ í† í°í™”\n    \n    Args:\n        examples: ë°°ì¹˜ ë°ì´í„° (questions, answers í¬í•¨)\n        \n    Returns:\n        í† í°í™”ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬\n    \"\"\"\n    # í”„ë¡¬í”„íŠ¸ ìƒì„± (ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)\n    prompts = []\n    for question, answer in zip(examples['question'], examples['answer']):\n        # None ê°’ ì²´í¬\n        if question is None or answer is None:\n            continue\n        # ë¬¸ìì—´ ë³€í™˜ ë° ê³µë°± ì œê±°\n        question = str(question).strip()\n        answer = str(answer).strip()\n        if question and answer:\n            prompt = format_prompt(question, answer, is_training=True)\n            prompts.append(prompt)\n    \n    # ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬\n    if not prompts:\n        raise ValueError(\"ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n    \n    # í† í°í™”\n    model_inputs = tokenizer(\n        prompts,\n        max_length=2048,  # ìµœëŒ€ í† í° ê¸¸ì´\n        padding=\"max_length\",  # ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©\n        truncation=True,  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸°\n        return_tensors=\"pt\"\n    )\n    \n    # ë ˆì´ë¸” ì„¤ì • (input_idsì™€ ë™ì¼í•˜ê²Œ, íŒ¨ë”© ë¶€ë¶„ì€ -100ìœ¼ë¡œ)\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n    \n    # íŒ¨ë”© í† í°ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸ (-100ìœ¼ë¡œ ì„¤ì •)\n    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n    \n    return model_inputs\n\n# ë°ì´í„°ë¥¼ HuggingFace Datasetìœ¼ë¡œ ë³€í™˜\ndef prepare_datasets(train_data: List[Dict], val_data: List[Dict]):\n    \"\"\"\n    í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¥¼ HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  í† í°í™”\n    \n    Args:\n        train_data: í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n        val_data: ê²€ì¦ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n        \n    Returns:\n        (í† í°í™”ëœ í•™ìŠµ ë°ì´í„°ì…‹, í† í°í™”ëœ ê²€ì¦ ë°ì´í„°ì…‹)\n    \"\"\"\n    # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ì‰¬ìš´ ì²˜ë¦¬ë¥¼ ìœ„í•´)\n    train_df = pd.DataFrame(train_data)\n    val_df = pd.DataFrame(val_data)\n    \n    # HuggingFace Datasetìœ¼ë¡œ ë³€í™˜\n    train_dataset = HFDataset.from_pandas(train_df)\n    val_dataset = HFDataset.from_pandas(val_df)\n    \n    # í† í°í™” ì ìš©\n    print(\"ğŸ”„ í•™ìŠµ ë°ì´í„° í† í°í™” ì¤‘...\")\n    tokenized_train = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=train_dataset.column_names\n    )\n    \n    print(\"ğŸ”„ ê²€ì¦ ë°ì´í„° í† í°í™” ì¤‘...\")\n    tokenized_val = val_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=val_dataset.column_names\n    )\n    \n    print(\"\\nâœ… í† í°í™” ì™„ë£Œ!\")\n    print(f\"  - í•™ìŠµ ë°ì´í„°: {len(tokenized_train)}ê°œ\")\n    print(f\"  - ê²€ì¦ ë°ì´í„°: {len(tokenized_val)}ê°œ\")\n    \n    return tokenized_train, tokenized_val\n\n# ë°ì´í„°ì…‹ ì¤€ë¹„\nif train_data and val_data:\n    tokenized_train_dataset, tokenized_val_dataset = prepare_datasets(train_data, val_data)\nelse:\n    print(\"âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n    tokenized_train_dataset, tokenized_val_dataset = None, None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í† í° ê¸¸ì´ ë¶„í¬ ì‹œê°í™”\ndef visualize_token_distribution(dataset, title=\"Token Length Distribution\"):\n    \"\"\"\n    ë°ì´í„°ì…‹ì˜ í† í° ê¸¸ì´ ë¶„í¬ë¥¼ ì‹œê°í™”\n    \n    Args:\n        dataset: í† í°í™”ëœ ë°ì´í„°ì…‹\n        title: ê·¸ë˜í”„ ì œëª©\n    \"\"\"\n    if dataset is None:\n        return\n    \n    # ì‹¤ì œ í† í° ê¸¸ì´ ê³„ì‚° (íŒ¨ë”© ì œì™¸) - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ\n    lengths = []\n    sample_size = min(1000, len(dataset))  # ìµœëŒ€ 1000ê°œ ìƒ˜í”Œë§Œ ë¶„ì„\n    indices = np.random.choice(len(dataset), sample_size, replace=False)\n    \n    for idx in indices:\n        item = dataset[int(idx)]\n        # attention_maskê°€ 1ì¸ ë¶€ë¶„ë§Œ ì‹¤ì œ í† í°\n        actual_length = sum(item['attention_mask'])\n        lengths.append(actual_length)\n    \n    print(f\"\\nğŸ“Š ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ (ì „ì²´ {len(dataset)}ê°œ ì¤‘)\")\n    \n    # í†µê³„ ê³„ì‚°\n    avg_length = np.mean(lengths)\n    median_length = np.median(lengths)\n    max_length = np.max(lengths)\n    \n    # ì‹œê°í™”\n    plt.figure(figsize=(10, 6))\n    plt.hist(lengths, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    plt.axvline(avg_length, color='red', linestyle='--', label=f'Average: {avg_length:.0f}')\n    plt.axvline(median_length, color='green', linestyle='--', label=f'Median: {median_length:.0f}')\n    plt.xlabel('Token Length')\n    plt.ylabel('Frequency')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    print(f\"ğŸ“Š í† í° ê¸¸ì´ í†µê³„:\")\n    print(f\"  - í‰ê· : {avg_length:.0f} í† í°\")\n    print(f\"  - ì¤‘ê°„ê°’: {median_length:.0f} í† í°\")\n    print(f\"  - ìµœëŒ€: {max_length} í† í°\")\n    print(f\"  - 2048 í† í° ì´ˆê³¼: {sum(1 for l in lengths if l >= 2048)}ê°œ ({sum(1 for l in lengths if l >= 2048)/len(lengths)*100:.1f}%)\")\n\n# í•™ìŠµ ë°ì´í„° í† í° ë¶„í¬ í™•ì¸\nif tokenized_train_dataset:\n    visualize_token_distribution(tokenized_train_dataset, \"Training Data Token Distribution\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ê°œì„ ëœ ë™ì  ì„¤ì •)\ndef get_training_args(output_dir: str = \"./models/checkpoints\"):\n    \"\"\"\n    RTX 4090 24GBì— ìµœì í™”ëœ í•™ìŠµ ì„¤ì • ë°˜í™˜\n    \n    Args:\n        output_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬\n        \n    Returns:\n        TrainingArguments ê°ì²´\n    \"\"\"\n    # ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ë” ì •êµí•œ ë©”ëª¨ë¦¬ ê´€ë¦¬)\n    batch_size = 4  # ê¸°ë³¸ê°’\n    gradient_accumulation = 2  # ê¸°ë³¸ê°’\n    \n    if torch.cuda.is_available():\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n        free_memory = total_memory - allocated_memory\n        \n        # ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •\n        if SELECTED_MODEL in AVAILABLE_MODELS:\n            model_size = float(AVAILABLE_MODELS[SELECTED_MODEL]['size'].replace('B', ''))\n        else:\n            model_size = 7.0  # ê¸°ë³¸ê°’\n        \n        # ë©”ëª¨ë¦¬ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ ê³ ë ¤í•œ ë°°ì¹˜ í¬ê¸° ê²°ì •\n        if free_memory < 8 or model_size > 10:\n            batch_size = 1\n            gradient_accumulation = 8\n        elif free_memory < 12:\n            batch_size = 2\n            gradient_accumulation = 4\n        elif free_memory < 16:\n            batch_size = 3\n            gradient_accumulation = 3\n        else:\n            batch_size = 4\n            gradient_accumulation = 2\n            \n        print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {free_memory:.1f}GB ì—¬ìœ \")\n        print(f\"ğŸ¤– ëª¨ë¸ í¬ê¸°: {model_size}B\")\n        print(f\"âš™ï¸ ë°°ì¹˜ í¬ê¸°: {batch_size}, Gradient Accumulation: {gradient_accumulation}\")\n        print(f\"   â†’ ì‹¤íš¨ ë°°ì¹˜ í¬ê¸°: {batch_size * gradient_accumulation}\")\n    \n    # ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ ê³„ì‚°\n    if tokenized_train_dataset:\n        steps_per_epoch = len(tokenized_train_dataset) // (batch_size * gradient_accumulation)\n        num_epochs = 3\n        total_steps = steps_per_epoch * num_epochs\n        \n        # ë°ì´í„°ê°€ ì ì€ ê²½ìš° ì—í­ ìˆ˜ ì¦ê°€\n        if len(tokenized_train_dataset) < 5000:\n            num_epochs = 5\n            total_steps = steps_per_epoch * num_epochs\n            print(f\"â„¹ï¸ ë°ì´í„°ê°€ ì ì–´ ì—í­ì„ {num_epochs}ë¡œ ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤.\")\n    else:\n        total_steps = 1000\n        num_epochs = 3\n    \n    # í•™ìŠµë¥  ìë™ ì¡°ì •\n    base_lr = 2e-4\n    if batch_size * gradient_accumulation < 8:\n        # ì‘ì€ ë°°ì¹˜ í¬ê¸°ì—ëŠ” ë” ë‚®ì€ í•™ìŠµë¥ \n        learning_rate = base_lr * 0.5\n    else:\n        learning_rate = base_lr\n    \n    return TrainingArguments(\n        # ê¸°ë³¸ ì„¤ì •\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        \n        # í•™ìŠµ ì„¤ì •\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        gradient_accumulation_steps=gradient_accumulation,\n        gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½\n        \n        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n        learning_rate=learning_rate,\n        weight_decay=0.01,\n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,  # Gradient clipping\n        optim=\"adamw_torch\",  # ë” íš¨ìœ¨ì ì¸ ì˜µí‹°ë§ˆì´ì €\n        \n        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n        lr_scheduler_type=\"cosine\",\n        warmup_steps=int(total_steps * 0.1),  # 10% warmup\n        warmup_ratio=0.0,  # warmup_steps ì‚¬ìš©ì‹œ 0ìœ¼ë¡œ ì„¤ì •\n        \n        # ë¡œê¹… ë° ì €ì¥\n        logging_steps=max(10, total_steps // 100),  # ìµœì†Œ 10 ìŠ¤í…, ìµœëŒ€ ì „ì²´ì˜ 1%\n        logging_first_step=True,\n        save_strategy=\"steps\",\n        save_steps=max(100, total_steps // 10),  # ìµœì†Œ 100 ìŠ¤í…, ìµœëŒ€ 10ê°œ ì²´í¬í¬ì¸íŠ¸\n        save_total_limit=3,  # ìµœëŒ€ 3ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€\n        save_safetensors=True,  # ë” ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥\n        \n        # í‰ê°€ ì„¤ì •\n        evaluation_strategy=\"steps\",\n        eval_steps=max(100, total_steps // 10),\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        load_best_model_at_end=True,\n        \n        # ì„±ëŠ¥ ìµœì í™”\n        fp16=True,  # Mixed precision training\n        fp16_opt_level=\"O1\",  # ì•ˆì •ì ì¸ mixed precision\n        tf32=True,  # A100/4090ì—ì„œ ì„±ëŠ¥ í–¥ìƒ\n        dataloader_num_workers=min(4, psutil.cpu_count()),\n        dataloader_pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ\n        remove_unused_columns=False,\n        \n        # ì¶”ê°€ ì„¤ì •\n        push_to_hub=False,\n        report_to=[\"tensorboard\"],\n        logging_dir=\"./logs\",\n        seed=42,\n        \n        # ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§\n        debug=\"underflow_overflow\" if torch.cuda.is_available() else \"\",\n        include_inputs_for_metrics=False,\n    )\n\n# í•™ìŠµ ì„¤ì • ìƒì„±\ntraining_args = get_training_args()\n\nprint(\"\\nâš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ!\")\nprint(f\"  - ì—í­ ìˆ˜: {training_args.num_train_epochs}\")\nprint(f\"  - ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size} Ã— {training_args.gradient_accumulation_steps} = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - í•™ìŠµë¥ : {training_args.learning_rate:.2e}\")\nprint(f\"  - Warmup ìŠ¤í…: {training_args.warmup_steps}\")\nprint(f\"  - ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©: {training_args.save_steps} ìŠ¤í…\")\nprint(f\"  - Mixed Precision: FP16={training_args.fp16}, TF32={training_args.tf32}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # 3ë²ˆì˜ í‰ê°€ì—ì„œ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "    early_stopping_threshold=0.001  # ìµœì†Œ ê°œì„  í­\n",
    ")\n",
    "\n",
    "print(\"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"  - Patience: {early_stopping_callback.early_stopping_patience}\")\n",
    "print(f\"  - Threshold: {early_stopping_callback.early_stopping_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì»¤ìŠ¤í…€ Trainer í´ë˜ìŠ¤ (í–¥ìƒëœ ëª¨ë‹ˆí„°ë§)\nclass FSKUTrainer(Trainer):\n    \"\"\"\n    FSKU í”„ë¡œì íŠ¸ìš© ì»¤ìŠ¤í…€ Trainer\n    í–¥ìƒëœ ë©”íŠ¸ë¦­ ì¶”ì ê³¼ ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ëŠ¥ í¬í•¨\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_history = []\n        self.perplexity_history = []\n        self.gpu_memory_history = []\n        self.learning_rate_history = []\n        self.best_perplexity = float('inf')\n        self.steps_since_improvement = 0\n        \n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"\n        ì†ì‹¤ ê³„ì‚° ë° ì¶”ê°€ ë©”íŠ¸ë¦­ ì¶”ì \n        \"\"\"\n        # ê¸°ë³¸ ì†ì‹¤ ê³„ì‚°\n        outputs = model(**inputs)\n        loss = outputs.loss\n        \n        # ì†ì‹¤ ê¸°ë¡\n        if loss is not None:\n            self.loss_history.append(loss.item())\n        \n        # Perplexity ê³„ì‚° ë° ëª¨ë‹ˆí„°ë§\n        if len(self.loss_history) % 50 == 0 and self.loss_history:\n            avg_loss = np.mean(self.loss_history[-50:])\n            perplexity = np.exp(min(avg_loss, 10))  # Overflow ë°©ì§€\n            self.perplexity_history.append(perplexity)\n            \n            # ê°œì„  ì¶”ì \n            if perplexity < self.best_perplexity:\n                self.best_perplexity = perplexity\n                self.steps_since_improvement = 0\n            else:\n                self.steps_since_improvement += 50\n            \n            # GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§\n            if torch.cuda.is_available():\n                gpu_memory = torch.cuda.memory_allocated() / 1024**3\n                gpu_util = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100\n                self.gpu_memory_history.append(gpu_memory)\n                \n                # ìƒíƒœ ì¶œë ¥\n                print(f\"\\nğŸ“Š [ìŠ¤í… {len(self.loss_history)}]\")\n                print(f\"   Perplexity: {perplexity:.2f} (ìµœê³ : {self.best_perplexity:.2f})\")\n                print(f\"   GPU: {gpu_memory:.1f}GB ({gpu_util:.1f}%)\")\n                print(f\"   ê°œì„  ì—†ìŒ: {self.steps_since_improvement} ìŠ¤í…\")\n                \n                # ë©”ëª¨ë¦¬ ê²½ê³ \n                if gpu_util > 90:\n                    print(\"   âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  90% ì´ˆê³¼!\")\n                \n                # í•™ìŠµ ì •ì²´ ê²½ê³ \n                if self.steps_since_improvement > 500:\n                    print(\"   âš ï¸ 500 ìŠ¤í… ì´ìƒ ê°œì„ ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.\")\n        \n        return (loss, outputs) if return_outputs else loss\n    \n    def log(self, logs):\n        \"\"\"í–¥ìƒëœ ë¡œê¹…\"\"\"\n        # GPU ë©”íŠ¸ë¦­ ì¶”ê°€\n        if torch.cuda.is_available():\n            logs[\"gpu_memory_gb\"] = torch.cuda.memory_allocated() / 1024**3\n            logs[\"gpu_utilization\"] = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100\n        \n        # í•™ìŠµë¥  ì¶”ì \n        if hasattr(self, 'lr_scheduler') and self.lr_scheduler is not None:\n            current_lr = self.lr_scheduler.get_last_lr()[0]\n            logs[\"learning_rate\"] = current_lr\n            self.learning_rate_history.append(current_lr)\n        \n        # Perplexity ì¶”ê°€\n        if self.perplexity_history:\n            logs[\"perplexity\"] = self.perplexity_history[-1]\n            \n        super().log(logs)\n    \n    def _save_checkpoint(self, model, trial, metrics=None):\n        \"\"\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì‹œ ì¶”ê°€ ì •ë³´ í¬í•¨\"\"\"\n        # ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n        super()._save_checkpoint(model, trial, metrics)\n        \n        # ì¶”ê°€ ë©”íŠ¸ë¦­ ì €ì¥\n        checkpoint_folder = os.path.join(\n            self.args.output_dir,\n            f\"{self.state.global_step}\"\n        )\n        \n        if os.path.exists(checkpoint_folder):\n            metrics_file = os.path.join(checkpoint_folder, \"training_metrics.json\")\n            additional_metrics = {\n                \"best_perplexity\": float(self.best_perplexity) if self.best_perplexity != float('inf') else None,\n                \"steps_since_improvement\": self.steps_since_improvement,\n                \"avg_gpu_memory\": float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else None,\n                \"current_step\": self.state.global_step,\n                \"timestamp\": datetime.now().isoformat()\n            }\n            \n            with open(metrics_file, 'w') as f:\n                json.dump(additional_metrics, f, indent=2)\n\n# Trainer ì´ˆê¸°í™”\nif tokenized_train_dataset and tokenized_val_dataset:\n    trainer = FSKUTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_dataset,\n        eval_dataset=tokenized_val_dataset,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False,  # Causal LMì´ë¯€ë¡œ MLM ë¹„í™œì„±í™”\n            pad_to_multiple_of=8  # íš¨ìœ¨ì„±ì„ ìœ„í•´ 8ì˜ ë°°ìˆ˜ë¡œ íŒ¨ë”©\n        ),\n        callbacks=[early_stopping_callback],\n    )\n    \n    print(\"\\nâœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ!\")\n    \n    # ì˜ˆìƒ í•™ìŠµ ì‹œê°„ ê³„ì‚°\n    total_steps = len(tokenized_train_dataset) // (trainer.args.per_device_train_batch_size * trainer.args.gradient_accumulation_steps) * trainer.args.num_train_epochs\n    estimated_time = total_steps * 0.5 / 60  # ìŠ¤í…ë‹¹ 0.5ì´ˆ ê°€ì •, ë¶„ ë‹¨ìœ„\n    \n    print(f\"ğŸ“Š í•™ìŠµ ì •ë³´:\")\n    print(f\"  - ì´ í•™ìŠµ ìŠ¤í…: {total_steps:,}\")\n    print(f\"  - ì˜ˆìƒ ì‹œê°„: ì•½ {estimated_time:.0f}ë¶„\")\n    print(f\"  - ì²´í¬í¬ì¸íŠ¸ ìˆ˜: ì•½ {total_steps // trainer.args.save_steps}ê°œ\")\nelse:\n    trainer = None\n    print(\"âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ Trainerë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)\ndef clear_gpu_memory():\n    \"\"\"\n    GPU ë©”ëª¨ë¦¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ OOM ë°©ì§€\n    \"\"\"\n    import gc\n    \n    if torch.cuda.is_available():\n        # ë©”ëª¨ë¦¬ ì‚¬ìš© ì „ ìƒíƒœ\n        before_allocated = torch.cuda.memory_allocated() / 1024**3\n        before_reserved = torch.cuda.memory_reserved() / 1024**3\n        \n        # ëª¨ë“  ìºì‹œëœ ë©”ëª¨ë¦¬ í•´ì œ\n        torch.cuda.synchronize()\n        \n        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\n        gc.collect()\n        \n        # CUDA ìºì‹œ ì •ë¦¬\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n        \n        # ë©”ëª¨ë¦¬ ì‚¬ìš© í›„ ìƒíƒœ\n        after_allocated = torch.cuda.memory_allocated() / 1024**3\n        after_reserved = torch.cuda.memory_reserved() / 1024**3\n        \n        # ì •ë¦¬ ê²°ê³¼ ì¶œë ¥\n        print(f\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")\n        print(f\"   í• ë‹¹ ë©”ëª¨ë¦¬: {before_allocated:.2f} â†’ {after_allocated:.2f} GB\")\n        print(f\"   ì˜ˆì•½ ë©”ëª¨ë¦¬: {before_reserved:.2f} â†’ {after_reserved:.2f} GB\")\n        print(f\"   í•´ì œëœ ë©”ëª¨ë¦¬: {(before_reserved - after_reserved):.2f} GB\")\n        \n        # ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ ê³„ì‚°\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        free_memory = total_memory - after_reserved\n        print(f\"   ì‚¬ìš© ê°€ëŠ¥: {free_memory:.2f} GB / {total_memory:.2f} GB\")\n        \n        # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³  ë° ëŒ€ì‘ ë°©ì•ˆ\n        if free_memory < 5:\n            print(\"\\nâš ï¸ GPU ë©”ëª¨ë¦¬ê°€ 5GB ë¯¸ë§Œì…ë‹ˆë‹¤!\")\n            print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n            print(\"   1. batch_sizeë¥¼ 1ë¡œ ì¤„ì´ê¸°\")\n            print(\"   2. gradient_accumulation_stepsë¥¼ 8-16ìœ¼ë¡œ ì¦ê°€\")\n            print(\"   3. max_lengthë¥¼ 1024ë¡œ ê°ì†Œ\")\n            print(\"   4. gradient_checkpointing=True í™•ì¸\")\n            print(\"   5. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\")\n            \n            # nvidia-smi ì •ë³´ ì œì•ˆ\n            print(\"\\nğŸ’¡ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸: nvidia-smi\")\n            \n        return free_memory\n    else:\n        print(\"â„¹ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n        return 0\n\n# í•™ìŠµ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬\nfree_memory = clear_gpu_memory()\n\n# ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ê²½ê³ \nif free_memory > 0 and free_memory < 3:\n    print(\"\\nâš ï¸ ì‹¬ê°í•œ ë©”ëª¨ë¦¬ ë¶€ì¡±! í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n    print(\"   - ë‹¤ë¥¸ ë…¸íŠ¸ë¶ì´ë‚˜ í”„ë¡œê·¸ë¨ ì¢…ë£Œ\")\n    print(\"   - ì»¤ë„ ì¬ì‹œì‘ ê³ ë ¤\")\n    print(\"   - batch_size=1ë¡œ ì„¤ì •\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (í–¥ìƒëœ ì˜¤ë¥˜ ì²˜ë¦¬)\nif trainer is not None:\n    print(\"\\nğŸš€ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ 1-4ì‹œê°„\")\n    print(\"ğŸ’¡ íŒ: í•™ìŠµ ì¤‘ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ batch_sizeë¥¼ ì¤„ì´ì„¸ìš”.\")\n    print(\"ğŸ“Š TensorBoard ëª¨ë‹ˆí„°ë§: tensorboard --logdir ./logs\\n\")\n    \n    # í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n    start_time = datetime.now()\n    \n    # í•™ìŠµ ì§„í–‰ ìƒíƒœ ì¶”ì \n    training_successful = False\n    error_count = 0\n    max_retries = 2\n    \n    while not training_successful and error_count < max_retries:\n        try:\n            # í•™ìŠµ ì‹¤í–‰\n            train_result = trainer.train()\n            training_successful = True\n            \n            # í•™ìŠµ ì™„ë£Œ\n            end_time = datetime.now()\n            training_time = end_time - start_time\n            \n            print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")\n            print(f\"â±ï¸ ì´ í•™ìŠµ ì‹œê°„: {training_time}\")\n            print(f\"ğŸ“Š ìµœì¢… í•™ìŠµ ì†ì‹¤: {train_result.training_loss:.4f}\")\n            \n            # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥ (ë” ìƒì„¸í•œ ì •ë³´)\n            metrics = {\n                \"training_loss\": float(train_result.training_loss),\n                \"training_time\": str(training_time),\n                \"training_time_seconds\": training_time.total_seconds(),\n                \"model_name\": MODEL_NAME,\n                \"model_type\": SELECTED_MODEL,\n                \"total_steps\": train_result.global_step,\n                \"epochs\": training_args.num_train_epochs,\n                \"train_samples\": len(train_data) if 'train_data' in globals() else 0,\n                \"val_samples\": len(val_data) if 'val_data' in globals() else 0,\n                \"batch_size\": training_args.per_device_train_batch_size,\n                \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n                \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n                \"learning_rate\": float(training_args.learning_rate),\n                \"lora_rank\": lora_config.r,\n                \"timestamp\": datetime.now().isoformat(),\n                \"retry_count\": error_count\n            }\n            \n            # ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ ë©”íŠ¸ë¦­ ì¶”ê°€\n            if hasattr(trainer, 'best_perplexity'):\n                metrics[\"best_perplexity\"] = float(trainer.best_perplexity) if trainer.best_perplexity != float('inf') else None\n            \n            if hasattr(trainer, 'gpu_memory_history') and trainer.gpu_memory_history:\n                metrics[\"avg_gpu_memory_gb\"] = float(np.mean(trainer.gpu_memory_history))\n                metrics[\"max_gpu_memory_gb\"] = float(np.max(trainer.gpu_memory_history))\n            \n            # ìµœì¢… ê²€ì¦ ì†ì‹¤ ì¶”ê°€\n            if hasattr(trainer.state, 'best_metric'):\n                metrics[\"best_eval_loss\"] = float(trainer.state.best_metric)\n            \n            # ë©”íŠ¸ë¦­ ì €ì¥\n            os.makedirs(\"results\", exist_ok=True)\n            with open(\"results/training_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n                json.dump(metrics, f, indent=2, ensure_ascii=False)\n            \n            print(\"\\nğŸ“Š í•™ìŠµ ë©”íŠ¸ë¦­ì´ results/training_metrics.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n            \n            # ì„±ëŠ¥ ìš”ì•½\n            if \"best_perplexity\" in metrics and metrics[\"best_perplexity\"]:\n                print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥:\")\n                print(f\"  - Perplexity: {metrics['best_perplexity']:.2f}\")\n            if \"best_eval_loss\" in metrics:\n                print(f\"  - ê²€ì¦ ì†ì‹¤: {metrics['best_eval_loss']:.4f}\")\n            if \"avg_gpu_memory_gb\" in metrics:\n                print(f\"  - í‰ê·  GPU ì‚¬ìš©: {metrics['avg_gpu_memory_gb']:.1f} GB\")\n                \n        except torch.cuda.OutOfMemoryError as e:\n            error_count += 1\n            print(f\"\\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜! (ì‹œë„ {error_count}/{max_retries})\")\n            \n            # ë©”ëª¨ë¦¬ ì •ë¦¬\n            clear_gpu_memory()\n            \n            if error_count < max_retries:\n                print(\"\\nğŸ’¡ ìë™ ë³µêµ¬ ì‹œë„ ì¤‘...\")\n                print(\"   ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê³  ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.\")\n                \n                # ë°°ì¹˜ í¬ê¸° ê°ì†Œ\n                trainer.args.per_device_train_batch_size = max(1, trainer.args.per_device_train_batch_size // 2)\n                trainer.args.gradient_accumulation_steps = min(16, trainer.args.gradient_accumulation_steps * 2)\n                \n                print(f\"   ìƒˆë¡œìš´ ë°°ì¹˜ í¬ê¸°: {trainer.args.per_device_train_batch_size}\")\n                print(f\"   ìƒˆë¡œìš´ Gradient Accumulation: {trainer.args.gradient_accumulation_steps}\")\n                \n                # ì ì‹œ ëŒ€ê¸°\n                import time\n                time.sleep(5)\n            else:\n                print(\"\\nğŸ’¡ ìˆ˜ë™ í•´ê²° ë°©ë²•:\")\n                print(\"  1. ì»¤ë„ ì¬ì‹œì‘ í›„ batch_size=1ë¡œ ì¬ì‹¤í–‰\")\n                print(\"  2. gradient_accumulation_stepsë¥¼ 8~16ìœ¼ë¡œ ì¦ê°€\")\n                print(\"  3. max_lengthë¥¼ 1024ë¡œ ê°ì†Œ\")\n                print(\"  4. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ê³ ë ¤\")\n                raise e\n                \n        except KeyboardInterrupt:\n            print(\"\\nâš ï¸ ì‚¬ìš©ìê°€ í•™ìŠµì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.\")\n            print(\"ğŸ’¡ í˜„ì¬ê¹Œì§€ì˜ ì²´í¬í¬ì¸íŠ¸ëŠ” ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n            training_successful = True  # ë£¨í”„ ì¢…ë£Œ\n            \n        except Exception as e:\n            error_count += 1\n            print(f\"\\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} (ì‹œë„ {error_count}/{max_retries})\")\n            \n            if error_count < max_retries:\n                print(\"ğŸ’¡ 5ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...\")\n                import time\n                time.sleep(5)\n            else:\n                print(\"\\nğŸ’¡ ì¼ë°˜ì ì¸ í•´ê²° ë°©ë²•:\")\n                print(\"  1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)\")\n                print(\"  2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ì²´í¬í¬ì¸íŠ¸ ì €ì¥)\")\n                print(\"  3. ì—ëŸ¬ ë©”ì‹œì§€ ê²€ìƒ‰\")\n                raise e\nelse:\n    print(\"âš ï¸ Trainerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n    print(\"\\nğŸ” ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n    print(\"   [ ] data/augmented/ í´ë”ì— JSON íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸\")\n    print(\"   [ ] GPUê°€ ì œëŒ€ë¡œ ì¸ì‹ë˜ëŠ”ì§€ í™•ì¸\")\n    print(\"   [ ] í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•™ìŠµ ê³¡ì„  ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë¡œê·¸ ì‹œê°í™”\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ ë³€í™”ë¥¼ ì‹œê°í™”\n",
    "    \n",
    "    Args:\n",
    "        trainer: í•™ìŠµì´ ì™„ë£Œëœ Trainer ê°ì²´\n",
    "    \"\"\"\n",
    "    if trainer is None or not hasattr(trainer.state, 'log_history'):\n",
    "        print(\"âš ï¸ í•™ìŠµ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        return\n",
    "    \n",
    "    # ë¡œê·¸ì—ì„œ ì†ì‹¤ ê°’ ì¶”ì¶œ\n",
    "    log_history = trainer.state.log_history\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in log_history:\n",
    "        if 'loss' in log:\n",
    "            train_loss.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_loss)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_loss.append(log['eval_loss'])\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # í•™ìŠµ ì†ì‹¤\n",
    "    if train_loss:\n",
    "        ax1.plot(steps[:len(train_loss)], train_loss, 'b-', label='Training Loss')\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss Over Time')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # ê²€ì¦ ì†ì‹¤\n",
    "    if eval_loss:\n",
    "        eval_steps = [i * training_args.eval_steps for i in range(1, len(eval_loss) + 1)]\n",
    "        ax2.plot(eval_steps, eval_loss, 'r-', marker='o', label='Validation Loss')\n",
    "        ax2.set_xlabel('Steps')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Validation Loss Over Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìµœì¢… ì†ì‹¤ ì¶œë ¥\n",
    "    if train_loss:\n",
    "        print(f\"\\nğŸ“Š ìµœì¢… í•™ìŠµ ì†ì‹¤: {train_loss[-1]:.4f}\")\n",
    "    if eval_loss:\n",
    "        print(f\"ğŸ“Š ìµœì¢… ê²€ì¦ ì†ì‹¤: {eval_loss[-1]:.4f}\")\n",
    "        print(f\"ğŸ“Š ìµœê³  ê²€ì¦ ì†ì‹¤: {min(eval_loss):.4f}\")\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°\n",
    "if trainer and hasattr(trainer, 'state'):\n",
    "    plot_training_history(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ìµœì¢… ëª¨ë¸ ì €ì¥\ndef save_final_model(trainer, output_dir: str = \"models/fsku_finetuned_model\"):\n    \"\"\"\n    í•™ìŠµëœ ëª¨ë¸ì„ ì¶”ë¡ ìš©ìœ¼ë¡œ ì €ì¥\n    \n    Args:\n        trainer: í•™ìŠµì´ ì™„ë£Œëœ Trainer ê°ì²´\n        output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n    \"\"\"\n    if trainer is None:\n        print(\"âš ï¸ ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n        return\n    \n    print(f\"\\nğŸ’¾ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤: {output_dir}\")\n    \n    # ë””ë ‰í† ë¦¬ ìƒì„±\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (LoRA ì–´ëŒ‘í„°ë§Œ)\n    trainer.save_model(output_dir)\n    \n    # í† í¬ë‚˜ì´ì €ë„ ì €ì¥\n    trainer.tokenizer.save_pretrained(output_dir)\n    \n    # ì„¤ì • ì •ë³´ ì €ì¥ (ë” ìƒì„¸í•œ ì •ë³´)\n    config_info = {\n        \"base_model\": MODEL_NAME,\n        \"model_type\": SELECTED_MODEL,\n        \"training_completed\": datetime.now().isoformat(),\n        \"lora_config\": {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"target_modules\": lora_config.target_modules\n        },\n        \"training_args\": {\n            \"num_train_epochs\": training_args.num_train_epochs,\n            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n            \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n            \"learning_rate\": training_args.learning_rate,\n            \"warmup_steps\": training_args.warmup_steps,\n            \"fp16\": training_args.fp16\n        },\n        \"dataset_info\": {\n            \"train_samples\": len(train_data) if 'train_data' in globals() else 0,\n            \"val_samples\": len(val_data) if 'val_data' in globals() else 0\n        },\n        \"final_loss\": float(trainer.state.log_history[-1].get('loss', 0)) if hasattr(trainer, 'state') and trainer.state.log_history else 'N/A',\n        \"quantization\": \"4bit\",\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    }\n    \n    with open(os.path.join(output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(config_info, f, indent=2, ensure_ascii=False)\n    \n    print(\"\\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n    print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}\")\n    print(\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n    print(\"  - adapter_model.safetensors (LoRA ê°€ì¤‘ì¹˜)\")\n    print(\"  - adapter_config.json (LoRA ì„¤ì •)\")\n    print(\"  - tokenizer íŒŒì¼ë“¤\")\n    print(\"  - training_config.json (í•™ìŠµ ì •ë³´)\")\n    print(\"\\nğŸ’¡ ì¶”ë¡ ì‹œ ì´ ê²½ë¡œë¥¼ FSKU_3_ì¶”ë¡ .ipynbì—ì„œ ì‚¬ìš©í•˜ì„¸ìš”!\")\n    \n    return output_dir\n\n# ëª¨ë¸ ì €ì¥ ì‹¤í–‰\nif trainer:\n    saved_model_path = save_final_model(trainer)\nelse:\n    print(\"âš ï¸ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ì–´ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ í‰ê°€ ë° ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒ˜í”Œ ìƒì„± í…ŒìŠ¤íŠ¸ (í–¥ìƒëœ ìƒì„± íŒŒë¼ë¯¸í„°)\ndef generate_sample(\n    model, \n    tokenizer, \n    prompt: str, \n    max_length: int = 512,\n    temperature: float = 0.8,\n    top_p: float = 0.95,\n    top_k: int = 50\n):\n    \"\"\"\n    í•™ìŠµëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ (ê°œì„ ëœ ìƒì„± íŒŒë¼ë¯¸í„°)\n    \n    Args:\n        model: í•™ìŠµëœ ëª¨ë¸\n        tokenizer: í† í¬ë‚˜ì´ì €\n        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸\n        max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´\n        temperature: ìƒì„± ë‹¤ì–‘ì„± (0.1~1.0)\n        top_p: nucleus sampling íŒŒë¼ë¯¸í„°\n        top_k: top-k sampling íŒŒë¼ë¯¸í„°\n        \n    Returns:\n        ìƒì„±ëœ í…ìŠ¤íŠ¸\n    \"\"\"\n    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ\n    model.eval()\n    \n    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•œ ìºì‹œ ì •ë¦¬\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # í”„ë¡¬í”„íŠ¸ í† í°í™”\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n    \n    # GPUë¡œ ì´ë™\n    if torch.cuda.is_available():\n        inputs = {k: v.cuda() for k, v in inputs.items()}\n    \n    # í…ìŠ¤íŠ¸ ìƒì„± (ë” ë‚˜ì€ ìƒì„± íŒŒë¼ë¯¸í„°)\n    with torch.no_grad():\n        try:\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_length,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                do_sample=True,\n                repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€\n                no_repeat_ngram_size=3,  # 3-gram ë°˜ë³µ ë°©ì§€\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n                early_stopping=True,  # EOS í† í° ë§Œë‚˜ë©´ ì¡°ê¸° ì¢…ë£Œ\n                num_beams=1,  # ë¹” ì„œì¹˜ ë¹„í™œì„±í™” (ë” ë¹ ë¥¸ ìƒì„±)\n            )\n        except torch.cuda.OutOfMemoryError:\n            print(\"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ë” ì§§ì€ ê¸¸ì´ë¡œ ì¬ì‹œë„...\")\n            torch.cuda.empty_cache()\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_length // 2,\n                temperature=temperature,\n                top_p=top_p,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n    \n    # ë””ì½”ë”©\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°\n    if generated_text.startswith(prompt):\n        response = generated_text[len(prompt):].strip()\n    else:\n        # í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°\n        response = generated_text.strip()\n    \n    return response\n\n# ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\nif trainer and val_data:\n    print(\"\\nğŸ§ª í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n    print(\"=\" * 60)\n    \n    # ê²€ì¦ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì„ íƒ\n    num_samples = min(3, len(val_data))\n    test_samples = random.sample(val_data, num_samples)\n    \n    # ë‹¤ì–‘í•œ ì˜¨ë„ë¡œ í…ŒìŠ¤íŠ¸\n    temperatures = [0.7, 0.8, 0.9]\n    \n    for i, sample in enumerate(test_samples):\n        print(f\"\\n[í…ŒìŠ¤íŠ¸ {i+1}]\")\n        print(f\"ì§ˆë¬¸: {sample['question'][:200]}...\")\n        print(f\"\\nì •ë‹µ: {sample['answer'][:300]}...\" if len(sample['answer']) > 300 else f\"\\nì •ë‹µ: {sample['answer']}\")\n        \n        # ì§ˆë¬¸ë§Œìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±\n        test_prompt = format_prompt(sample['question'], \"\", is_training=False)\n        \n        # ë‹¤ì–‘í•œ ì˜¨ë„ë¡œ ìƒì„±\n        temp = temperatures[i % len(temperatures)]\n        print(f\"\\nëª¨ë¸ ìƒì„± (temperature={temp}):\")\n        \n        try:\n            generated = generate_sample(\n                model, \n                tokenizer, \n                test_prompt,\n                temperature=temp,\n                max_length=min(512, len(sample['answer']) + 100)\n            )\n            print(generated[:300] + \"...\" if len(generated) > 300 else generated)\n        except Exception as e:\n            print(f\"ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n        \n        print(\"-\" * 60)\n    \n    # ìƒì„± í’ˆì§ˆ í‰ê°€ íŒ\n    print(\"\\nğŸ’¡ ìƒì„± í’ˆì§ˆ í‰ê°€ ê¸°ì¤€:\")\n    print(\"  1. ë‹µë³€ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±\")\n    print(\"  2. ë¬¸ì¥ì˜ ìœ ì°½ì„±ê³¼ ì¼ê´€ì„±\")\n    print(\"  3. ê¸ˆìœµ ìš©ì–´ì˜ ì ì ˆí•œ ì‚¬ìš©\")\n    print(\"  4. ë‹µë³€ ê¸¸ì´ì˜ ì ì ˆì„±\")\nelse:\n    print(\"âš ï¸ í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì´ë‚˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ìµœì¢… ìš”ì•½ ë° ì²´í¬ë¦¬ìŠ¤íŠ¸\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ğŸ“Š FSKU ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ìš”ì•½\")\nprint(\"=\" * 60)\n\nif trainer:\n    # í•™ìŠµ ì™„ë£Œ ì •ë³´\n    print(f\"\\nâœ… ëª¨ë¸: {MODEL_NAME}\")\n    print(f\"âœ… ëª¨ë¸ íƒ€ì…: {SELECTED_MODEL}\")\n    print(f\"âœ… í•™ìŠµ ë°ì´í„°: {len(train_data):,}ê°œ\")\n    print(f\"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ\")\n    print(f\"âœ… í•™ìŠµ ì—í­: {training_args.num_train_epochs}\")\n    print(f\"âœ… ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size} Ã— {training_args.gradient_accumulation_steps}\")\n    print(f\"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: models/fsku_finetuned_model/\")\n    \n    # ì„±ëŠ¥ ìš”ì•½\n    if hasattr(trainer, 'state') and trainer.state.log_history:\n        final_train_loss = trainer.state.log_history[-1].get('loss', 'N/A')\n        if isinstance(final_train_loss, float):\n            print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:\")\n            print(f\"  - ìµœì¢… í•™ìŠµ ì†ì‹¤: {final_train_loss:.4f}\")\n            print(f\"  - ìµœì¢… Perplexity: {np.exp(min(final_train_loss, 10)):.2f}\")\n            \n        if hasattr(trainer, 'best_perplexity') and trainer.best_perplexity != float('inf'):\n            print(f\"  - ìµœê³  Perplexity: {trainer.best_perplexity:.2f}\")\n    \n    # ìµœì í™” íŒ\n    print(f\"\\nğŸ’¡ ì¶”ë¡  ìµœì í™” íŒ:\")\n    print(f\"  1. ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ\")\n    print(f\"  2. Temperature 0.7-0.8 ì‚¬ìš©\")\n    print(f\"  3. Max tokensë¥¼ ì ì ˆíˆ ì œí•œ\")\n    print(f\"  4. GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§\")\n    \n    # ë‹¤ìŒ ë‹¨ê³„ ì²´í¬ë¦¬ìŠ¤íŠ¸\n    print(f\"\\nğŸ“ ë‹¤ìŒ ë‹¨ê³„ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n    print(f\"  â˜ FSKU_3_ì¶”ë¡ .ipynb íŒŒì¼ ì—´ê¸°\")\n    print(f\"  â˜ ëª¨ë¸ ê²½ë¡œ í™•ì¸: 'models/fsku_finetuned_model'\")\n    print(f\"  â˜ test.csv íŒŒì¼ ì¤€ë¹„\")\n    print(f\"  â˜ ì¶”ë¡  ì‹¤í–‰ (270ë¶„ ë‚´ ì™„ë£Œ ëª©í‘œ)\")\n    print(f\"  â˜ ê²°ê³¼ ê²€ì¦ ë° ì œì¶œ\")\n    \n    # ìœ ìš©í•œ ëª…ë ¹ì–´\n    print(f\"\\nğŸ”§ ìœ ìš©í•œ ëª…ë ¹ì–´:\")\n    print(f\"  â€¢ TensorBoard ì‹¤í–‰: tensorboard --logdir ./logs\")\n    print(f\"  â€¢ ëª¨ë¸ í¬ê¸° í™•ì¸: du -sh models/fsku_finetuned_model/\")\n    print(f\"  â€¢ GPU ëª¨ë‹ˆí„°ë§: watch -n 1 nvidia-smi\")\n    print(f\"  â€¢ ë©”ëª¨ë¦¬ ì •ë¦¬: torch.cuda.empty_cache()\")\n    \n    # íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n    print(f\"\\nğŸš¨ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°:\")\n    print(f\"  â€¢ OOM ì˜¤ë¥˜: batch_size=1, gradient_accumulation_steps=16\")\n    print(f\"  â€¢ ëŠë¦° ì¶”ë¡ : vLLM ë˜ëŠ” TGI ì‚¬ìš© ê³ ë ¤\")\n    print(f\"  â€¢ í’ˆì§ˆ ë¬¸ì œ: ë” ë§ì€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ\")\n    print(f\"  â€¢ í† í° ì´ˆê³¼: max_length ì¡°ì •\")\n    \nelse:\n    print(\"\\nâŒ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n    print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n    print(\"  1. FSKU_1_ë°ì´í„°ì¦ê°•_RAG.ipynb ì‹¤í–‰í•˜ì—¬ ë°ì´í„° ìƒì„±\")\n    print(\"  2. data/augmented/ í´ë” í™•ì¸\")\n    print(\"  3. GPU ë° ë©”ëª¨ë¦¬ í™•ì¸\")\n    print(\"  4. ì´ ë…¸íŠ¸ë¶ ì¬ì‹¤í–‰\")\n    \n    print(\"\\nğŸ” ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n    print(\"  â˜ data/augmented/ í´ë”ì— JSON íŒŒì¼ ì¡´ì¬\")\n    print(\"  â˜ GPU ì¸ì‹ (torch.cuda.is_available())\")\n    print(\"  â˜ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n    print(\"  â˜ ì¸í„°ë„· ì—°ê²° (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ğŸ¯ FSKU ê¸ˆìœµ AI Challenge - ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ ì™„ë£Œ!\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "source": "# í•™ìŠµ ê²°ê³¼ ì¢…í•© ë¶„ì„\ndef analyze_training_results(trainer, train_data, val_data):\n    \"\"\"\n    í•™ìŠµ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê°œì„  ì œì•ˆ\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ğŸ“Š í•™ìŠµ ê²°ê³¼ ì¢…í•© ë¶„ì„\")\n    print(\"=\" * 60)\n    \n    if not trainer or not hasattr(trainer, 'state'):\n        print(\"âŒ ë¶„ì„í•  í•™ìŠµ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n        return\n    \n    # 1. ë°ì´í„° ë¶„ì„\n    print(\"\\n1ï¸âƒ£ ë°ì´í„° í†µê³„\")\n    print(f\"  - í•™ìŠµ ë°ì´í„°: {len(train_data):,}ê°œ\")\n    print(f\"  - ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ\")\n    print(f\"  - í•™ìŠµ/ê²€ì¦ ë¹„ìœ¨: {len(train_data)/(len(train_data)+len(val_data))*100:.1f}%\")\n    \n    # ë°ì´í„° í’ˆì§ˆ í‰ê°€\n    if len(train_data) < 1000:\n        print(\"  âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ê¶Œì¥: 5,000ê°œ ì´ìƒ)\")\n    elif len(train_data) < 5000:\n        print(\"  â„¹ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì ë‹¹í•©ë‹ˆë‹¤ (ê¶Œì¥: 5,000ê°œ ì´ìƒ)\")\n    else:\n        print(\"  âœ… ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„°\")\n    \n    # 2. í•™ìŠµ ê³¼ì • ë¶„ì„\n    print(\"\\n2ï¸âƒ£ í•™ìŠµ ê³¼ì •\")\n    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n        total_steps = trainer.state.global_step\n        print(f\"  - ì´ í•™ìŠµ ìŠ¤í…: {total_steps:,}\")\n        print(f\"  - ì™„ë£Œ ì—í­: {trainer.state.epoch:.1f}/{training_args.num_train_epochs}\")\n        \n        # ì†ì‹¤ ì¶”ì„¸ ë¶„ì„\n        losses = [log.get('loss', 0) for log in trainer.state.log_history if 'loss' in log]\n        if losses:\n            initial_loss = losses[0]\n            final_loss = losses[-1]\n            improvement = (initial_loss - final_loss) / initial_loss * 100\n            \n            print(f\"  - ì´ˆê¸° ì†ì‹¤: {initial_loss:.4f}\")\n            print(f\"  - ìµœì¢… ì†ì‹¤: {final_loss:.4f}\")\n            print(f\"  - ê°œì„ ìœ¨: {improvement:.1f}%\")\n            \n            if improvement < 10:\n                print(\"  âš ï¸ í•™ìŠµ ê°œì„ ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì • í•„ìš”\")\n            elif improvement < 30:\n                print(\"  â„¹ï¸ ì ì ˆí•œ í•™ìŠµ ì§„í–‰\")\n            else:\n                print(\"  âœ… ìš°ìˆ˜í•œ í•™ìŠµ ê°œì„ \")\n    \n    # 3. ì„±ëŠ¥ ì§€í‘œ\n    print(\"\\n3ï¸âƒ£ ì„±ëŠ¥ ì§€í‘œ\")\n    if hasattr(trainer, 'best_perplexity') and trainer.best_perplexity != float('inf'):\n        print(f\"  - ìµœê³  Perplexity: {trainer.best_perplexity:.2f}\")\n        \n        if trainer.best_perplexity < 10:\n            print(\"  âœ… ìš°ìˆ˜í•œ ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥\")\n        elif trainer.best_perplexity < 50:\n            print(\"  â„¹ï¸ ì ì ˆí•œ ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥\")\n        else:\n            print(\"  âš ï¸ ê°œì„ ì´ í•„ìš”í•œ ì„±ëŠ¥\")\n    \n    if hasattr(trainer.state, 'best_metric'):\n        print(f\"  - ìµœê³  ê²€ì¦ ì†ì‹¤: {trainer.state.best_metric:.4f}\")\n    \n    # 4. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©\n    print(\"\\n4ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©\")\n    if hasattr(trainer, 'gpu_memory_history') and trainer.gpu_memory_history:\n        avg_gpu = np.mean(trainer.gpu_memory_history)\n        max_gpu = np.max(trainer.gpu_memory_history)\n        print(f\"  - í‰ê·  GPU ë©”ëª¨ë¦¬: {avg_gpu:.1f} GB\")\n        print(f\"  - ìµœëŒ€ GPU ë©”ëª¨ë¦¬: {max_gpu:.1f} GB\")\n        \n        gpu_util = max_gpu / 24 * 100  # RTX 4090 24GB ê¸°ì¤€\n        if gpu_util > 90:\n            print(\"  âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤\")\n        elif gpu_util > 70:\n            print(\"  â„¹ï¸ GPU ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš© ì¤‘\")\n        else:\n            print(\"  ğŸ’¡ ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ë„ ë©ë‹ˆë‹¤\")\n    \n    # 5. ê°œì„  ì œì•ˆ\n    print(\"\\n5ï¸âƒ£ ê°œì„  ì œì•ˆ\")\n    suggestions = []\n    \n    # ë°ì´í„° ê´€ë ¨\n    if len(train_data) < 5000:\n        suggestions.append(\"â€¢ ë” ë§ì€ í•™ìŠµ ë°ì´í„° ìƒì„± (ëª©í‘œ: 10,000ê°œ)\")\n    \n    # ì„±ëŠ¥ ê´€ë ¨\n    if hasattr(trainer, 'best_perplexity') and trainer.best_perplexity > 30:\n        suggestions.append(\"â€¢ í•™ìŠµë¥  ì¡°ì • ë˜ëŠ” ì—í­ ìˆ˜ ì¦ê°€\")\n        suggestions.append(\"â€¢ LoRA rank ì¦ê°€ (í˜„ì¬: 16 â†’ 32)\")\n    \n    # íš¨ìœ¨ì„± ê´€ë ¨\n    if hasattr(trainer, 'gpu_memory_history') and trainer.gpu_memory_history:\n        if np.max(trainer.gpu_memory_history) < 18:  # 24GBì˜ 75%\n            suggestions.append(\"â€¢ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥\")\n    \n    if suggestions:\n        for suggestion in suggestions:\n            print(suggestion)\n    else:\n        print(\"âœ… í˜„ì¬ ì„¤ì •ì´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n    \n    # 6. ë‹¤ìŒ ë‹¨ê³„\n    print(\"\\n6ï¸âƒ£ ë‹¤ìŒ ë‹¨ê³„\")\n    print(\"  1. ëª¨ë¸ ì €ì¥ í™•ì¸: models/fsku_finetuned_model/\")\n    print(\"  2. ì¶”ë¡  ë…¸íŠ¸ë¶ ì‹¤í–‰: FSKU_3_ì¶”ë¡ .ipynb\")\n    print(\"  3. ì„±ëŠ¥ í‰ê°€ ë° ì œì¶œ\")\n    \n    return {\n        'train_samples': len(train_data),\n        'val_samples': len(val_data),\n        'best_perplexity': getattr(trainer, 'best_perplexity', None),\n        'suggestions': suggestions\n    }\n\n# í•™ìŠµ ê²°ê³¼ ë¶„ì„ ì‹¤í–‰\nif trainer and train_data and val_data:\n    analysis_results = analyze_training_results(trainer, train_data, val_data)\nelse:\n    print(\"âš ï¸ ë¶„ì„í•  í•™ìŠµ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}