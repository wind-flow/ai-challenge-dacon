{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 FSKU_2_모델학습\n",
    "\n",
    "## 📋 노트북 개요\n",
    "\n",
    "### 목적\n",
    "이 노트북은 FSKU 금융 AI Challenge를 위한 모델 학습 단계입니다. `FSKU_1_데이터증강_RAG.ipynb`에서 생성된 증강 데이터를 사용하여 금융 전문 AI 모델을 학습합니다.\n",
    "\n",
    "### 입력 데이터\n",
    "- **위치**: `data/augmented/` 폴더\n",
    "- **형식**: JSON 파일 (questions_*.json)\n",
    "- **내용**: 증강된 금융 관련 질문-답변 쌍\n",
    "\n",
    "### 출력물\n",
    "- **학습된 모델**: `models/fsku_finetuned_model/` (추론 단계에서 사용)\n",
    "- **체크포인트**: `models/checkpoints/`\n",
    "- **학습 메트릭**: `results/training_metrics.json`\n",
    "\n",
    "### 핵심 제약사항\n",
    "- RTX 4090 24GB 메모리 제한\n",
    "- 단일 LLM만 사용 (앙상블 불가)\n",
    "- 오프라인 환경에서 실행 가능\n",
    "- 270분 내 515문항 처리 가능한 속도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 필수 라이브러리 설치 확인 및 자동 설치\nimport subprocess\nimport sys\nimport importlib.util\n\ndef install_package(package):\n    \"\"\"\n    패키지가 설치되어 있지 않으면 자동으로 설치\n    \n    Args:\n        package: 설치할 패키지 이름 (버전 포함 가능)\n    \"\"\"\n    package_name = package.split('>')[0].split('=')[0].split('[')[0]\n    \n    # 더 효율적인 패키지 확인 방법\n    if importlib.util.find_spec(package_name) is None:\n        print(f\"📦 {package} 설치 중...\")\n        try:\n            subprocess.check_call(\n                [sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"],\n                timeout=300  # 5분 타임아웃\n            )\n            print(f\"✅ {package} 설치 완료!\")\n        except subprocess.TimeoutExpired:\n            print(f\"⚠️ {package} 설치 시간 초과 - 수동 설치 필요\")\n        except subprocess.CalledProcessError as e:\n            print(f\"❌ {package} 설치 실패: {e}\")\n\n# 필수 패키지 목록\nrequired_packages = [\n    \"transformers>=4.36.0\",\n    \"peft>=0.7.0\",\n    \"bitsandbytes>=0.41.0\",\n    \"accelerate>=0.25.0\",\n    \"datasets\",\n    \"sentencepiece\",\n    \"protobuf\",\n    \"scipy\",\n    \"scikit-learn\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"psutil\",  # 시스템 모니터링용\n    \"tensorboard\",  # 학습 모니터링용\n    \"tqdm>=4.65.0\"  # 진행 표시줄\n]\n\nprint(\"🔍 필수 패키지 확인 중...\")\nfor package in required_packages:\n    install_package(package)\nprint(\"\\n✅ 모든 필수 패키지가 준비되었습니다!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 기본 라이브러리 임포트\nimport os\nimport json\nimport glob\nimport random\nimport warnings\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# 딥러닝 관련 라이브러리\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Hugging Face 라이브러리\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n    logging as transformers_logging\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom datasets import Dataset as HFDataset\n\n# 시각화 라이브러리\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 경고 메시지 설정\nwarnings.filterwarnings('ignore')\ntransformers_logging.set_verbosity_error()\n\n# 한글 폰트 설정 (시각화용)\nimport platform\n\nif platform.system() == 'Darwin':  # macOS\n    plt.rcParams['font.family'] = 'AppleGothic'\nelif platform.system() == 'Windows':\n    plt.rcParams['font.family'] = 'Malgun Gothic'\nelse:  # Linux\n    plt.rcParams['font.family'] = 'NanumGothic'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 시스템 리소스 모니터링\nimport psutil\nprint(f\"\\n💻 시스템 정보:\")\nprint(f\"  - CPU 코어: {psutil.cpu_count(logical=False)}개 (논리: {psutil.cpu_count()}개)\")\nprint(f\"  - RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\nprint(f\"  - 사용 가능 RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n\nprint(\"\\n✅ 라이브러리 임포트 완료!\")\nprint(f\"📊 PyTorch 버전: {torch.__version__}\")\nprint(f\"🖥️ CUDA 사용 가능: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"💾 GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 (재현성을 위해)\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    모든 랜덤 시드를 고정하여 재현 가능한 결과를 보장\n",
    "    \n",
    "    Args:\n",
    "        seed: 랜덤 시드 값\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"🎲 랜덤 시드 고정 완료 (seed=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 로딩 함수 (메모리 효율성 개선)\ndef load_augmented_data(\n    data_dir: str = \"data/augmented\", \n    max_samples: Optional[int] = None,\n    min_answer_length: int = 10,\n    max_answer_length: int = 4096\n) -> List[Dict]:\n    \"\"\"\n    증강된 데이터를 메모리 효율적으로 로드하여 반환\n    \n    Args:\n        data_dir: 증강 데이터가 저장된 디렉토리 경로\n        max_samples: 최대 로드할 샘플 수 (메모리 제한시 사용)\n        min_answer_length: 최소 답변 길이\n        max_answer_length: 최대 답변 길이\n        \n    Returns:\n        모든 질문-답변 쌍이 담긴 리스트\n    \"\"\"\n    all_data = []\n    \n    # JSON 파일 패턴으로 모든 증강 데이터 파일 찾기\n    json_files = sorted(glob.glob(os.path.join(data_dir, \"questions_*.json\")))\n    \n    if not json_files:\n        print(f\"⚠️ 경고: {data_dir}에서 증강 데이터를 찾을 수 없습니다!\")\n        print(\"💡 먼저 FSKU_1_데이터증강_RAG.ipynb를 실행하여 데이터를 생성하세요.\")\n        return []\n    \n    print(f\"📂 발견된 데이터 파일 수: {len(json_files)}개\")\n    \n    # 데이터 품질 통계\n    total_items = 0\n    valid_items = 0\n    duplicate_items = 0\n    seen_questions = set()\n    quality_stats = {\n        'too_short': 0,\n        'too_long': 0,\n        'invalid_format': 0,\n        'empty_fields': 0\n    }\n    \n    # 각 파일에서 데이터 로드 (메모리 효율적 처리)\n    failed_files = []\n    with tqdm(total=len(json_files), desc=\"데이터 파일 로딩\") as pbar:\n        for file_path in json_files:\n            if max_samples and len(all_data) >= max_samples:\n                print(f\"\\nℹ️ 최대 샘플 수({max_samples})에 도달하여 로딩 중단\")\n                break\n                \n            try:\n                # 메모리 효율적인 청크 단위 로딩\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    total_items += len(data)\n                    \n                    # 데이터 검증 및 중복 제거\n                    for item in data:\n                        if max_samples and len(all_data) >= max_samples:\n                            break\n                            \n                        # 필수 필드 검증\n                        if not all(k in item for k in ['question', 'answer']):\n                            quality_stats['invalid_format'] += 1\n                            continue\n                            \n                        # 빈 문자열 및 길이 검증\n                        question = item['question'].strip()\n                        answer = item['answer'].strip()\n                        \n                        if not question or not answer:\n                            quality_stats['empty_fields'] += 1\n                            continue\n                        \n                        # 답변 길이 검증\n                        if len(answer) < min_answer_length:\n                            quality_stats['too_short'] += 1\n                            continue\n                        \n                        if len(answer) > max_answer_length:\n                            quality_stats['too_long'] += 1\n                            answer = answer[:max_answer_length] + \"...\"\n                            \n                        # 중복 검사 (해시 기반으로 개선)\n                        question_hash = hash(question)\n                        if question_hash in seen_questions:\n                            duplicate_items += 1\n                            continue\n                            \n                        seen_questions.add(question_hash)\n                        valid_items += 1\n                        \n                        # 데이터 정규화\n                        all_data.append({\n                            'question': question,\n                            'answer': answer,\n                            'question_type': item.get('question_type', 'general'),\n                            'source_file': os.path.basename(file_path),\n                            'answer_length': len(answer)  # 통계용\n                        })\n                        \n            except json.JSONDecodeError as e:\n                print(f\"\\n❌ JSON 파싱 오류: {os.path.basename(file_path)}\")\n                failed_files.append(file_path)\n            except MemoryError:\n                print(f\"\\n⚠️ 메모리 부족! 현재까지 로드된 데이터만 사용합니다.\")\n                break\n            except Exception as e:\n                print(f\"\\n❌ 파일 로드 실패: {os.path.basename(file_path)} - {str(e)}\")\n                failed_files.append(file_path)\n            \n            pbar.update(1)\n    \n    # 로딩 통계 출력 (더 상세한 품질 통계)\n    print(f\"\\n📊 데이터 로딩 통계:\")\n    print(f\"  - 전체 항목: {total_items:,}개\")\n    print(f\"  - 유효 항목: {valid_items:,}개 ({valid_items/max(total_items,1)*100:.1f}%)\")\n    print(f\"  - 중복 제거: {duplicate_items:,}개\")\n    print(f\"  - 품질 필터링:\")\n    print(f\"    • 너무 짧음 (<{min_answer_length}자): {quality_stats['too_short']:,}개\")\n    print(f\"    • 너무 김 (>{max_answer_length}자): {quality_stats['too_long']:,}개\")\n    print(f\"    • 형식 오류: {quality_stats['invalid_format']:,}개\")\n    print(f\"    • 빈 필드: {quality_stats['empty_fields']:,}개\")\n    print(f\"  - 최종 로드: {len(all_data):,}개\")\n    \n    if failed_files:\n        print(f\"  - 실패 파일: {len(failed_files)}개\")\n    \n    # 메모리 사용량 추정\n    estimated_memory = sys.getsizeof(all_data) / (1024**2)\n    print(f\"  - 예상 메모리: {estimated_memory:.1f} MB\")\n    \n    # 데이터 품질 경고\n    if len(all_data) < 1000:\n        print(\"\\n⚠️ 경고: 학습 데이터가 1,000개 미만입니다. 더 많은 데이터 생성을 권장합니다.\")\n    \n    return all_data\n\n# 데이터 로드\nraw_data = load_augmented_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 통계 분석\ndef analyze_data(data: List[Dict]) -> None:\n    \"\"\"\n    로드된 데이터의 통계 정보를 분석하고 출력\n    \n    Args:\n        data: 분석할 데이터 리스트\n    \"\"\"\n    if not data:\n        print(\"❌ 분석할 데이터가 없습니다!\")\n        return\n    \n    # 기본 통계\n    print(\"📊 데이터 통계 분석\")\n    print(\"=\" * 50)\n    print(f\"총 데이터 수: {len(data):,}개\")\n    \n    # 질문 유형별 분포\n    question_types = {}\n    answer_lengths = []\n    \n    for item in data:\n        # 질문 유형 카운트\n        q_type = item.get('question_type', 'unknown')\n        question_types[q_type] = question_types.get(q_type, 0) + 1\n        \n        # 답변 길이 수집\n        answer = item.get('answer', '')\n        answer_lengths.append(len(answer))\n    \n    # 질문 유형 출력\n    print(\"\\n📝 질문 유형별 분포:\")\n    for q_type, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True):\n        percentage = (count / len(data)) * 100\n        print(f\"  - {q_type}: {count:,}개 ({percentage:.1f}%)\")\n    \n    # 답변 길이 통계\n    if answer_lengths:\n        print(f\"\\n📏 답변 길이 통계:\")\n        print(f\"  - 평균: {np.mean(answer_lengths):.0f}자\")\n        print(f\"  - 중간값: {np.median(answer_lengths):.0f}자\")\n        print(f\"  - 최소: {np.min(answer_lengths)}자\")\n        print(f\"  - 최대: {np.max(answer_lengths)}자\")\n        print(f\"  - 표준편차: {np.std(answer_lengths):.0f}자\")\n        \n        # 이상치 검출\n        q1 = np.percentile(answer_lengths, 25)\n        q3 = np.percentile(answer_lengths, 75)\n        iqr = q3 - q1\n        outliers = sum(1 for l in answer_lengths if l < q1 - 1.5*iqr or l > q3 + 1.5*iqr)\n        if outliers > 0:\n            print(f\"  - 이상치: {outliers}개 ({outliers/len(answer_lengths)*100:.1f}%)\")\n    \n    # 샘플 데이터 출력\n    print(\"\\n🔍 샘플 데이터 (첫 3개):\")\n    for i, item in enumerate(data[:3]):\n        print(f\"\\n[샘플 {i+1}]\")\n        print(f\"질문: {item.get('question', '')[:100]}...\")\n        print(f\"답변: {item.get('answer', '')[:100]}...\")\n        print(f\"유형: {item.get('question_type', 'unknown')}\")\n\n# 데이터 분석 실행\nif raw_data:\n    analyze_data(raw_data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 학습/검증 데이터 분할 (층화 샘플링 지원)\ndef split_data(\n    data: List[Dict], \n    test_size: float = 0.2, \n    seed: int = 42,\n    stratify_by: Optional[str] = 'question_type'\n) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    데이터를 학습용과 검증용으로 분할 (층화 샘플링 지원)\n    \n    Args:\n        data: 전체 데이터 리스트\n        test_size: 검증 데이터 비율 (기본값: 0.2)\n        seed: 랜덤 시드\n        stratify_by: 층화 샘플링 기준 필드 (None이면 단순 랜덤 분할)\n        \n    Returns:\n        (학습 데이터, 검증 데이터) 튜플\n    \"\"\"\n    if not data:\n        return [], []\n    \n    random.seed(seed)\n    \n    # 층화 샘플링 시도\n    if stratify_by and stratify_by in data[0]:\n        # 카테고리별로 데이터 그룹화\n        grouped_data = {}\n        for item in data:\n            key = item.get(stratify_by, 'unknown')\n            if key not in grouped_data:\n                grouped_data[key] = []\n            grouped_data[key].append(item)\n        \n        train_data = []\n        val_data = []\n        \n        # 각 카테고리별로 비율에 맞게 분할\n        for category, items in grouped_data.items():\n            random.shuffle(items)\n            split_idx = int(len(items) * (1 - test_size))\n            train_data.extend(items[:split_idx])\n            val_data.extend(items[split_idx:])\n        \n        # 최종 셔플\n        random.shuffle(train_data)\n        random.shuffle(val_data)\n        \n        print(f\"✂️ 층화 샘플링 분할 완료! (기준: {stratify_by})\")\n    else:\n        # 단순 랜덤 분할\n        shuffled_data = data.copy()\n        random.shuffle(shuffled_data)\n        \n        # 분할 지점 계산\n        split_idx = int(len(shuffled_data) * (1 - test_size))\n        \n        # 데이터 분할\n        train_data = shuffled_data[:split_idx]\n        val_data = shuffled_data[split_idx:]\n        \n        print(f\"✂️ 랜덤 분할 완료!\")\n    \n    print(f\"  - 학습 데이터: {len(train_data):,}개 ({len(train_data)/len(data)*100:.1f}%)\")\n    print(f\"  - 검증 데이터: {len(val_data):,}개 ({len(val_data)/len(data)*100:.1f}%)\")\n    \n    # 카테고리 분포 확인\n    if stratify_by:\n        train_categories = {}\n        val_categories = {}\n        \n        for item in train_data:\n            cat = item.get(stratify_by, 'unknown')\n            train_categories[cat] = train_categories.get(cat, 0) + 1\n            \n        for item in val_data:\n            cat = item.get(stratify_by, 'unknown')\n            val_categories[cat] = val_categories.get(cat, 0) + 1\n        \n        print(f\"\\n📊 카테고리 분포:\")\n        all_categories = set(train_categories.keys()) | set(val_categories.keys())\n        for cat in sorted(all_categories):\n            train_cnt = train_categories.get(cat, 0)\n            val_cnt = val_categories.get(cat, 0)\n            total_cnt = train_cnt + val_cnt\n            if total_cnt > 0:\n                print(f\"  - {cat}: 학습 {train_cnt}개 ({train_cnt/total_cnt*100:.1f}%), 검증 {val_cnt}개 ({val_cnt/total_cnt*100:.1f}%)\")\n    \n    return train_data, val_data\n\n# 데이터 분할\nif raw_data:\n    train_data, val_data = split_data(raw_data, stratify_by='question_type')\nelse:\n    train_data, val_data = [], []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 선택 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 모델 목록\n",
    "AVAILABLE_MODELS = {\n",
    "    \"exaone\": {\n",
    "        \"name\": \"LG-AI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "        \"description\": \"LG AI Research의 한국어 특화 모델 (추천)\",\n",
    "        \"size\": \"7.8B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"solar\": {\n",
    "        \"name\": \"upstage/SOLAR-10.7B-v1.0\",\n",
    "        \"description\": \"Upstage의 한국어 강화 모델\",\n",
    "        \"size\": \"10.7B\",\n",
    "        \"korean_specialized\": True\n",
    "    },\n",
    "    \"qwen\": {\n",
    "        \"name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"description\": \"다국어 성능이 우수한 모델\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": False\n",
    "    },\n",
    "    \"llama-ko\": {\n",
    "        \"name\": \"beomi/llama-2-ko-7b\",\n",
    "        \"description\": \"한국어로 파인튜닝된 Llama 모델\",\n",
    "        \"size\": \"7B\",\n",
    "        \"korean_specialized\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🤖 사용 가능한 모델 목록:\")\n",
    "print(\"=\" * 60)\n",
    "for key, info in AVAILABLE_MODELS.items():\n",
    "    print(f\"\\n[{key}] {info['name']}\")\n",
    "    print(f\"  - 설명: {info['description']}\")\n",
    "    print(f\"  - 크기: {info['size']}\")\n",
    "    print(f\"  - 한국어 특화: {'✅' if info['korean_specialized'] else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델 선택 (환경변수로도 설정 가능)\nimport os\n\n# 옵션: \"exaone\", \"solar\", \"qwen\", \"llama-ko\"\nSELECTED_MODEL = os.getenv('FSKU_MODEL', 'exaone')  # 환경변수 또는 기본값\n\n# 모델 선택 검증\nif SELECTED_MODEL not in AVAILABLE_MODELS:\n    print(f\"⚠️ 잘못된 모델 선택: {SELECTED_MODEL}\")\n    print(f\"사용 가능한 모델: {list(AVAILABLE_MODELS.keys())}\")\n    SELECTED_MODEL = \"exaone\"  # 기본값으로 복원\n    print(f\"기본 모델로 변경: {SELECTED_MODEL}\")\n\n# 선택된 모델 정보\nmodel_info = AVAILABLE_MODELS[SELECTED_MODEL]\nMODEL_NAME = model_info[\"name\"]\n\nprint(f\"\\n✅ 선택된 모델: {MODEL_NAME}\")\nprint(f\"   {model_info['description']}\")\n\n# 모델별 특수 설정\nif SELECTED_MODEL == \"exaone\":\n    # EXAONE 모델은 특별한 토큰 처리가 필요할 수 있음\n    print(\"\\n💡 EXAONE 모델 특수 설정 적용\")\n    USE_SPECIAL_TOKENS = True\nelif SELECTED_MODEL == \"solar\":\n    # SOLAR 모델은 긴 컨텍스트 처리에 강함\n    print(\"\\n💡 SOLAR 모델 특수 설정 적용\")\n    USE_SPECIAL_TOKENS = False\nelse:\n    USE_SPECIAL_TOKENS = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QLoRA 설정 및 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# QLoRA를 위한 4bit 양자화 설정\ndef get_quantization_config():\n    \"\"\"\n    RTX 4090 24GB에 최적화된 4bit 양자화 설정 반환\n    \n    Returns:\n        BitsAndBytesConfig 객체\n    \"\"\"\n    return BitsAndBytesConfig(\n        load_in_4bit=True,  # 4bit 양자화 사용\n        bnb_4bit_compute_dtype=torch.float16,  # 계산은 FP16으로\n        bnb_4bit_use_double_quant=True,  # 이중 양자화로 메모리 추가 절약\n        bnb_4bit_quant_type=\"nf4\"  # NormalFloat4 양자화 (더 나은 성능)\n    )\n\n# LoRA 설정\ndef get_lora_config():\n    \"\"\"\n    LoRA (Low-Rank Adaptation) 설정 반환\n    \n    Returns:\n        LoraConfig 객체\n    \"\"\"\n    # 모델별 타겟 모듈 설정 (더 세밀한 설정)\n    if \"qwen\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"solar\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"llama\" in MODEL_NAME.lower():\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"exaone\" in MODEL_NAME.lower():\n        # EXAONE 모델은 특별한 구조를 가질 수 있음\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    else:\n        # 대부분의 모델에서 작동하는 기본 설정\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    \n    return LoraConfig(\n        r=16,  # LoRA rank (8~32 범위, 높을수록 표현력 증가)\n        lora_alpha=32,  # LoRA scaling parameter (일반적으로 r*2)\n        target_modules=target_modules,  # 적용할 모듈\n        lora_dropout=0.1,  # Dropout 비율\n        bias=\"none\",  # Bias 학습 여부\n        task_type=TaskType.CAUSAL_LM,  # 언어 모델링 태스크\n    )\n\n# 설정 생성\nquantization_config = get_quantization_config()\nlora_config = get_lora_config()\n\nprint(\"⚙️ QLoRA 설정 완료!\")\nprint(f\"  - 양자화: 4bit (NF4)\")\nprint(f\"  - LoRA rank: {lora_config.r}\")\nprint(f\"  - LoRA alpha: {lora_config.lora_alpha}\")\nprint(f\"  - 타겟 모듈: {lora_config.target_modules}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델과 토크나이저 로딩\nprint(f\"\\n🚀 모델 로딩 시작: {MODEL_NAME}\")\nprint(\"⏳ 첫 실행시 모델 다운로드로 시간이 걸릴 수 있습니다 (10-20GB)...\")\n\ntry:\n    # 토크나이저 로드\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,  # 일부 모델은 커스텀 코드 필요\n        use_fast=True  # Fast tokenizer 사용 (더 빠름)\n    )\n    \n    # 패딩 토큰 설정 (없는 경우)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"ℹ️ 패딩 토큰을 EOS 토큰으로 설정했습니다.\")\n    \n    # GPU 메모리 확인\n    if torch.cuda.is_available():\n        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n        print(f\"\\n💾 사용 가능한 GPU 메모리: {free_memory / 1024**3:.1f} GB\")\n        \n        # 메모리가 부족한 경우 경고\n        if free_memory < 10 * 1024**3:  # 10GB 미만\n            print(\"⚠️ GPU 메모리가 부족할 수 있습니다. batch_size를 줄이는 것을 권장합니다.\")\n    \n    # 모델 로드 (4bit 양자화 적용)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=quantization_config,\n        device_map=\"auto\",  # GPU에 자동 배치\n        trust_remote_code=True,\n        torch_dtype=torch.float16,  # FP16 사용\n        low_cpu_mem_usage=True  # CPU 메모리 사용량 감소\n    )\n    \n    # 학습을 위한 모델 준비\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA 적용\n    model = get_peft_model(model, lora_config)\n    \n    # 모델 정보 출력\n    print(\"\\n✅ 모델 로딩 완료!\")\n    print(f\"📊 학습 가능한 파라미터:\")\n    model.print_trainable_parameters()\n    \nexcept Exception as e:\n    print(f\"\\n❌ 모델 로딩 실패: {str(e)}\")\n    print(\"💡 해결 방법:\")\n    print(\"  1. 인터넷 연결 확인\")\n    print(\"  2. Hugging Face 토큰 설정 확인\")\n    print(\"  3. GPU 메모리 확인\")\n    raise e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터셋 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "def format_prompt(question: str, answer: str, is_training: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    질문과 답변을 모델 학습용 프롬프트로 포맷팅\n",
    "    \n",
    "    Args:\n",
    "        question: 질문 텍스트\n",
    "        answer: 답변 텍스트\n",
    "        is_training: 학습용인지 여부 (학습시에는 답변 포함)\n",
    "        \n",
    "    Returns:\n",
    "        포맷된 프롬프트 문자열\n",
    "    \"\"\"\n",
    "    # 모델별 프롬프트 템플릿\n",
    "    if \"exaone\" in MODEL_NAME.lower():\n",
    "        # EXAONE 모델용 템플릿\n",
    "        if is_training:\n",
    "            prompt = f\"[|시스템|]당신은 금융 전문 AI 어시스턴트입니다. 정확하고 전문적인 답변을 제공하세요.[|종료|]\\n[|사용자|]{question}[|종료|]\\n[|AI|]{answer}[|종료|]\"\n",
    "        else:\n",
    "            prompt = f\"[|시스템|]당신은 금융 전문 AI 어시스턴트입니다. 정확하고 전문적인 답변을 제공하세요.[|종료|]\\n[|사용자|]{question}[|종료|]\\n[|AI|]\"\n",
    "    elif \"solar\" in MODEL_NAME.lower():\n",
    "        # SOLAR 모델용 템플릿\n",
    "        system_prompt = \"당신은 한국 금융 시장에 정통한 전문가입니다. 질문에 대해 정확하고 상세한 답변을 제공하세요.\"\n",
    "        if is_training:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n{answer}\"\n",
    "        else:\n",
    "            prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{question}\\n\\n### Assistant:\\n\"\n",
    "    else:\n",
    "        # 기본 템플릿 (Qwen, Llama 등)\n",
    "        if is_training:\n",
    "            prompt = f\"질문: {question}\\n\\n답변: {answer}\"\n",
    "        else:\n",
    "            prompt = f\"질문: {question}\\n\\n답변: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 샘플 프롬프트 확인\n",
    "if train_data:\n",
    "    sample = train_data[0]\n",
    "    sample_prompt = format_prompt(sample['question'], sample['answer'])\n",
    "    print(\"📝 프롬프트 템플릿 예시:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(sample_prompt[:500] + \"...\" if len(sample_prompt) > 500 else sample_prompt)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 토큰화 함수\ndef tokenize_function(examples: Dict[str, List]) -> Dict[str, List]:\n    \"\"\"\n    배치 단위로 데이터를 토큰화\n    \n    Args:\n        examples: 배치 데이터 (questions, answers 포함)\n        \n    Returns:\n        토큰화된 데이터 딕셔너리\n    \"\"\"\n    # 프롬프트 생성 (에러 처리 추가)\n    prompts = []\n    for question, answer in zip(examples['question'], examples['answer']):\n        # None 값 체크\n        if question is None or answer is None:\n            continue\n        # 문자열 변환 및 공백 제거\n        question = str(question).strip()\n        answer = str(answer).strip()\n        if question and answer:\n            prompt = format_prompt(question, answer, is_training=True)\n            prompts.append(prompt)\n    \n    # 유효한 프롬프트가 없는 경우 처리\n    if not prompts:\n        raise ValueError(\"유효한 프롬프트가 없습니다!\")\n    \n    # 토큰화\n    model_inputs = tokenizer(\n        prompts,\n        max_length=2048,  # 최대 토큰 길이\n        padding=\"max_length\",  # 최대 길이까지 패딩\n        truncation=True,  # 긴 텍스트는 자르기\n        return_tensors=\"pt\"\n    )\n    \n    # 레이블 설정 (input_ids와 동일하게, 패딩 부분은 -100으로)\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n    \n    # 패딩 토큰은 손실 계산에서 제외 (-100으로 설정)\n    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n    \n    return model_inputs\n\n# 데이터를 HuggingFace Dataset으로 변환\ndef prepare_datasets(train_data: List[Dict], val_data: List[Dict]):\n    \"\"\"\n    학습/검증 데이터를 HuggingFace Dataset 형식으로 변환하고 토큰화\n    \n    Args:\n        train_data: 학습 데이터 리스트\n        val_data: 검증 데이터 리스트\n        \n    Returns:\n        (토큰화된 학습 데이터셋, 토큰화된 검증 데이터셋)\n    \"\"\"\n    # 리스트를 DataFrame으로 변환 (쉬운 처리를 위해)\n    train_df = pd.DataFrame(train_data)\n    val_df = pd.DataFrame(val_data)\n    \n    # HuggingFace Dataset으로 변환\n    train_dataset = HFDataset.from_pandas(train_df)\n    val_dataset = HFDataset.from_pandas(val_df)\n    \n    # 토큰화 적용\n    print(\"🔄 학습 데이터 토큰화 중...\")\n    tokenized_train = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=train_dataset.column_names\n    )\n    \n    print(\"🔄 검증 데이터 토큰화 중...\")\n    tokenized_val = val_dataset.map(\n        tokenize_function,\n        batched=True,\n        batch_size=32,\n        remove_columns=val_dataset.column_names\n    )\n    \n    print(\"\\n✅ 토큰화 완료!\")\n    print(f\"  - 학습 데이터: {len(tokenized_train)}개\")\n    print(f\"  - 검증 데이터: {len(tokenized_val)}개\")\n    \n    return tokenized_train, tokenized_val\n\n# 데이터셋 준비\nif train_data and val_data:\n    tokenized_train_dataset, tokenized_val_dataset = prepare_datasets(train_data, val_data)\nelse:\n    print(\"⚠️ 학습 데이터가 없습니다! FSKU_1_데이터증강_RAG.ipynb를 먼저 실행하세요.\")\n    tokenized_train_dataset, tokenized_val_dataset = None, None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 토큰 길이 분포 시각화\ndef visualize_token_distribution(dataset, title=\"Token Length Distribution\"):\n    \"\"\"\n    데이터셋의 토큰 길이 분포를 시각화\n    \n    Args:\n        dataset: 토큰화된 데이터셋\n        title: 그래프 제목\n    \"\"\"\n    if dataset is None:\n        return\n    \n    # 실제 토큰 길이 계산 (패딩 제외) - 메모리 효율적으로\n    lengths = []\n    sample_size = min(1000, len(dataset))  # 최대 1000개 샘플만 분석\n    indices = np.random.choice(len(dataset), sample_size, replace=False)\n    \n    for idx in indices:\n        item = dataset[int(idx)]\n        # attention_mask가 1인 부분만 실제 토큰\n        actual_length = sum(item['attention_mask'])\n        lengths.append(actual_length)\n    \n    print(f\"\\n📊 샘플 크기: {sample_size}개 (전체 {len(dataset)}개 중)\")\n    \n    # 통계 계산\n    avg_length = np.mean(lengths)\n    median_length = np.median(lengths)\n    max_length = np.max(lengths)\n    \n    # 시각화\n    plt.figure(figsize=(10, 6))\n    plt.hist(lengths, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    plt.axvline(avg_length, color='red', linestyle='--', label=f'Average: {avg_length:.0f}')\n    plt.axvline(median_length, color='green', linestyle='--', label=f'Median: {median_length:.0f}')\n    plt.xlabel('Token Length')\n    plt.ylabel('Frequency')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    print(f\"📊 토큰 길이 통계:\")\n    print(f\"  - 평균: {avg_length:.0f} 토큰\")\n    print(f\"  - 중간값: {median_length:.0f} 토큰\")\n    print(f\"  - 최대: {max_length} 토큰\")\n    print(f\"  - 2048 토큰 초과: {sum(1 for l in lengths if l >= 2048)}개 ({sum(1 for l in lengths if l >= 2048)/len(lengths)*100:.1f}%)\")\n\n# 학습 데이터 토큰 분포 확인\nif tokenized_train_dataset:\n    visualize_token_distribution(tokenized_train_dataset, \"Training Data Token Distribution\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 학습 하이퍼파라미터 설정 (개선된 동적 설정)\ndef get_training_args(output_dir: str = \"./models/checkpoints\"):\n    \"\"\"\n    RTX 4090 24GB에 최적화된 학습 설정 반환\n    \n    Args:\n        output_dir: 체크포인트 저장 디렉토리\n        \n    Returns:\n        TrainingArguments 객체\n    \"\"\"\n    # 동적 배치 크기 계산 (더 정교한 메모리 관리)\n    batch_size = 4  # 기본값\n    gradient_accumulation = 2  # 기본값\n    \n    if torch.cuda.is_available():\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n        free_memory = total_memory - allocated_memory\n        \n        # 모델 크기에 따른 동적 배치 크기 조정\n        if SELECTED_MODEL in AVAILABLE_MODELS:\n            model_size = float(AVAILABLE_MODELS[SELECTED_MODEL]['size'].replace('B', ''))\n        else:\n            model_size = 7.0  # 기본값\n        \n        # 메모리와 모델 크기를 고려한 배치 크기 결정\n        if free_memory < 8 or model_size > 10:\n            batch_size = 1\n            gradient_accumulation = 8\n        elif free_memory < 12:\n            batch_size = 2\n            gradient_accumulation = 4\n        elif free_memory < 16:\n            batch_size = 3\n            gradient_accumulation = 3\n        else:\n            batch_size = 4\n            gradient_accumulation = 2\n            \n        print(f\"💾 GPU 메모리: {free_memory:.1f}GB 여유\")\n        print(f\"🤖 모델 크기: {model_size}B\")\n        print(f\"⚙️ 배치 크기: {batch_size}, Gradient Accumulation: {gradient_accumulation}\")\n        print(f\"   → 실효 배치 크기: {batch_size * gradient_accumulation}\")\n    \n    # 전체 학습 스텝 수 계산\n    if tokenized_train_dataset:\n        steps_per_epoch = len(tokenized_train_dataset) // (batch_size * gradient_accumulation)\n        num_epochs = 3\n        total_steps = steps_per_epoch * num_epochs\n        \n        # 데이터가 적은 경우 에폭 수 증가\n        if len(tokenized_train_dataset) < 5000:\n            num_epochs = 5\n            total_steps = steps_per_epoch * num_epochs\n            print(f\"ℹ️ 데이터가 적어 에폭을 {num_epochs}로 증가시켰습니다.\")\n    else:\n        total_steps = 1000\n        num_epochs = 3\n    \n    # 학습률 자동 조정\n    base_lr = 2e-4\n    if batch_size * gradient_accumulation < 8:\n        # 작은 배치 크기에는 더 낮은 학습률\n        learning_rate = base_lr * 0.5\n    else:\n        learning_rate = base_lr\n    \n    return TrainingArguments(\n        # 기본 설정\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        \n        # 학습 설정\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        gradient_accumulation_steps=gradient_accumulation,\n        gradient_checkpointing=True,  # 메모리 절약\n        \n        # 옵티마이저 설정\n        learning_rate=learning_rate,\n        weight_decay=0.01,\n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,  # Gradient clipping\n        optim=\"adamw_torch\",  # 더 효율적인 옵티마이저\n        \n        # 학습률 스케줄러\n        lr_scheduler_type=\"cosine\",\n        warmup_steps=int(total_steps * 0.1),  # 10% warmup\n        warmup_ratio=0.0,  # warmup_steps 사용시 0으로 설정\n        \n        # 로깅 및 저장\n        logging_steps=max(10, total_steps // 100),  # 최소 10 스텝, 최대 전체의 1%\n        logging_first_step=True,\n        save_strategy=\"steps\",\n        save_steps=max(100, total_steps // 10),  # 최소 100 스텝, 최대 10개 체크포인트\n        save_total_limit=3,  # 최대 3개의 체크포인트만 유지\n        save_safetensors=True,  # 더 안전한 형식으로 저장\n        \n        # 평가 설정\n        evaluation_strategy=\"steps\",\n        eval_steps=max(100, total_steps // 10),\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        load_best_model_at_end=True,\n        \n        # 성능 최적화\n        fp16=True,  # Mixed precision training\n        fp16_opt_level=\"O1\",  # 안정적인 mixed precision\n        tf32=True,  # A100/4090에서 성능 향상\n        dataloader_num_workers=min(4, psutil.cpu_count()),\n        dataloader_pin_memory=True,  # GPU 전송 속도 향상\n        remove_unused_columns=False,\n        \n        # 추가 설정\n        push_to_hub=False,\n        report_to=[\"tensorboard\"],\n        logging_dir=\"./logs\",\n        seed=42,\n        \n        # 디버깅 및 모니터링\n        debug=\"underflow_overflow\" if torch.cuda.is_available() else \"\",\n        include_inputs_for_metrics=False,\n    )\n\n# 학습 설정 생성\ntraining_args = get_training_args()\n\nprint(\"\\n⚙️ 학습 설정 완료!\")\nprint(f\"  - 에폭 수: {training_args.num_train_epochs}\")\nprint(f\"  - 배치 크기: {training_args.per_device_train_batch_size} × {training_args.gradient_accumulation_steps} = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - 학습률: {training_args.learning_rate:.2e}\")\nprint(f\"  - Warmup 스텝: {training_args.warmup_steps}\")\nprint(f\"  - 체크포인트 저장 간격: {training_args.save_steps} 스텝\")\nprint(f\"  - Mixed Precision: FP16={training_args.fp16}, TF32={training_args.tf32}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기 종료 콜백 설정\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # 3번의 평가에서 개선이 없으면 종료\n",
    "    early_stopping_threshold=0.001  # 최소 개선 폭\n",
    ")\n",
    "\n",
    "print(\"🛑 조기 종료 설정 완료!\")\n",
    "print(f\"  - Patience: {early_stopping_callback.early_stopping_patience}\")\n",
    "print(f\"  - Threshold: {early_stopping_callback.early_stopping_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 커스텀 Trainer 클래스 (향상된 모니터링)\nclass FSKUTrainer(Trainer):\n    \"\"\"\n    FSKU 프로젝트용 커스텀 Trainer\n    향상된 메트릭 추적과 메모리 관리 기능 포함\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_history = []\n        self.perplexity_history = []\n        self.gpu_memory_history = []\n        self.learning_rate_history = []\n        self.best_perplexity = float('inf')\n        self.steps_since_improvement = 0\n        \n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"\n        손실 계산 및 추가 메트릭 추적\n        \"\"\"\n        # 기본 손실 계산\n        outputs = model(**inputs)\n        loss = outputs.loss\n        \n        # 손실 기록\n        if loss is not None:\n            self.loss_history.append(loss.item())\n        \n        # Perplexity 계산 및 모니터링\n        if len(self.loss_history) % 50 == 0 and self.loss_history:\n            avg_loss = np.mean(self.loss_history[-50:])\n            perplexity = np.exp(min(avg_loss, 10))  # Overflow 방지\n            self.perplexity_history.append(perplexity)\n            \n            # 개선 추적\n            if perplexity < self.best_perplexity:\n                self.best_perplexity = perplexity\n                self.steps_since_improvement = 0\n            else:\n                self.steps_since_improvement += 50\n            \n            # GPU 메모리 모니터링\n            if torch.cuda.is_available():\n                gpu_memory = torch.cuda.memory_allocated() / 1024**3\n                gpu_util = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100\n                self.gpu_memory_history.append(gpu_memory)\n                \n                # 상태 출력\n                print(f\"\\n📊 [스텝 {len(self.loss_history)}]\")\n                print(f\"   Perplexity: {perplexity:.2f} (최고: {self.best_perplexity:.2f})\")\n                print(f\"   GPU: {gpu_memory:.1f}GB ({gpu_util:.1f}%)\")\n                print(f\"   개선 없음: {self.steps_since_improvement} 스텝\")\n                \n                # 메모리 경고\n                if gpu_util > 90:\n                    print(\"   ⚠️ GPU 메모리 사용률 90% 초과!\")\n                \n                # 학습 정체 경고\n                if self.steps_since_improvement > 500:\n                    print(\"   ⚠️ 500 스텝 이상 개선이 없습니다. 학습률 조정을 고려하세요.\")\n        \n        return (loss, outputs) if return_outputs else loss\n    \n    def log(self, logs):\n        \"\"\"향상된 로깅\"\"\"\n        # GPU 메트릭 추가\n        if torch.cuda.is_available():\n            logs[\"gpu_memory_gb\"] = torch.cuda.memory_allocated() / 1024**3\n            logs[\"gpu_utilization\"] = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100\n        \n        # 학습률 추적\n        if hasattr(self, 'lr_scheduler') and self.lr_scheduler is not None:\n            current_lr = self.lr_scheduler.get_last_lr()[0]\n            logs[\"learning_rate\"] = current_lr\n            self.learning_rate_history.append(current_lr)\n        \n        # Perplexity 추가\n        if self.perplexity_history:\n            logs[\"perplexity\"] = self.perplexity_history[-1]\n            \n        super().log(logs)\n    \n    def _save_checkpoint(self, model, trial, metrics=None):\n        \"\"\"체크포인트 저장시 추가 정보 포함\"\"\"\n        # 기본 체크포인트 저장\n        super()._save_checkpoint(model, trial, metrics)\n        \n        # 추가 메트릭 저장\n        checkpoint_folder = os.path.join(\n            self.args.output_dir,\n            f\"{self.state.global_step}\"\n        )\n        \n        if os.path.exists(checkpoint_folder):\n            metrics_file = os.path.join(checkpoint_folder, \"training_metrics.json\")\n            additional_metrics = {\n                \"best_perplexity\": float(self.best_perplexity) if self.best_perplexity != float('inf') else None,\n                \"steps_since_improvement\": self.steps_since_improvement,\n                \"avg_gpu_memory\": float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else None,\n                \"current_step\": self.state.global_step,\n                \"timestamp\": datetime.now().isoformat()\n            }\n            \n            with open(metrics_file, 'w') as f:\n                json.dump(additional_metrics, f, indent=2)\n\n# Trainer 초기화\nif tokenized_train_dataset and tokenized_val_dataset:\n    trainer = FSKUTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_dataset,\n        eval_dataset=tokenized_val_dataset,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False,  # Causal LM이므로 MLM 비활성화\n            pad_to_multiple_of=8  # 효율성을 위해 8의 배수로 패딩\n        ),\n        callbacks=[early_stopping_callback],\n    )\n    \n    print(\"\\n✅ Trainer 초기화 완료!\")\n    \n    # 예상 학습 시간 계산\n    total_steps = len(tokenized_train_dataset) // (trainer.args.per_device_train_batch_size * trainer.args.gradient_accumulation_steps) * trainer.args.num_train_epochs\n    estimated_time = total_steps * 0.5 / 60  # 스텝당 0.5초 가정, 분 단위\n    \n    print(f\"📊 학습 정보:\")\n    print(f\"  - 총 학습 스텝: {total_steps:,}\")\n    print(f\"  - 예상 시간: 약 {estimated_time:.0f}분\")\n    print(f\"  - 체크포인트 수: 약 {total_steps // trainer.args.save_steps}개\")\nelse:\n    trainer = None\n    print(\"⚠️ 학습 데이터가 없어 Trainer를 초기화할 수 없습니다!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU 메모리 정리 함수 (개선된 버전)\ndef clear_gpu_memory():\n    \"\"\"\n    GPU 메모리를 효과적으로 정리하여 OOM 방지\n    \"\"\"\n    import gc\n    \n    if torch.cuda.is_available():\n        # 메모리 사용 전 상태\n        before_allocated = torch.cuda.memory_allocated() / 1024**3\n        before_reserved = torch.cuda.memory_reserved() / 1024**3\n        \n        # 모든 캐시된 메모리 해제\n        torch.cuda.synchronize()\n        \n        # Python 가비지 컬렉션\n        gc.collect()\n        \n        # CUDA 캐시 정리\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n        \n        # 메모리 사용 후 상태\n        after_allocated = torch.cuda.memory_allocated() / 1024**3\n        after_reserved = torch.cuda.memory_reserved() / 1024**3\n        \n        # 정리 결과 출력\n        print(f\"🧹 GPU 메모리 정리 완료!\")\n        print(f\"   할당 메모리: {before_allocated:.2f} → {after_allocated:.2f} GB\")\n        print(f\"   예약 메모리: {before_reserved:.2f} → {after_reserved:.2f} GB\")\n        print(f\"   해제된 메모리: {(before_reserved - after_reserved):.2f} GB\")\n        \n        # 사용 가능 메모리 계산\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        free_memory = total_memory - after_reserved\n        print(f\"   사용 가능: {free_memory:.2f} GB / {total_memory:.2f} GB\")\n        \n        # 메모리 부족 경고 및 대응 방안\n        if free_memory < 5:\n            print(\"\\n⚠️ GPU 메모리가 5GB 미만입니다!\")\n            print(\"💡 해결 방법:\")\n            print(\"   1. batch_size를 1로 줄이기\")\n            print(\"   2. gradient_accumulation_steps를 8-16으로 증가\")\n            print(\"   3. max_length를 1024로 감소\")\n            print(\"   4. gradient_checkpointing=True 확인\")\n            print(\"   5. 다른 GPU 프로세스 종료\")\n            \n            # nvidia-smi 정보 제안\n            print(\"\\n💡 다른 프로세스 확인: nvidia-smi\")\n            \n        return free_memory\n    else:\n        print(\"ℹ️ GPU를 사용할 수 없습니다.\")\n        return 0\n\n# 학습 전 메모리 정리\nfree_memory = clear_gpu_memory()\n\n# 메모리가 부족한 경우 경고\nif free_memory > 0 and free_memory < 3:\n    print(\"\\n⚠️ 심각한 메모리 부족! 학습을 시작하기 전에 다음을 확인하세요:\")\n    print(\"   - 다른 노트북이나 프로그램 종료\")\n    print(\"   - 커널 재시작 고려\")\n    print(\"   - batch_size=1로 설정\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델 학습 실행 (향상된 오류 처리)\nif trainer is not None:\n    print(\"\\n🚀 모델 학습을 시작합니다!\")\n    print(\"⏱️ 예상 소요 시간: 데이터 크기에 따라 1-4시간\")\n    print(\"💡 팁: 학습 중 GPU 메모리가 부족하면 batch_size를 줄이세요.\")\n    print(\"📊 TensorBoard 모니터링: tensorboard --logdir ./logs\\n\")\n    \n    # 학습 시작 시간 기록\n    start_time = datetime.now()\n    \n    # 학습 진행 상태 추적\n    training_successful = False\n    error_count = 0\n    max_retries = 2\n    \n    while not training_successful and error_count < max_retries:\n        try:\n            # 학습 실행\n            train_result = trainer.train()\n            training_successful = True\n            \n            # 학습 완료\n            end_time = datetime.now()\n            training_time = end_time - start_time\n            \n            print(f\"\\n✅ 학습 완료!\")\n            print(f\"⏱️ 총 학습 시간: {training_time}\")\n            print(f\"📊 최종 학습 손실: {train_result.training_loss:.4f}\")\n            \n            # 학습 메트릭 저장 (더 상세한 정보)\n            metrics = {\n                \"training_loss\": float(train_result.training_loss),\n                \"training_time\": str(training_time),\n                \"training_time_seconds\": training_time.total_seconds(),\n                \"model_name\": MODEL_NAME,\n                \"model_type\": SELECTED_MODEL,\n                \"total_steps\": train_result.global_step,\n                \"epochs\": training_args.num_train_epochs,\n                \"train_samples\": len(train_data) if 'train_data' in globals() else 0,\n                \"val_samples\": len(val_data) if 'val_data' in globals() else 0,\n                \"batch_size\": training_args.per_device_train_batch_size,\n                \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n                \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n                \"learning_rate\": float(training_args.learning_rate),\n                \"lora_rank\": lora_config.r,\n                \"timestamp\": datetime.now().isoformat(),\n                \"retry_count\": error_count\n            }\n            \n            # 커스텀 트레이너 메트릭 추가\n            if hasattr(trainer, 'best_perplexity'):\n                metrics[\"best_perplexity\"] = float(trainer.best_perplexity) if trainer.best_perplexity != float('inf') else None\n            \n            if hasattr(trainer, 'gpu_memory_history') and trainer.gpu_memory_history:\n                metrics[\"avg_gpu_memory_gb\"] = float(np.mean(trainer.gpu_memory_history))\n                metrics[\"max_gpu_memory_gb\"] = float(np.max(trainer.gpu_memory_history))\n            \n            # 최종 검증 손실 추가\n            if hasattr(trainer.state, 'best_metric'):\n                metrics[\"best_eval_loss\"] = float(trainer.state.best_metric)\n            \n            # 메트릭 저장\n            os.makedirs(\"results\", exist_ok=True)\n            with open(\"results/training_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n                json.dump(metrics, f, indent=2, ensure_ascii=False)\n            \n            print(\"\\n📊 학습 메트릭이 results/training_metrics.json에 저장되었습니다.\")\n            \n            # 성능 요약\n            if \"best_perplexity\" in metrics and metrics[\"best_perplexity\"]:\n                print(f\"\\n🏆 최고 성능:\")\n                print(f\"  - Perplexity: {metrics['best_perplexity']:.2f}\")\n            if \"best_eval_loss\" in metrics:\n                print(f\"  - 검증 손실: {metrics['best_eval_loss']:.4f}\")\n            if \"avg_gpu_memory_gb\" in metrics:\n                print(f\"  - 평균 GPU 사용: {metrics['avg_gpu_memory_gb']:.1f} GB\")\n                \n        except torch.cuda.OutOfMemoryError as e:\n            error_count += 1\n            print(f\"\\n❌ GPU 메모리 부족 오류! (시도 {error_count}/{max_retries})\")\n            \n            # 메모리 정리\n            clear_gpu_memory()\n            \n            if error_count < max_retries:\n                print(\"\\n💡 자동 복구 시도 중...\")\n                print(\"   배치 크기를 줄이고 다시 시도합니다.\")\n                \n                # 배치 크기 감소\n                trainer.args.per_device_train_batch_size = max(1, trainer.args.per_device_train_batch_size // 2)\n                trainer.args.gradient_accumulation_steps = min(16, trainer.args.gradient_accumulation_steps * 2)\n                \n                print(f\"   새로운 배치 크기: {trainer.args.per_device_train_batch_size}\")\n                print(f\"   새로운 Gradient Accumulation: {trainer.args.gradient_accumulation_steps}\")\n                \n                # 잠시 대기\n                import time\n                time.sleep(5)\n            else:\n                print(\"\\n💡 수동 해결 방법:\")\n                print(\"  1. 커널 재시작 후 batch_size=1로 재실행\")\n                print(\"  2. gradient_accumulation_steps를 8~16으로 증가\")\n                print(\"  3. max_length를 1024로 감소\")\n                print(\"  4. 더 작은 모델 사용 고려\")\n                raise e\n                \n        except KeyboardInterrupt:\n            print(\"\\n⚠️ 사용자가 학습을 중단했습니다.\")\n            print(\"💡 현재까지의 체크포인트는 저장되었습니다.\")\n            training_successful = True  # 루프 종료\n            \n        except Exception as e:\n            error_count += 1\n            print(f\"\\n❌ 학습 중 오류 발생: {str(e)} (시도 {error_count}/{max_retries})\")\n            \n            if error_count < max_retries:\n                print(\"💡 5초 후 재시도합니다...\")\n                import time\n                time.sleep(5)\n            else:\n                print(\"\\n💡 일반적인 해결 방법:\")\n                print(\"  1. 인터넷 연결 확인 (모델 다운로드)\")\n                print(\"  2. 디스크 공간 확인 (체크포인트 저장)\")\n                print(\"  3. 에러 메시지 검색\")\n                raise e\nelse:\n    print(\"⚠️ Trainer가 초기화되지 않아 학습을 시작할 수 없습니다!\")\n    print(\"\\n🔍 체크리스트:\")\n    print(\"   [ ] data/augmented/ 폴더에 JSON 파일이 있는지 확인\")\n    print(\"   [ ] GPU가 제대로 인식되는지 확인\")\n    print(\"   [ ] 필요한 패키지가 모두 설치되었는지 확인\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 곡선 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 로그 시각화\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    학습 과정의 손실 변화를 시각화\n",
    "    \n",
    "    Args:\n",
    "        trainer: 학습이 완료된 Trainer 객체\n",
    "    \"\"\"\n",
    "    if trainer is None or not hasattr(trainer.state, 'log_history'):\n",
    "        print(\"⚠️ 학습 로그가 없습니다!\")\n",
    "        return\n",
    "    \n",
    "    # 로그에서 손실 값 추출\n",
    "    log_history = trainer.state.log_history\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in log_history:\n",
    "        if 'loss' in log:\n",
    "            train_loss.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_loss)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_loss.append(log['eval_loss'])\n",
    "    \n",
    "    # 시각화\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 학습 손실\n",
    "    if train_loss:\n",
    "        ax1.plot(steps[:len(train_loss)], train_loss, 'b-', label='Training Loss')\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss Over Time')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # 검증 손실\n",
    "    if eval_loss:\n",
    "        eval_steps = [i * training_args.eval_steps for i in range(1, len(eval_loss) + 1)]\n",
    "        ax2.plot(eval_steps, eval_loss, 'r-', marker='o', label='Validation Loss')\n",
    "        ax2.set_xlabel('Steps')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Validation Loss Over Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 최종 손실 출력\n",
    "    if train_loss:\n",
    "        print(f\"\\n📊 최종 학습 손실: {train_loss[-1]:.4f}\")\n",
    "    if eval_loss:\n",
    "        print(f\"📊 최종 검증 손실: {eval_loss[-1]:.4f}\")\n",
    "        print(f\"📊 최고 검증 손실: {min(eval_loss):.4f}\")\n",
    "\n",
    "# 학습 곡선 그리기\n",
    "if trainer and hasattr(trainer, 'state'):\n",
    "    plot_training_history(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 최종 모델 저장\ndef save_final_model(trainer, output_dir: str = \"models/fsku_finetuned_model\"):\n    \"\"\"\n    학습된 모델을 추론용으로 저장\n    \n    Args:\n        trainer: 학습이 완료된 Trainer 객체\n        output_dir: 모델 저장 경로\n    \"\"\"\n    if trainer is None:\n        print(\"⚠️ 저장할 모델이 없습니다!\")\n        return\n    \n    print(f\"\\n💾 모델을 저장합니다: {output_dir}\")\n    \n    # 디렉토리 생성\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # 최고 성능 모델 저장 (LoRA 어댑터만)\n    trainer.save_model(output_dir)\n    \n    # 토크나이저도 저장\n    trainer.tokenizer.save_pretrained(output_dir)\n    \n    # 설정 정보 저장 (더 상세한 정보)\n    config_info = {\n        \"base_model\": MODEL_NAME,\n        \"model_type\": SELECTED_MODEL,\n        \"training_completed\": datetime.now().isoformat(),\n        \"lora_config\": {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"target_modules\": lora_config.target_modules\n        },\n        \"training_args\": {\n            \"num_train_epochs\": training_args.num_train_epochs,\n            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n            \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n            \"learning_rate\": training_args.learning_rate,\n            \"warmup_steps\": training_args.warmup_steps,\n            \"fp16\": training_args.fp16\n        },\n        \"dataset_info\": {\n            \"train_samples\": len(train_data) if 'train_data' in globals() else 0,\n            \"val_samples\": len(val_data) if 'val_data' in globals() else 0\n        },\n        \"final_loss\": float(trainer.state.log_history[-1].get('loss', 0)) if hasattr(trainer, 'state') and trainer.state.log_history else 'N/A',\n        \"quantization\": \"4bit\",\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    }\n    \n    with open(os.path.join(output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(config_info, f, indent=2, ensure_ascii=False)\n    \n    print(\"\\n✅ 모델 저장 완료!\")\n    print(f\"📁 저장 위치: {output_dir}\")\n    print(\"\\n📝 저장된 파일:\")\n    print(\"  - adapter_model.safetensors (LoRA 가중치)\")\n    print(\"  - adapter_config.json (LoRA 설정)\")\n    print(\"  - tokenizer 파일들\")\n    print(\"  - training_config.json (학습 정보)\")\n    print(\"\\n💡 추론시 이 경로를 FSKU_3_추론.ipynb에서 사용하세요!\")\n    \n    return output_dir\n\n# 모델 저장 실행\nif trainer:\n    saved_model_path = save_final_model(trainer)\nelse:\n    print(\"⚠️ 학습된 모델이 없어 저장할 수 없습니다!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 모델 평가 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 학습된 모델로 샘플 생성 테스트 (향상된 생성 파라미터)\ndef generate_sample(\n    model, \n    tokenizer, \n    prompt: str, \n    max_length: int = 512,\n    temperature: float = 0.8,\n    top_p: float = 0.95,\n    top_k: int = 50\n):\n    \"\"\"\n    학습된 모델로 텍스트 생성 테스트 (개선된 생성 파라미터)\n    \n    Args:\n        model: 학습된 모델\n        tokenizer: 토크나이저\n        prompt: 입력 프롬프트\n        max_length: 최대 생성 길이\n        temperature: 생성 다양성 (0.1~1.0)\n        top_p: nucleus sampling 파라미터\n        top_k: top-k sampling 파라미터\n        \n    Returns:\n        생성된 텍스트\n    \"\"\"\n    # 모델을 평가 모드로\n    model.eval()\n    \n    # 메모리 효율을 위한 캐시 정리\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # 프롬프트 토큰화\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n    \n    # GPU로 이동\n    if torch.cuda.is_available():\n        inputs = {k: v.cuda() for k, v in inputs.items()}\n    \n    # 텍스트 생성 (더 나은 생성 파라미터)\n    with torch.no_grad():\n        try:\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_length,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                do_sample=True,\n                repetition_penalty=1.2,  # 반복 방지\n                no_repeat_ngram_size=3,  # 3-gram 반복 방지\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n                early_stopping=True,  # EOS 토큰 만나면 조기 종료\n                num_beams=1,  # 빔 서치 비활성화 (더 빠른 생성)\n            )\n        except torch.cuda.OutOfMemoryError:\n            print(\"⚠️ GPU 메모리 부족! 더 짧은 길이로 재시도...\")\n            torch.cuda.empty_cache()\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_length // 2,\n                temperature=temperature,\n                top_p=top_p,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n    \n    # 디코딩\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # 입력 프롬프트 제거\n    if generated_text.startswith(prompt):\n        response = generated_text[len(prompt):].strip()\n    else:\n        # 프롬프트가 포함되지 않은 경우\n        response = generated_text.strip()\n    \n    return response\n\n# 샘플 테스트\nif trainer and val_data:\n    print(\"\\n🧪 학습된 모델 테스트\")\n    print(\"=\" * 60)\n    \n    # 검증 데이터에서 샘플 선택\n    num_samples = min(3, len(val_data))\n    test_samples = random.sample(val_data, num_samples)\n    \n    # 다양한 온도로 테스트\n    temperatures = [0.7, 0.8, 0.9]\n    \n    for i, sample in enumerate(test_samples):\n        print(f\"\\n[테스트 {i+1}]\")\n        print(f\"질문: {sample['question'][:200]}...\")\n        print(f\"\\n정답: {sample['answer'][:300]}...\" if len(sample['answer']) > 300 else f\"\\n정답: {sample['answer']}\")\n        \n        # 질문만으로 프롬프트 생성\n        test_prompt = format_prompt(sample['question'], \"\", is_training=False)\n        \n        # 다양한 온도로 생성\n        temp = temperatures[i % len(temperatures)]\n        print(f\"\\n모델 생성 (temperature={temp}):\")\n        \n        try:\n            generated = generate_sample(\n                model, \n                tokenizer, \n                test_prompt,\n                temperature=temp,\n                max_length=min(512, len(sample['answer']) + 100)\n            )\n            print(generated[:300] + \"...\" if len(generated) > 300 else generated)\n        except Exception as e:\n            print(f\"생성 실패: {str(e)}\")\n        \n        print(\"-\" * 60)\n    \n    # 생성 품질 평가 팁\n    print(\"\\n💡 생성 품질 평가 기준:\")\n    print(\"  1. 답변의 관련성과 정확성\")\n    print(\"  2. 문장의 유창성과 일관성\")\n    print(\"  3. 금융 용어의 적절한 사용\")\n    print(\"  4. 답변 길이의 적절성\")\nelse:\n    print(\"⚠️ 테스트할 모델이나 데이터가 없습니다!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 최종 요약 및 체크리스트\nprint(\"\\n\" + \"=\" * 60)\nprint(\"📊 FSKU 모델 학습 완료 요약\")\nprint(\"=\" * 60)\n\nif trainer:\n    # 학습 완료 정보\n    print(f\"\\n✅ 모델: {MODEL_NAME}\")\n    print(f\"✅ 모델 타입: {SELECTED_MODEL}\")\n    print(f\"✅ 학습 데이터: {len(train_data):,}개\")\n    print(f\"✅ 검증 데이터: {len(val_data):,}개\")\n    print(f\"✅ 학습 에폭: {training_args.num_train_epochs}\")\n    print(f\"✅ 배치 크기: {training_args.per_device_train_batch_size} × {training_args.gradient_accumulation_steps}\")\n    print(f\"✅ 최종 모델 저장: models/fsku_finetuned_model/\")\n    \n    # 성능 요약\n    if hasattr(trainer, 'state') and trainer.state.log_history:\n        final_train_loss = trainer.state.log_history[-1].get('loss', 'N/A')\n        if isinstance(final_train_loss, float):\n            print(f\"\\n📈 성능 지표:\")\n            print(f\"  - 최종 학습 손실: {final_train_loss:.4f}\")\n            print(f\"  - 최종 Perplexity: {np.exp(min(final_train_loss, 10)):.2f}\")\n            \n        if hasattr(trainer, 'best_perplexity') and trainer.best_perplexity != float('inf'):\n            print(f\"  - 최고 Perplexity: {trainer.best_perplexity:.2f}\")\n    \n    # 최적화 팁\n    print(f\"\\n💡 추론 최적화 팁:\")\n    print(f\"  1. 배치 처리로 속도 향상\")\n    print(f\"  2. Temperature 0.7-0.8 사용\")\n    print(f\"  3. Max tokens를 적절히 제한\")\n    print(f\"  4. GPU 메모리 모니터링\")\n    \n    # 다음 단계 체크리스트\n    print(f\"\\n📝 다음 단계 체크리스트:\")\n    print(f\"  ☐ FSKU_3_추론.ipynb 파일 열기\")\n    print(f\"  ☐ 모델 경로 확인: 'models/fsku_finetuned_model'\")\n    print(f\"  ☐ test.csv 파일 준비\")\n    print(f\"  ☐ 추론 실행 (270분 내 완료 목표)\")\n    print(f\"  ☐ 결과 검증 및 제출\")\n    \n    # 유용한 명령어\n    print(f\"\\n🔧 유용한 명령어:\")\n    print(f\"  • TensorBoard 실행: tensorboard --logdir ./logs\")\n    print(f\"  • 모델 크기 확인: du -sh models/fsku_finetuned_model/\")\n    print(f\"  • GPU 모니터링: watch -n 1 nvidia-smi\")\n    print(f\"  • 메모리 정리: torch.cuda.empty_cache()\")\n    \n    # 트러블슈팅\n    print(f\"\\n🚨 자주 발생하는 문제 해결:\")\n    print(f\"  • OOM 오류: batch_size=1, gradient_accumulation_steps=16\")\n    print(f\"  • 느린 추론: vLLM 또는 TGI 사용 고려\")\n    print(f\"  • 품질 문제: 더 많은 데이터로 재학습\")\n    print(f\"  • 토큰 초과: max_length 조정\")\n    \nelse:\n    print(\"\\n❌ 학습이 완료되지 않았습니다!\")\n    print(\"\\n💡 해결 방법:\")\n    print(\"  1. FSKU_1_데이터증강_RAG.ipynb 실행하여 데이터 생성\")\n    print(\"  2. data/augmented/ 폴더 확인\")\n    print(\"  3. GPU 및 메모리 확인\")\n    print(\"  4. 이 노트북 재실행\")\n    \n    print(\"\\n🔍 디버깅 체크리스트:\")\n    print(\"  ☐ data/augmented/ 폴더에 JSON 파일 존재\")\n    print(\"  ☐ GPU 인식 (torch.cuda.is_available())\")\n    print(\"  ☐ 필수 패키지 설치\")\n    print(\"  ☐ 인터넷 연결 (모델 다운로드)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"🎯 FSKU 금융 AI Challenge - 모델 학습 단계 완료!\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "source": "# 학습 결과 종합 분석\ndef analyze_training_results(trainer, train_data, val_data):\n    \"\"\"\n    학습 결과를 종합적으로 분석하고 개선 제안\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"📊 학습 결과 종합 분석\")\n    print(\"=\" * 60)\n    \n    if not trainer or not hasattr(trainer, 'state'):\n        print(\"❌ 분석할 학습 결과가 없습니다.\")\n        return\n    \n    # 1. 데이터 분석\n    print(\"\\n1️⃣ 데이터 통계\")\n    print(f\"  - 학습 데이터: {len(train_data):,}개\")\n    print(f\"  - 검증 데이터: {len(val_data):,}개\")\n    print(f\"  - 학습/검증 비율: {len(train_data)/(len(train_data)+len(val_data))*100:.1f}%\")\n    \n    # 데이터 품질 평가\n    if len(train_data) < 1000:\n        print(\"  ⚠️ 학습 데이터가 부족합니다 (권장: 5,000개 이상)\")\n    elif len(train_data) < 5000:\n        print(\"  ℹ️ 학습 데이터가 적당합니다 (권장: 5,000개 이상)\")\n    else:\n        print(\"  ✅ 충분한 학습 데이터\")\n    \n    # 2. 학습 과정 분석\n    print(\"\\n2️⃣ 학습 과정\")\n    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n        total_steps = trainer.state.global_step\n        print(f\"  - 총 학습 스텝: {total_steps:,}\")\n        print(f\"  - 완료 에폭: {trainer.state.epoch:.1f}/{training_args.num_train_epochs}\")\n        \n        # 손실 추세 분석\n        losses = [log.get('loss', 0) for log in trainer.state.log_history if 'loss' in log]\n        if losses:\n            initial_loss = losses[0]\n            final_loss = losses[-1]\n            improvement = (initial_loss - final_loss) / initial_loss * 100\n            \n            print(f\"  - 초기 손실: {initial_loss:.4f}\")\n            print(f\"  - 최종 손실: {final_loss:.4f}\")\n            print(f\"  - 개선율: {improvement:.1f}%\")\n            \n            if improvement < 10:\n                print(\"  ⚠️ 학습 개선이 미미합니다. 학습률 조정 필요\")\n            elif improvement < 30:\n                print(\"  ℹ️ 적절한 학습 진행\")\n            else:\n                print(\"  ✅ 우수한 학습 개선\")\n    \n    # 3. 성능 지표\n    print(\"\\n3️⃣ 성능 지표\")\n    if hasattr(trainer, 'best_perplexity') and trainer.best_perplexity != float('inf'):\n        print(f\"  - 최고 Perplexity: {trainer.best_perplexity:.2f}\")\n        \n        if trainer.best_perplexity < 10:\n            print(\"  ✅ 우수한 언어 모델 성능\")\n        elif trainer.best_perplexity < 50:\n            print(\"  ℹ️ 적절한 언어 모델 성능\")\n        else:\n            print(\"  ⚠️ 개선이 필요한 성능\")\n    \n    if hasattr(trainer.state, 'best_metric'):\n        print(f\"  - 최고 검증 손실: {trainer.state.best_metric:.4f}\")\n    \n    # 4. 리소스 사용\n    print(\"\\n4️⃣ 리소스 사용\")\n    if hasattr(trainer, 'gpu_memory_history') and trainer.gpu_memory_history:\n        avg_gpu = np.mean(trainer.gpu_memory_history)\n        max_gpu = np.max(trainer.gpu_memory_history)\n        print(f\"  - 평균 GPU 메모리: {avg_gpu:.1f} GB\")\n        print(f\"  - 최대 GPU 메모리: {max_gpu:.1f} GB\")\n        \n        gpu_util = max_gpu / 24 * 100  # RTX 4090 24GB 기준\n        if gpu_util > 90:\n            print(\"  ⚠️ GPU 메모리 사용률이 매우 높습니다\")\n        elif gpu_util > 70:\n            print(\"  ℹ️ GPU 메모리를 효율적으로 사용 중\")\n        else:\n            print(\"  💡 배치 크기를 늘려도 됩니다\")\n    \n    # 5. 개선 제안\n    print(\"\\n5️⃣ 개선 제안\")\n    suggestions = []\n    \n    # 데이터 관련\n    if len(train_data) < 5000:\n        suggestions.append(\"• 더 많은 학습 데이터 생성 (목표: 10,000개)\")\n    \n    # 성능 관련\n    if hasattr(trainer, 'best_perplexity') and trainer.best_perplexity > 30:\n        suggestions.append(\"• 학습률 조정 또는 에폭 수 증가\")\n        suggestions.append(\"• LoRA rank 증가 (현재: 16 → 32)\")\n    \n    # 효율성 관련\n    if hasattr(trainer, 'gpu_memory_history') and trainer.gpu_memory_history:\n        if np.max(trainer.gpu_memory_history) < 18:  # 24GB의 75%\n            suggestions.append(\"• 배치 크기 증가 가능\")\n    \n    if suggestions:\n        for suggestion in suggestions:\n            print(suggestion)\n    else:\n        print(\"✅ 현재 설정이 최적화되어 있습니다!\")\n    \n    # 6. 다음 단계\n    print(\"\\n6️⃣ 다음 단계\")\n    print(\"  1. 모델 저장 확인: models/fsku_finetuned_model/\")\n    print(\"  2. 추론 노트북 실행: FSKU_3_추론.ipynb\")\n    print(\"  3. 성능 평가 및 제출\")\n    \n    return {\n        'train_samples': len(train_data),\n        'val_samples': len(val_data),\n        'best_perplexity': getattr(trainer, 'best_perplexity', None),\n        'suggestions': suggestions\n    }\n\n# 학습 결과 분석 실행\nif trainer and train_data and val_data:\n    analysis_results = analyze_training_results(trainer, train_data, val_data)\nelse:\n    print(\"⚠️ 분석할 학습 결과가 없습니다.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}